{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Rv2c5woiB1mq",
        "sRRTA--cCbA3",
        "bAry7wq9CmxO",
        "34h672TsCw7t",
        "aEzmzBGzvRe1",
        "ksteuBpWYalm",
        "z0KGww12f0Kp",
        "DZ6ZsK9atJR2",
        "fFAHur8xuISV",
        "VmdHuLKfuPn8",
        "JRIeC2zaulbf",
        "-QfSJM-bup4e",
        "zc6xNnpKu1vK",
        "yjlSwgnlvGLL",
        "FBboPLoyvNdp",
        "JdQbOpzQvS5R",
        "XlvfVFR0yXdR",
        "S0237Xobvj7K",
        "v0kQ5F2bvrvC",
        "ep0QmwuIvx3D"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv2c5woiB1mq",
        "colab_type": "text"
      },
      "source": [
        "# Data Load  & preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRRTA--cCbA3",
        "colab_type": "text"
      },
      "source": [
        "## Download an Exctract data to Colab dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIj6q4RN_TVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### download dataset in zip file on colab (found in 'Files' section)\n",
        "\n",
        "!wget -qq http://download.cs.stanford.edu/deep/MRNet-v1.0.zip -P ./data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQwiEt4Ilf-n",
        "colab_type": "code",
        "outputId": "4cd249cd-8d88-4638-b053-19862d699657",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### unzip datset in google drive\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !unzip -qq data/MRNet-v1.0.zip -d \"/content/drive/My Drive/Colab Notebooks\"  \n",
        "\n",
        "!unzip -qq data/MRNet-v1.0.zip -d ./data\n",
        "\n",
        "#!unzip -qq data/MRNet-v1.0.zip -d \"drive/My Drive\"  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "warning [data/MRNet-v1.0.zip]:  4294967296 extra bytes at beginning or within zipfile\n",
            "  (attempting to process anyway)\n",
            "file #1:  bad zipfile offset (local header sig):  4294967296\n",
            "  (attempting to re-compensate)\n",
            "file #2547:  bad zipfile offset (local header sig):  1353202\n",
            "  (attempting to re-compensate)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAry7wq9CmxO",
        "colab_type": "text"
      },
      "source": [
        "## Load Dataset to Numpy Arrays "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2yDhRFJgD8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Load all dataset in numpy arrays\n",
        "import numpy as np\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import csv \n",
        "\n",
        "\n",
        "### dirs(srcs)\n",
        "default_src = './data/MRNet-v1.0/'\n",
        "\n",
        "axial_train_src   = default_src+\"train/axial/\"\n",
        "coronal_train_src = default_src+\"train/coronal/\"\n",
        "sagital_train_src = default_src+\"train/sagittal/\"\n",
        "\n",
        "axial_valid_src   = default_src+\"valid/axial/\"\n",
        "coronal_valid_src = default_src+\"valid/coronal/\"\n",
        "sagital_valid_src = default_src+\"valid/sagittal/\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrSlC2Ld-PtX",
        "colab_type": "code",
        "outputId": "b19ee9ef-0829-4adb-b01f-e49acf9ed312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "### sort all  dirs\n",
        "\n",
        "## Train\n",
        "\n",
        "axial_train = [f for f in listdir(axial_train_src) if isfile(join(axial_train_src, f)) and f.endswith(\".npy\") ]\n",
        "axial_train = sorted(axial_train)\n",
        "print(axial_train)\n",
        "\n",
        "coronal_train = [f for f in listdir(coronal_train_src) if isfile(join(coronal_train_src, f)) and f.endswith(\".npy\") ]\n",
        "coronal_train = sorted(coronal_train)\n",
        "\n",
        "sagital_train = [f for f in listdir(sagital_train_src) if isfile(join(sagital_train_src, f)) and f.endswith(\".npy\") ]\n",
        "sagital_train = sorted(sagital_train)\n",
        "\n",
        "\n",
        "##Valid\n",
        "\n",
        "axial_valid = [f for f in listdir(axial_valid_src) if isfile(join(axial_valid_src, f)) and f.endswith(\".npy\") ]\n",
        "axial_valid = sorted(axial_valid)\n",
        "\n",
        "coronal_valid = [f for f in listdir(coronal_valid_src) if isfile(join(coronal_valid_src, f)) and f.endswith(\".npy\") ]\n",
        "coronal_valid = sorted(coronal_valid)\n",
        "\n",
        "sagital_valid = [f for f in listdir(sagital_valid_src) if isfile(join(sagital_valid_src, f)) and f.endswith(\".npy\") ]\n",
        "sagital_valid = sorted(sagital_valid)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['0000.npy', '0001.npy', '0002.npy', '0003.npy', '0004.npy', '0005.npy', '0006.npy', '0007.npy', '0008.npy', '0009.npy', '0010.npy', '0011.npy', '0012.npy', '0013.npy', '0014.npy', '0015.npy', '0016.npy', '0017.npy', '0018.npy', '0019.npy', '0020.npy', '0021.npy', '0022.npy', '0023.npy', '0024.npy', '0025.npy', '0026.npy', '0027.npy', '0028.npy', '0029.npy', '0030.npy', '0031.npy', '0032.npy', '0033.npy', '0034.npy', '0035.npy', '0036.npy', '0037.npy', '0038.npy', '0039.npy', '0040.npy', '0041.npy', '0042.npy', '0043.npy', '0044.npy', '0045.npy', '0046.npy', '0047.npy', '0048.npy', '0049.npy', '0050.npy', '0051.npy', '0052.npy', '0053.npy', '0054.npy', '0055.npy', '0056.npy', '0057.npy', '0058.npy', '0059.npy', '0060.npy', '0061.npy', '0062.npy', '0063.npy', '0064.npy', '0065.npy', '0066.npy', '0067.npy', '0068.npy', '0069.npy', '0070.npy', '0071.npy', '0072.npy', '0073.npy', '0074.npy', '0075.npy', '0076.npy', '0077.npy', '0078.npy', '0079.npy', '0080.npy', '0081.npy', '0082.npy', '0083.npy', '0084.npy', '0085.npy', '0086.npy', '0087.npy', '0088.npy', '0089.npy', '0090.npy', '0091.npy', '0092.npy', '0093.npy', '0094.npy', '0095.npy', '0096.npy', '0097.npy', '0098.npy', '0099.npy', '0100.npy', '0101.npy', '0102.npy', '0103.npy', '0104.npy', '0105.npy', '0106.npy', '0107.npy', '0108.npy', '0109.npy', '0110.npy', '0111.npy', '0112.npy', '0113.npy', '0114.npy', '0115.npy', '0116.npy', '0117.npy', '0118.npy', '0119.npy', '0120.npy', '0121.npy', '0122.npy', '0123.npy', '0124.npy', '0125.npy', '0126.npy', '0127.npy', '0128.npy', '0129.npy', '0130.npy', '0131.npy', '0132.npy', '0133.npy', '0134.npy', '0135.npy', '0136.npy', '0137.npy', '0138.npy', '0139.npy', '0140.npy', '0141.npy', '0142.npy', '0143.npy', '0144.npy', '0145.npy', '0146.npy', '0147.npy', '0148.npy', '0149.npy', '0150.npy', '0151.npy', '0152.npy', '0153.npy', '0154.npy', '0155.npy', '0156.npy', '0157.npy', '0158.npy', '0159.npy', '0160.npy', '0161.npy', '0162.npy', '0163.npy', '0164.npy', '0165.npy', '0166.npy', '0167.npy', '0168.npy', '0169.npy', '0170.npy', '0171.npy', '0172.npy', '0173.npy', '0174.npy', '0175.npy', '0176.npy', '0177.npy', '0178.npy', '0179.npy', '0180.npy', '0181.npy', '0182.npy', '0183.npy', '0184.npy', '0185.npy', '0186.npy', '0187.npy', '0188.npy', '0189.npy', '0190.npy', '0191.npy', '0192.npy', '0193.npy', '0194.npy', '0195.npy', '0196.npy', '0197.npy', '0198.npy', '0199.npy', '0200.npy', '0201.npy', '0202.npy', '0203.npy', '0204.npy', '0205.npy', '0206.npy', '0207.npy', '0208.npy', '0209.npy', '0210.npy', '0211.npy', '0212.npy', '0213.npy', '0214.npy', '0215.npy', '0216.npy', '0217.npy', '0218.npy', '0219.npy', '0220.npy', '0221.npy', '0222.npy', '0223.npy', '0224.npy', '0225.npy', '0226.npy', '0227.npy', '0228.npy', '0229.npy', '0230.npy', '0231.npy', '0232.npy', '0233.npy', '0234.npy', '0235.npy', '0236.npy', '0237.npy', '0238.npy', '0239.npy', '0240.npy', '0241.npy', '0242.npy', '0243.npy', '0244.npy', '0245.npy', '0246.npy', '0247.npy', '0248.npy', '0249.npy', '0250.npy', '0251.npy', '0252.npy', '0253.npy', '0254.npy', '0255.npy', '0256.npy', '0257.npy', '0258.npy', '0259.npy', '0260.npy', '0261.npy', '0262.npy', '0263.npy', '0264.npy', '0265.npy', '0266.npy', '0267.npy', '0268.npy', '0269.npy', '0270.npy', '0271.npy', '0272.npy', '0273.npy', '0274.npy', '0275.npy', '0276.npy', '0277.npy', '0278.npy', '0279.npy', '0280.npy', '0281.npy', '0282.npy', '0283.npy', '0284.npy', '0285.npy', '0286.npy', '0287.npy', '0288.npy', '0289.npy', '0290.npy', '0291.npy', '0292.npy', '0293.npy', '0294.npy', '0295.npy', '0296.npy', '0297.npy', '0298.npy', '0299.npy', '0300.npy', '0301.npy', '0302.npy', '0303.npy', '0304.npy', '0305.npy', '0306.npy', '0307.npy', '0308.npy', '0309.npy', '0310.npy', '0311.npy', '0312.npy', '0313.npy', '0314.npy', '0315.npy', '0316.npy', '0317.npy', '0318.npy', '0319.npy', '0320.npy', '0321.npy', '0322.npy', '0323.npy', '0324.npy', '0325.npy', '0326.npy', '0327.npy', '0328.npy', '0329.npy', '0330.npy', '0331.npy', '0332.npy', '0333.npy', '0334.npy', '0335.npy', '0336.npy', '0337.npy', '0338.npy', '0339.npy', '0340.npy', '0341.npy', '0342.npy', '0343.npy', '0344.npy', '0345.npy', '0346.npy', '0347.npy', '0348.npy', '0349.npy', '0350.npy', '0351.npy', '0352.npy', '0353.npy', '0354.npy', '0355.npy', '0356.npy', '0357.npy', '0358.npy', '0359.npy', '0360.npy', '0361.npy', '0362.npy', '0363.npy', '0364.npy', '0365.npy', '0366.npy', '0367.npy', '0368.npy', '0369.npy', '0370.npy', '0371.npy', '0372.npy', '0373.npy', '0374.npy', '0375.npy', '0376.npy', '0377.npy', '0378.npy', '0379.npy', '0380.npy', '0381.npy', '0382.npy', '0383.npy', '0384.npy', '0385.npy', '0386.npy', '0387.npy', '0388.npy', '0389.npy', '0390.npy', '0391.npy', '0392.npy', '0393.npy', '0394.npy', '0395.npy', '0396.npy', '0397.npy', '0398.npy', '0399.npy', '0400.npy', '0401.npy', '0402.npy', '0403.npy', '0404.npy', '0405.npy', '0406.npy', '0407.npy', '0408.npy', '0409.npy', '0410.npy', '0411.npy', '0412.npy', '0413.npy', '0414.npy', '0415.npy', '0416.npy', '0417.npy', '0418.npy', '0419.npy', '0420.npy', '0421.npy', '0422.npy', '0423.npy', '0424.npy', '0425.npy', '0426.npy', '0427.npy', '0428.npy', '0429.npy', '0430.npy', '0431.npy', '0432.npy', '0433.npy', '0434.npy', '0435.npy', '0436.npy', '0437.npy', '0438.npy', '0439.npy', '0440.npy', '0441.npy', '0442.npy', '0443.npy', '0444.npy', '0445.npy', '0446.npy', '0447.npy', '0448.npy', '0449.npy', '0450.npy', '0451.npy', '0452.npy', '0453.npy', '0454.npy', '0455.npy', '0456.npy', '0457.npy', '0458.npy', '0459.npy', '0460.npy', '0461.npy', '0462.npy', '0463.npy', '0464.npy', '0465.npy', '0466.npy', '0467.npy', '0468.npy', '0469.npy', '0470.npy', '0471.npy', '0472.npy', '0473.npy', '0474.npy', '0475.npy', '0476.npy', '0477.npy', '0478.npy', '0479.npy', '0480.npy', '0481.npy', '0482.npy', '0483.npy', '0484.npy', '0485.npy', '0486.npy', '0487.npy', '0488.npy', '0489.npy', '0490.npy', '0491.npy', '0492.npy', '0493.npy', '0494.npy', '0495.npy', '0496.npy', '0497.npy', '0498.npy', '0499.npy', '0500.npy', '0501.npy', '0502.npy', '0503.npy', '0504.npy', '0505.npy', '0506.npy', '0507.npy', '0508.npy', '0509.npy', '0510.npy', '0511.npy', '0512.npy', '0513.npy', '0514.npy', '0515.npy', '0516.npy', '0517.npy', '0518.npy', '0519.npy', '0520.npy', '0521.npy', '0522.npy', '0523.npy', '0524.npy', '0525.npy', '0526.npy', '0527.npy', '0528.npy', '0529.npy', '0530.npy', '0531.npy', '0532.npy', '0533.npy', '0534.npy', '0535.npy', '0536.npy', '0537.npy', '0538.npy', '0539.npy', '0540.npy', '0541.npy', '0542.npy', '0543.npy', '0544.npy', '0545.npy', '0546.npy', '0547.npy', '0548.npy', '0549.npy', '0550.npy', '0551.npy', '0552.npy', '0553.npy', '0554.npy', '0555.npy', '0556.npy', '0557.npy', '0558.npy', '0559.npy', '0560.npy', '0561.npy', '0562.npy', '0563.npy', '0564.npy', '0565.npy', '0566.npy', '0567.npy', '0568.npy', '0569.npy', '0570.npy', '0571.npy', '0572.npy', '0573.npy', '0574.npy', '0575.npy', '0576.npy', '0577.npy', '0578.npy', '0579.npy', '0580.npy', '0581.npy', '0582.npy', '0583.npy', '0584.npy', '0585.npy', '0586.npy', '0587.npy', '0588.npy', '0589.npy', '0590.npy', '0591.npy', '0592.npy', '0593.npy', '0594.npy', '0595.npy', '0596.npy', '0597.npy', '0598.npy', '0599.npy', '0600.npy', '0601.npy', '0602.npy', '0603.npy', '0604.npy', '0605.npy', '0606.npy', '0607.npy', '0608.npy', '0609.npy', '0610.npy', '0611.npy', '0612.npy', '0613.npy', '0614.npy', '0615.npy', '0616.npy', '0617.npy', '0618.npy', '0619.npy', '0620.npy', '0621.npy', '0622.npy', '0623.npy', '0624.npy', '0625.npy', '0626.npy', '0627.npy', '0628.npy', '0629.npy', '0630.npy', '0631.npy', '0632.npy', '0633.npy', '0634.npy', '0635.npy', '0636.npy', '0637.npy', '0638.npy', '0639.npy', '0640.npy', '0641.npy', '0642.npy', '0643.npy', '0644.npy', '0645.npy', '0646.npy', '0647.npy', '0648.npy', '0649.npy', '0650.npy', '0651.npy', '0652.npy', '0653.npy', '0654.npy', '0655.npy', '0656.npy', '0657.npy', '0658.npy', '0659.npy', '0660.npy', '0661.npy', '0662.npy', '0663.npy', '0664.npy', '0665.npy', '0666.npy', '0667.npy', '0668.npy', '0669.npy', '0670.npy', '0671.npy', '0672.npy', '0673.npy', '0674.npy', '0675.npy', '0676.npy', '0677.npy', '0678.npy', '0679.npy', '0680.npy', '0681.npy', '0682.npy', '0683.npy', '0684.npy', '0685.npy', '0686.npy', '0687.npy', '0688.npy', '0689.npy', '0690.npy', '0691.npy', '0692.npy', '0693.npy', '0694.npy', '0695.npy', '0696.npy', '0697.npy', '0698.npy', '0699.npy', '0700.npy', '0701.npy', '0702.npy', '0703.npy', '0704.npy', '0705.npy', '0706.npy', '0707.npy', '0708.npy', '0709.npy', '0710.npy', '0711.npy', '0712.npy', '0713.npy', '0714.npy', '0715.npy', '0716.npy', '0717.npy', '0718.npy', '0719.npy', '0720.npy', '0721.npy', '0722.npy', '0723.npy', '0724.npy', '0725.npy', '0726.npy', '0727.npy', '0728.npy', '0729.npy', '0730.npy', '0731.npy', '0732.npy', '0733.npy', '0734.npy', '0735.npy', '0736.npy', '0737.npy', '0738.npy', '0739.npy', '0740.npy', '0741.npy', '0742.npy', '0743.npy', '0744.npy', '0745.npy', '0746.npy', '0747.npy', '0748.npy', '0749.npy', '0750.npy', '0751.npy', '0752.npy', '0753.npy', '0754.npy', '0755.npy', '0756.npy', '0757.npy', '0758.npy', '0759.npy', '0760.npy', '0761.npy', '0762.npy', '0763.npy', '0764.npy', '0765.npy', '0766.npy', '0767.npy', '0768.npy', '0769.npy', '0770.npy', '0771.npy', '0772.npy', '0773.npy', '0774.npy', '0775.npy', '0776.npy', '0777.npy', '0778.npy', '0779.npy', '0780.npy', '0781.npy', '0782.npy', '0783.npy', '0784.npy', '0785.npy', '0786.npy', '0787.npy', '0788.npy', '0789.npy', '0790.npy', '0791.npy', '0792.npy', '0793.npy', '0794.npy', '0795.npy', '0796.npy', '0797.npy', '0798.npy', '0799.npy', '0800.npy', '0801.npy', '0802.npy', '0803.npy', '0804.npy', '0805.npy', '0806.npy', '0807.npy', '0808.npy', '0809.npy', '0810.npy', '0811.npy', '0812.npy', '0813.npy', '0814.npy', '0815.npy', '0816.npy', '0817.npy', '0818.npy', '0819.npy', '0820.npy', '0821.npy', '0822.npy', '0823.npy', '0824.npy', '0825.npy', '0826.npy', '0827.npy', '0828.npy', '0829.npy', '0830.npy', '0831.npy', '0832.npy', '0833.npy', '0834.npy', '0835.npy', '0836.npy', '0837.npy', '0838.npy', '0839.npy', '0840.npy', '0841.npy', '0842.npy', '0843.npy', '0844.npy', '0845.npy', '0846.npy', '0847.npy', '0848.npy', '0849.npy', '0850.npy', '0851.npy', '0852.npy', '0853.npy', '0854.npy', '0855.npy', '0856.npy', '0857.npy', '0858.npy', '0859.npy', '0860.npy', '0861.npy', '0862.npy', '0863.npy', '0864.npy', '0865.npy', '0866.npy', '0867.npy', '0868.npy', '0869.npy', '0870.npy', '0871.npy', '0872.npy', '0873.npy', '0874.npy', '0875.npy', '0876.npy', '0877.npy', '0878.npy', '0879.npy', '0880.npy', '0881.npy', '0882.npy', '0883.npy', '0884.npy', '0885.npy', '0886.npy', '0887.npy', '0888.npy', '0889.npy', '0890.npy', '0891.npy', '0892.npy', '0893.npy', '0894.npy', '0895.npy', '0896.npy', '0897.npy', '0898.npy', '0899.npy', '0900.npy', '0901.npy', '0902.npy', '0903.npy', '0904.npy', '0905.npy', '0906.npy', '0907.npy', '0908.npy', '0909.npy', '0910.npy', '0911.npy', '0912.npy', '0913.npy', '0914.npy', '0915.npy', '0916.npy', '0917.npy', '0918.npy', '0919.npy', '0920.npy', '0921.npy', '0922.npy', '0923.npy', '0924.npy', '0925.npy', '0926.npy', '0927.npy', '0928.npy', '0929.npy', '0930.npy', '0931.npy', '0932.npy', '0933.npy', '0934.npy', '0935.npy', '0936.npy', '0937.npy', '0938.npy', '0939.npy', '0940.npy', '0941.npy', '0942.npy', '0943.npy', '0944.npy', '0945.npy', '0946.npy', '0947.npy', '0948.npy', '0949.npy', '0950.npy', '0951.npy', '0952.npy', '0953.npy', '0954.npy', '0955.npy', '0956.npy', '0957.npy', '0958.npy', '0959.npy', '0960.npy', '0961.npy', '0962.npy', '0963.npy', '0964.npy', '0965.npy', '0966.npy', '0967.npy', '0968.npy', '0969.npy', '0970.npy', '0971.npy', '0972.npy', '0973.npy', '0974.npy', '0975.npy', '0976.npy', '0977.npy', '0978.npy', '0979.npy', '0980.npy', '0981.npy', '0982.npy', '0983.npy', '0984.npy', '0985.npy', '0986.npy', '0987.npy', '0988.npy', '0989.npy', '0990.npy', '0991.npy', '0992.npy', '0993.npy', '0994.npy', '0995.npy', '0996.npy', '0997.npy', '0998.npy', '0999.npy', '1000.npy', '1001.npy', '1002.npy', '1003.npy', '1004.npy', '1005.npy', '1006.npy', '1007.npy', '1008.npy', '1009.npy', '1010.npy', '1011.npy', '1012.npy', '1013.npy', '1014.npy', '1015.npy', '1016.npy', '1017.npy', '1018.npy', '1019.npy', '1020.npy', '1021.npy', '1022.npy', '1023.npy', '1024.npy', '1025.npy', '1026.npy', '1027.npy', '1028.npy', '1029.npy', '1030.npy', '1031.npy', '1032.npy', '1033.npy', '1034.npy', '1035.npy', '1036.npy', '1037.npy', '1038.npy', '1039.npy', '1040.npy', '1041.npy', '1042.npy', '1043.npy', '1044.npy', '1045.npy', '1046.npy', '1047.npy', '1048.npy', '1049.npy', '1050.npy', '1051.npy', '1052.npy', '1053.npy', '1054.npy', '1055.npy', '1056.npy', '1057.npy', '1058.npy', '1059.npy', '1060.npy', '1061.npy', '1062.npy', '1063.npy', '1064.npy', '1065.npy', '1066.npy', '1067.npy', '1068.npy', '1069.npy', '1070.npy', '1071.npy', '1072.npy', '1073.npy', '1074.npy', '1075.npy', '1076.npy', '1077.npy', '1078.npy', '1079.npy', '1080.npy', '1081.npy', '1082.npy', '1083.npy', '1084.npy', '1085.npy', '1086.npy', '1087.npy', '1088.npy', '1089.npy', '1090.npy', '1091.npy', '1092.npy', '1093.npy', '1094.npy', '1095.npy', '1096.npy', '1097.npy', '1098.npy', '1099.npy', '1100.npy', '1101.npy', '1102.npy', '1103.npy', '1104.npy', '1105.npy', '1106.npy', '1107.npy', '1108.npy', '1109.npy', '1110.npy', '1111.npy', '1112.npy', '1113.npy', '1114.npy', '1115.npy', '1116.npy', '1117.npy', '1118.npy', '1119.npy', '1120.npy', '1121.npy', '1122.npy', '1123.npy', '1124.npy', '1125.npy', '1126.npy', '1127.npy', '1128.npy', '1129.npy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAY38qJDFFax",
        "colab_type": "code",
        "outputId": "4253ff6a-1b99-4ee5-ddf3-e9d7494d2936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "### X Labels (.npy arrays)\n",
        "\n",
        "## Training - 1130 patients\n",
        "\n",
        "# axial_train_src\n",
        "X_train_axial  =  []\n",
        "\n",
        "for i in range (0,len(axial_train)):\n",
        "  data = np.load(axial_train_src+axial_train[i])\n",
        "  X_train_axial.append(data)\n",
        "X_train_axial = np.asarray(X_train_axial)\n",
        "print(X_train_axial[3].shape)\n",
        "\n",
        "\n",
        "# coronal_train_src\n",
        "X_train_coronal = []\n",
        "\n",
        "for i in range (0,len(coronal_train)):\n",
        "  data = np.load(coronal_train_src+coronal_train[i])\n",
        "  X_train_coronal.append(data)\n",
        "X_train_coronal = np.asarray(X_train_coronal)\n",
        "print(X_train_coronal.shape)  \n",
        "\n",
        "# sagital_train_src\n",
        "X_train_sagital = []\n",
        "for i in range (0,len(sagital_train)):\n",
        "  data = np.load(sagital_train_src+sagital_train[i])\n",
        "  X_train_sagital.append(data)\n",
        "X_train_sagital = np.asarray(X_train_sagital)\n",
        "print(X_train_sagital[3].shape)  \n",
        "\n",
        "\n",
        "\n",
        "## valid - 120 patients\n",
        "\n",
        "# axial_valid_src\n",
        "X_valid_axial  =  []\n",
        "\n",
        "for i in range (0,len(axial_valid)):\n",
        "  data = np.load(axial_valid_src+axial_valid[i])\n",
        "  X_valid_axial.append(data)\n",
        "X_valid_axial = np.asarray(X_valid_axial)\n",
        "print(X_valid_axial.shape)\n",
        "\n",
        "\n",
        "# coronal_valid_src\n",
        "X_valid_coronal = []\n",
        "\n",
        "for i in range (0,len(coronal_valid)):\n",
        "  data = np.load(coronal_valid_src+coronal_valid[i])\n",
        "  X_valid_coronal.append(data)\n",
        "X_valid_coronal = np.asarray(X_valid_coronal)\n",
        "print(X_valid_coronal[3].shape)  \n",
        "\n",
        "# sagital_valid_src\n",
        "X_valid_sagital = []\n",
        "for i in range (0,len(sagital_valid)):\n",
        "  data = np.load(sagital_valid_src+sagital_valid[i])\n",
        "  X_valid_sagital.append(data)\n",
        "X_valid_sagital = np.asarray(X_valid_sagital)\n",
        "print(X_train_sagital.shape)  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(33, 256, 256)\n",
            "(1130,)\n",
            "(30, 256, 256)\n",
            "(120,)\n",
            "(22, 256, 256)\n",
            "(1130,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFvTjP80ptEG",
        "colab_type": "code",
        "outputId": "84b305bb-b90d-44e1-ff93-10d7df01d289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "### Y Labels (.csv files) \n",
        "#default_src\n",
        "\n",
        "## Train- 1130 patients\n",
        "\n",
        "Y_train_abnormal = []\n",
        "with open(default_src+\"train-abnormal.csv\", 'r') as csvfile: \n",
        "    csvreader = csv.reader(csvfile) \n",
        "    for row in csvreader: \n",
        "      Y_train_abnormal.append(row[1])\n",
        "Y_train_abnormal = np.asarray(Y_train_abnormal)\n",
        "print(Y_train_abnormal.shape)\n",
        "\n",
        "        \n",
        "Y_train_acl      = []\n",
        "with open(default_src+\"train-acl.csv\", 'r') as csvfile: \n",
        "    csvreader = csv.reader(csvfile) \n",
        "    for row in csvreader: \n",
        "      Y_train_acl.append(row[1])\n",
        "Y_train_acl = np.asarray(Y_train_acl)\n",
        "print(Y_train_acl.shape)\n",
        "\n",
        "\n",
        "\n",
        "Y_train_meniscus = []\n",
        "with open(default_src+\"train-meniscus.csv\", 'r') as csvfile: \n",
        "    csvreader = csv.reader(csvfile) \n",
        "    for row in csvreader: \n",
        "      Y_train_meniscus.append(row[1])\n",
        "Y_train_meniscus = np.asarray(Y_train_meniscus)\n",
        "print(Y_train_meniscus.shape)\n",
        "\n",
        "\n",
        "## Valid- 120 patients\n",
        "\n",
        "Y_valid_abnormal = []\n",
        "with open(default_src+\"valid-abnormal.csv\", 'r') as csvfile: \n",
        "    csvreader = csv.reader(csvfile) \n",
        "    for row in csvreader: \n",
        "      Y_valid_abnormal.append(row[1])\n",
        "Y_valid_abnormal = np.asarray(Y_valid_abnormal)\n",
        "print(Y_valid_abnormal.shape)\n",
        "\n",
        "        \n",
        "Y_valid_acl      = []\n",
        "with open(default_src+\"valid-acl.csv\", 'r') as csvfile: \n",
        "    csvreader = csv.reader(csvfile) \n",
        "    for row in csvreader: \n",
        "      Y_valid_acl.append(row[1])\n",
        "Y_valid_acl = np.asarray(Y_valid_acl)\n",
        "print(Y_valid_acl.shape)\n",
        "\n",
        "\n",
        "Y_valid_meniscus = []\n",
        "with open(default_src+\"valid-meniscus.csv\", 'r') as csvfile: \n",
        "    csvreader = csv.reader(csvfile) \n",
        "    for row in csvreader: \n",
        "      Y_valid_meniscus.append(row[1])\n",
        "Y_valid_meniscus = np.asarray(Y_valid_meniscus)\n",
        "print(Y_valid_meniscus.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1130,)\n",
            "(1130,)\n",
            "(1130,)\n",
            "(120,)\n",
            "(120,)\n",
            "(120,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oXBhLdvwxXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Final loaded arrays\n",
        "\n",
        "# X_train_axial  =  []\n",
        "# X_train_coronal = []\n",
        "# X_train_sagital = []\n",
        "\n",
        "# Y_train_abnormal = []\n",
        "# Y_train_acl      = []\n",
        "# Y_train_mesiscus = []\n",
        "\n",
        "# X_valid_axial   = []\n",
        "# X_valid_coronal = []\n",
        "# X_valid_sagital = []\n",
        "\n",
        "# Y_valid_abnormal = []\n",
        "# Y_valid_acl      = []\n",
        "# Y_valid_mesiscus = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34h672TsCw7t",
        "colab_type": "text"
      },
      "source": [
        "## Display MRI Image -Test-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYqY7xw8oYIx",
        "colab_type": "code",
        "outputId": "1eef1397-807f-4b24-e000-fb72a1badd4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "import PIL.Image\n",
        "from io import StringIO\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "\n",
        "### test displaying an numpyt image\n",
        "\n",
        "#load a npy file\n",
        "test_src = default_src + \"train/axial/\"\n",
        "data = np.load(test_src+\"0000.npy\")\n",
        "\n",
        "\n",
        "#display only 1 slice\n",
        "data = data[0,:,:]\n",
        "\n",
        "print(\"np array\")\n",
        "print(data.shape)\n",
        "print(data)\n",
        "\n",
        "# display image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.imshow(data, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "np array\n",
            "(256, 256)\n",
            "[[ 0  0  0 ...  5  4  3]\n",
            " [ 0  0  0 ...  8  6  8]\n",
            " [ 0  0  0 ... 14 11 11]\n",
            " ...\n",
            " [ 0  0  0 ... 16 15 12]\n",
            " [ 0  0  0 ... 12 13 12]\n",
            " [ 0  0  0 ... 11  7  9]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPcQ+OxJ8AwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEzmzBGzvRe1",
        "colab_type": "text"
      },
      "source": [
        "## Final Numpy Arrays (Median 3 slices/patient )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LaoA1bfvhUV",
        "colab_type": "code",
        "outputId": "d2c1f26d-3859-4112-e37b-b282959a8b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "## Training - 1130 patients\n",
        "\n",
        "final_X_train_axial  =  []\n",
        "\n",
        "for i in range (0,len(axial_train)):\n",
        "  slices = int((X_train_axial[i].shape[0])/2)\n",
        "  tmp = []\n",
        "  tmp.append(X_train_axial[i][slices-1,:,:])\n",
        "  tmp.append(X_train_axial[i][slices,:,:])\n",
        "  tmp.append(X_train_axial[i][slices+2,:,:])\n",
        "  tmp = np.asarray(tmp)\n",
        "  tmp = np.transpose(np.resize(tmp,(3,224,224)))\n",
        "  final_X_train_axial.append(tmp)\n",
        "final_X_train_axial = np.asarray(final_X_train_axial)\n",
        "print(final_X_train_axial.shape)\n",
        "\n",
        "\n",
        "final_X_train_coronal = []\n",
        "\n",
        "for i in range (0,len(coronal_train)):\n",
        "  slices = int((X_train_coronal[i].shape[0])/2)\n",
        "  tmp = []\n",
        "  tmp.append(X_train_coronal[i][slices-1,:,:])\n",
        "  tmp.append(X_train_coronal[i][slices,:,:])\n",
        "  tmp.append(X_train_coronal[i][slices+2,:,:])\n",
        "  tmp = np.asarray(tmp)\n",
        "  tmp = np.transpose(np.resize(tmp,(3,224,224)))\n",
        "  final_X_train_coronal.append(tmp)\n",
        "final_X_train_coronal = np.asarray(final_X_train_coronal)\n",
        "print(final_X_train_coronal.shape)\n",
        "\n",
        "\n",
        "final_X_train_sagital = []\n",
        "\n",
        "for i in range (0,len(sagital_train)):\n",
        "  slices = int((X_train_sagital[i].shape[0])/2)\n",
        "  tmp = []\n",
        "  tmp.append(X_train_sagital[i][slices-1,:,:])\n",
        "  tmp.append(X_train_sagital[i][slices,:,:])\n",
        "  tmp.append(X_train_sagital[i][slices+2,:,:])\n",
        "  tmp = np.asarray(tmp)\n",
        "  tmp = np.transpose(np.resize(tmp,(3,224,224)))\n",
        "  final_X_train_sagital.append(tmp)\n",
        "final_X_train_sagital = np.asarray(final_X_train_sagital)\n",
        "print(final_X_train_sagital.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1130, 224, 224, 3)\n",
            "(1130, 224, 224, 3)\n",
            "(1130, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_esN7srp33OW",
        "colab_type": "code",
        "outputId": "68a8c65f-943e-486a-b3f8-f385d2e83f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "## valid - 120 patients\n",
        "\n",
        "final_X_valid_axial  =  []\n",
        "\n",
        "for i in range (0,len(axial_valid)):\n",
        "  slices = int((X_valid_axial[i].shape[0])/2)\n",
        "  tmp = []\n",
        "  tmp.append(X_valid_axial[i][slices-1,:,:])\n",
        "  tmp.append(X_valid_axial[i][slices,:,:])\n",
        "  tmp.append(X_valid_axial[i][slices+2,:,:])\n",
        "  tmp = np.asarray(tmp)\n",
        "  tmp = np.transpose(np.resize(tmp,(3,224,224)))\n",
        "  final_X_valid_axial.append(tmp)\n",
        "final_X_valid_axial = np.asarray(final_X_valid_axial)\n",
        "print(final_X_valid_axial.shape)\n",
        "\n",
        "\n",
        "final_X_valid_coronal = []\n",
        "\n",
        "for i in range (0,len(coronal_valid)):\n",
        "  slices = int((X_valid_coronal[i].shape[0])/2)\n",
        "  tmp = []\n",
        "  tmp.append(X_valid_coronal[i][slices-1,:,:])\n",
        "  tmp.append(X_valid_coronal[i][slices,:,:])\n",
        "  tmp.append(X_valid_coronal[i][slices+2,:,:])\n",
        "  tmp = np.asarray(tmp)\n",
        "  tmp = np.transpose(np.resize(tmp,(3,224,224)))\n",
        "  final_X_valid_coronal.append(tmp)\n",
        "final_X_valid_coronal = np.asarray(final_X_valid_coronal)\n",
        "print(final_X_valid_coronal.shape)\n",
        "\n",
        "\n",
        "final_X_valid_sagital = []\n",
        "\n",
        "for i in range (0,len(sagital_valid)):\n",
        "  slices = int((X_valid_sagital[i].shape[0])/2)\n",
        "  tmp = []\n",
        "  tmp.append(X_valid_sagital[i][slices-1,:,:])\n",
        "  tmp.append(X_valid_sagital[i][slices,:,:])\n",
        "  tmp.append(X_valid_sagital[i][slices+2,:,:])\n",
        "  tmp = np.asarray(tmp)\n",
        "  tmp = np.transpose(np.resize(tmp,(3,224,224)))\n",
        "  final_X_valid_sagital.append(tmp)\n",
        "final_X_valid_sagital = np.asarray(final_X_valid_sagital)\n",
        "print(final_X_valid_sagital.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(120, 224, 224, 3)\n",
            "(120, 224, 224, 3)\n",
            "(120, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyScVA11Ax5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### delete original Numpy arrays -- free RAM space\n",
        "\n",
        "del X_train_axial\n",
        "del X_train_coronal\n",
        "del X_train_sagital\n",
        "\n",
        "del X_valid_axial   \n",
        "del X_valid_coronal \n",
        "del X_valid_sagital\n",
        "\n",
        "del axial_train \n",
        "del coronal_train \n",
        "del sagital_train \n",
        "\n",
        "del axial_valid \n",
        "del coronal_valid \n",
        "del sagital_valid \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1-035ZXZM4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Split part of validation set  as Test set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Test size <-> validation size \n",
        "\n",
        "final_X_test_axial, final_X_valid_axial, Y_test_abnormal, Y_valid_abnormal     = train_test_split(final_X_valid_axial, Y_valid_abnormal , test_size=0.2)\n",
        "\n",
        "final_X_test_coronal, final_X_valid_coronal, Y_test_acl, Y_valid_acl      = train_test_split(final_X_valid_coronal, Y_valid_acl, test_size=0.2)\n",
        "\n",
        "final_X_test_sagital, final_X_valid_sagital, Y_test_meniscus, Y_valid_meniscus = train_test_split(final_X_valid_sagital, Y_valid_meniscus, test_size=0.2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NklnnLVmn5kJ",
        "colab_type": "code",
        "outputId": "a3524d1d-e4dd-441b-9d11-859f04142cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(final_X_test_axial.shape)\n",
        "print(final_X_valid_axial.shape)\n",
        "print(Y_test_abnormal.shape)\n",
        "print(Y_valid_abnormal.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(96, 224, 224, 3)\n",
            "(24, 224, 224, 3)\n",
            "(96,)\n",
            "(24,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iQUeD46xknV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Final Loaded Arrays\n",
        "\n",
        "# final_X_train_axial  =  []\n",
        "# final_X_train_coronal = []\n",
        "# final_X_train_sagital = []\n",
        "\n",
        "# Y_train_abnormal = []\n",
        "# Y_train_acl      = []\n",
        "# Y_train_meniscus = []\n",
        "\n",
        "# final_X_valid_axial   = []\n",
        "# final_X_valid_coronal = []\n",
        "# final_X_valid_sagital = []\n",
        "\n",
        "# Y_valid_abnormal = []\n",
        "# Y_valid_acl      = []\n",
        "# Y_valid_meniscus = []\n",
        "\n",
        "\n",
        "# final_X_test_axial  =  []\n",
        "# final_X_test_coronal = []\n",
        "# final_X_test_sagital = []\n",
        "\n",
        "# Y_test_abnormal = []\n",
        "# Y_test_acl      = []\n",
        "# Y_test_meniscus = []\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksteuBpWYalm",
        "colab_type": "text"
      },
      "source": [
        "# CNN - VGG16 Pretrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0KGww12f0Kp",
        "colab_type": "text"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q33KUi6weSFk",
        "colab_type": "code",
        "outputId": "154a9160-15e7-4118-ee7c-710894816100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "####### HELPER Functions #######\n",
        "\n",
        "\n",
        "# Extract features\n",
        "import os, shutil\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 32\n",
        "\n",
        "def extract_features(conv_base, X, Y, sample_count):\n",
        "  \n",
        "    features = np.zeros(shape=(sample_count, 7, 7, 512))  # Must be equal to the output of the convolutional base\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    # Preprocess data\n",
        "    generator = datagen.flow( X,\n",
        "                              Y,\n",
        "                              batch_size = batch_size)\n",
        "    \n",
        "      \n",
        "    # Pass data through convolutional base\n",
        "    i = 0\n",
        "    for inputs_batch, labels_batch in generator:\n",
        "        features_batch = conv_base.predict(inputs_batch)\n",
        "        features[i * batch_size: (i + 1) * batch_size] = features_batch\n",
        "        labels[i * batch_size: (i + 1) * batch_size] = labels_batch\n",
        "        i += 1\n",
        "        if i * batch_size >= sample_count:\n",
        "            break\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ6ZsK9atJR2",
        "colab_type": "text"
      },
      "source": [
        "## 1. Sagittal - Abnormal CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FCjMug5YmR9",
        "colab_type": "code",
        "outputId": "9553637f-bbeb-4de4-ef8d-9c654aebd84c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        }
      },
      "source": [
        "# Instantiate convolutional base\n",
        "from keras.applications import VGG16\n",
        "\n",
        "\n",
        "# include_top as false means that we want to train our own classifier on top of the convolutional base\n",
        "\n",
        "sagital_abnormal_conv_base = VGG16(weights='imagenet', \n",
        "                                  include_top=False,\n",
        "                                  input_shape=(224, 224, 3))  # 3 = number of channels in RGB pictures\n",
        "\n",
        "# Check architecture\n",
        "sagital_abnormal_conv_base.summary()\n",
        "\n",
        "\n",
        "\n",
        "#Extract features\n",
        "\n",
        "#  X-> final_X_train_sagital\n",
        "#  Y - > Y_train_abnormal\n",
        "    \n",
        "sagital_abnormal_train_features, sagital_abnormal_train_labels = extract_features(sagital_abnormal_conv_base, final_X_train_sagital, Y_train_abnormal, final_X_train_sagital.shape[0]) \n",
        "\n",
        "sagital_abnormal_validation_features, sagital_abnormal_validation_labels = extract_features(sagital_abnormal_conv_base, final_X_valid_sagital, Y_valid_abnormal, final_X_valid_sagital.shape[0])\n",
        "\n",
        "sagital_abnormal_test_features, sagital_abnormal_test_labels = extract_features(sagital_abnormal_conv_base, final_X_test_sagital, Y_test_abnormal, final_X_test_sagital.shape[0])\n",
        "\n",
        "print(\"DONE !\")\n",
        "\n",
        "# print(\"Check: \",np.array_equal(Y_train_abnormal,train_labels))\n",
        "\n",
        "# print(\"Labels\")\n",
        "# print(sagital_abnormal_train_labels.shape)\n",
        "# print(sagital_abnormal_train_labels)\n",
        "# print(\"Feautures\")\n",
        "# print(sagital_abnormal_train_features.shape)\n",
        "# print(sagital_abnormal_train_features)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "DONE !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcVrlQ6p03sx",
        "colab_type": "code",
        "outputId": "44ea69cf-9048-454f-88c1-3e34238d2a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Define Dense Model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "sagital_abnormal_model = models.Sequential()\n",
        "sagital_abnormal_model.add(layers.Flatten(input_shape=(7,7,512)))\n",
        "sagital_abnormal_model.add(layers.Dense(256, activation='relu', input_dim=(7*7*512)))\n",
        "sagital_abnormal_model.add(layers.Dense(128, activation='relu'))\n",
        "sagital_abnormal_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "sagital_abnormal_model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 6,455,809\n",
            "Trainable params: 6,455,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNCt3Fgm1BHo",
        "colab_type": "code",
        "outputId": "e99bd1fd-9250-493a-9b5b-f58bcdb4718a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3464
        }
      },
      "source": [
        "# Compile model\n",
        "sagital_abnormal_model.compile(optimizer=optimizers.Adam(),\n",
        "                                loss='binary_crossentropy',\n",
        "                                metrics=['acc'])\n",
        "\n",
        "\n",
        "# Train model\n",
        "sagital_abnormal_history = sagital_abnormal_model.fit(sagital_abnormal_train_features, sagital_abnormal_train_labels,\n",
        "                                      epochs=epochs,\n",
        "                                      batch_size=batch_size,\n",
        "                                      validation_data=(sagital_abnormal_validation_features, sagital_abnormal_validation_labels))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 1130 samples, validate on 24 samples\n",
            "Epoch 1/100\n",
            "1130/1130 [==============================] - 2s 2ms/step - loss: 0.6326 - acc: 0.7673 - val_loss: 0.5740 - val_acc: 0.7917\n",
            "Epoch 2/100\n",
            "1130/1130 [==============================] - 0s 421us/step - loss: 0.4252 - acc: 0.8080 - val_loss: 0.5498 - val_acc: 0.8333\n",
            "Epoch 3/100\n",
            "1130/1130 [==============================] - 0s 410us/step - loss: 0.4001 - acc: 0.8133 - val_loss: 0.5738 - val_acc: 0.7917\n",
            "Epoch 4/100\n",
            "1130/1130 [==============================] - 0s 422us/step - loss: 0.4272 - acc: 0.8071 - val_loss: 0.5899 - val_acc: 0.7917\n",
            "Epoch 5/100\n",
            "1130/1130 [==============================] - 0s 409us/step - loss: 0.4094 - acc: 0.7965 - val_loss: 0.5717 - val_acc: 0.7083\n",
            "Epoch 6/100\n",
            "1130/1130 [==============================] - 0s 436us/step - loss: 0.3452 - acc: 0.8425 - val_loss: 0.8284 - val_acc: 0.7917\n",
            "Epoch 7/100\n",
            "1130/1130 [==============================] - 0s 427us/step - loss: 0.3792 - acc: 0.8248 - val_loss: 0.6300 - val_acc: 0.7500\n",
            "Epoch 8/100\n",
            "1130/1130 [==============================] - 0s 441us/step - loss: 0.3355 - acc: 0.8460 - val_loss: 0.7555 - val_acc: 0.5833\n",
            "Epoch 9/100\n",
            "1130/1130 [==============================] - 1s 453us/step - loss: 0.3765 - acc: 0.8389 - val_loss: 0.6325 - val_acc: 0.7500\n",
            "Epoch 10/100\n",
            "1130/1130 [==============================] - 1s 446us/step - loss: 0.3288 - acc: 0.8540 - val_loss: 0.6705 - val_acc: 0.7500\n",
            "Epoch 11/100\n",
            "1130/1130 [==============================] - 0s 429us/step - loss: 0.3737 - acc: 0.8310 - val_loss: 0.6122 - val_acc: 0.7917\n",
            "Epoch 12/100\n",
            "1130/1130 [==============================] - 1s 444us/step - loss: 0.3242 - acc: 0.8478 - val_loss: 0.7188 - val_acc: 0.7083\n",
            "Epoch 13/100\n",
            "1130/1130 [==============================] - 0s 433us/step - loss: 0.3596 - acc: 0.8319 - val_loss: 0.6498 - val_acc: 0.7083\n",
            "Epoch 14/100\n",
            "1130/1130 [==============================] - 1s 452us/step - loss: 0.2969 - acc: 0.8708 - val_loss: 0.7126 - val_acc: 0.7083\n",
            "Epoch 15/100\n",
            "1130/1130 [==============================] - 1s 449us/step - loss: 0.3223 - acc: 0.8602 - val_loss: 0.7182 - val_acc: 0.6667\n",
            "Epoch 16/100\n",
            "1130/1130 [==============================] - 0s 441us/step - loss: 0.2810 - acc: 0.8841 - val_loss: 0.7124 - val_acc: 0.7500\n",
            "Epoch 17/100\n",
            "1130/1130 [==============================] - 0s 431us/step - loss: 0.2642 - acc: 0.8894 - val_loss: 0.7933 - val_acc: 0.6667\n",
            "Epoch 18/100\n",
            "1130/1130 [==============================] - 0s 433us/step - loss: 0.2671 - acc: 0.8858 - val_loss: 0.7724 - val_acc: 0.7500\n",
            "Epoch 19/100\n",
            "1130/1130 [==============================] - 0s 437us/step - loss: 0.2376 - acc: 0.8947 - val_loss: 0.8544 - val_acc: 0.7500\n",
            "Epoch 20/100\n",
            "1130/1130 [==============================] - 0s 438us/step - loss: 0.2307 - acc: 0.9044 - val_loss: 0.9254 - val_acc: 0.7500\n",
            "Epoch 21/100\n",
            "1130/1130 [==============================] - 0s 441us/step - loss: 0.2744 - acc: 0.8805 - val_loss: 0.8200 - val_acc: 0.6250\n",
            "Epoch 22/100\n",
            "1130/1130 [==============================] - 0s 428us/step - loss: 0.2342 - acc: 0.9000 - val_loss: 0.9498 - val_acc: 0.7500\n",
            "Epoch 23/100\n",
            "1130/1130 [==============================] - 0s 441us/step - loss: 0.1850 - acc: 0.9274 - val_loss: 0.9533 - val_acc: 0.6667\n",
            "Epoch 24/100\n",
            "1130/1130 [==============================] - 0s 433us/step - loss: 0.1813 - acc: 0.9274 - val_loss: 1.0079 - val_acc: 0.7500\n",
            "Epoch 25/100\n",
            "1130/1130 [==============================] - 0s 440us/step - loss: 0.3102 - acc: 0.8708 - val_loss: 0.7903 - val_acc: 0.5833\n",
            "Epoch 26/100\n",
            "1130/1130 [==============================] - 0s 431us/step - loss: 0.2565 - acc: 0.8920 - val_loss: 0.8467 - val_acc: 0.6667\n",
            "Epoch 27/100\n",
            "1130/1130 [==============================] - 0s 429us/step - loss: 0.2321 - acc: 0.8938 - val_loss: 0.8540 - val_acc: 0.7083\n",
            "Epoch 28/100\n",
            "1130/1130 [==============================] - 0s 409us/step - loss: 0.1785 - acc: 0.9310 - val_loss: 0.9655 - val_acc: 0.6250\n",
            "Epoch 29/100\n",
            "1130/1130 [==============================] - 0s 418us/step - loss: 0.1968 - acc: 0.9142 - val_loss: 0.9454 - val_acc: 0.6667\n",
            "Epoch 30/100\n",
            "1130/1130 [==============================] - 0s 409us/step - loss: 0.1350 - acc: 0.9504 - val_loss: 1.0199 - val_acc: 0.6250\n",
            "Epoch 31/100\n",
            "1130/1130 [==============================] - 0s 417us/step - loss: 0.1760 - acc: 0.9310 - val_loss: 1.1217 - val_acc: 0.6250\n",
            "Epoch 32/100\n",
            "1130/1130 [==============================] - 0s 408us/step - loss: 0.1342 - acc: 0.9425 - val_loss: 1.0711 - val_acc: 0.6250\n",
            "Epoch 33/100\n",
            "1130/1130 [==============================] - 0s 402us/step - loss: 0.1290 - acc: 0.9469 - val_loss: 1.8285 - val_acc: 0.5833\n",
            "Epoch 34/100\n",
            "1130/1130 [==============================] - 0s 405us/step - loss: 0.2727 - acc: 0.8779 - val_loss: 0.8432 - val_acc: 0.7083\n",
            "Epoch 35/100\n",
            "1130/1130 [==============================] - 0s 404us/step - loss: 0.1554 - acc: 0.9381 - val_loss: 1.2110 - val_acc: 0.5833\n",
            "Epoch 36/100\n",
            "1130/1130 [==============================] - 0s 403us/step - loss: 0.2179 - acc: 0.9053 - val_loss: 1.0884 - val_acc: 0.6250\n",
            "Epoch 37/100\n",
            "1130/1130 [==============================] - 0s 402us/step - loss: 0.1666 - acc: 0.9221 - val_loss: 1.0619 - val_acc: 0.6250\n",
            "Epoch 38/100\n",
            "1130/1130 [==============================] - 0s 407us/step - loss: 0.1426 - acc: 0.9496 - val_loss: 1.1146 - val_acc: 0.7083\n",
            "Epoch 39/100\n",
            "1130/1130 [==============================] - 0s 424us/step - loss: 0.0963 - acc: 0.9673 - val_loss: 1.1597 - val_acc: 0.6250\n",
            "Epoch 40/100\n",
            "1130/1130 [==============================] - 0s 432us/step - loss: 0.0846 - acc: 0.9735 - val_loss: 1.4074 - val_acc: 0.5833\n",
            "Epoch 41/100\n",
            "1130/1130 [==============================] - 0s 427us/step - loss: 0.2230 - acc: 0.9177 - val_loss: 1.0616 - val_acc: 0.6250\n",
            "Epoch 42/100\n",
            "1130/1130 [==============================] - 0s 434us/step - loss: 0.0867 - acc: 0.9708 - val_loss: 1.2457 - val_acc: 0.7083\n",
            "Epoch 43/100\n",
            "1130/1130 [==============================] - 0s 427us/step - loss: 0.1728 - acc: 0.9221 - val_loss: 1.2191 - val_acc: 0.6667\n",
            "Epoch 44/100\n",
            "1130/1130 [==============================] - 0s 437us/step - loss: 0.1111 - acc: 0.9540 - val_loss: 1.1927 - val_acc: 0.7083\n",
            "Epoch 45/100\n",
            "1130/1130 [==============================] - 0s 440us/step - loss: 0.0769 - acc: 0.9752 - val_loss: 1.2778 - val_acc: 0.7083\n",
            "Epoch 46/100\n",
            "1130/1130 [==============================] - 0s 434us/step - loss: 0.0595 - acc: 0.9832 - val_loss: 1.3345 - val_acc: 0.6250\n",
            "Epoch 47/100\n",
            "1130/1130 [==============================] - 0s 426us/step - loss: 0.0614 - acc: 0.9788 - val_loss: 1.4160 - val_acc: 0.7083\n",
            "Epoch 48/100\n",
            "1130/1130 [==============================] - 0s 424us/step - loss: 0.0749 - acc: 0.9690 - val_loss: 1.5694 - val_acc: 0.5833\n",
            "Epoch 49/100\n",
            "1130/1130 [==============================] - 1s 450us/step - loss: 0.0866 - acc: 0.9628 - val_loss: 1.8193 - val_acc: 0.6667\n",
            "Epoch 50/100\n",
            "1130/1130 [==============================] - 0s 429us/step - loss: 0.1075 - acc: 0.9513 - val_loss: 1.5098 - val_acc: 0.6667\n",
            "Epoch 51/100\n",
            "1130/1130 [==============================] - 0s 435us/step - loss: 0.2303 - acc: 0.9035 - val_loss: 1.4375 - val_acc: 0.6250\n",
            "Epoch 52/100\n",
            "1130/1130 [==============================] - 0s 437us/step - loss: 0.1058 - acc: 0.9655 - val_loss: 1.3784 - val_acc: 0.6250\n",
            "Epoch 53/100\n",
            "1130/1130 [==============================] - 1s 446us/step - loss: 0.0484 - acc: 0.9841 - val_loss: 1.4148 - val_acc: 0.7083\n",
            "Epoch 54/100\n",
            "1130/1130 [==============================] - 0s 422us/step - loss: 0.1080 - acc: 0.9513 - val_loss: 1.3972 - val_acc: 0.6667\n",
            "Epoch 55/100\n",
            "1130/1130 [==============================] - 0s 433us/step - loss: 0.0323 - acc: 0.9956 - val_loss: 1.4806 - val_acc: 0.6250\n",
            "Epoch 56/100\n",
            "1130/1130 [==============================] - 0s 429us/step - loss: 0.0591 - acc: 0.9805 - val_loss: 1.5935 - val_acc: 0.7500\n",
            "Epoch 57/100\n",
            "1130/1130 [==============================] - 0s 441us/step - loss: 0.0304 - acc: 0.9938 - val_loss: 1.5229 - val_acc: 0.6667\n",
            "Epoch 58/100\n",
            "1130/1130 [==============================] - 0s 428us/step - loss: 0.0225 - acc: 0.9982 - val_loss: 1.6024 - val_acc: 0.7083\n",
            "Epoch 59/100\n",
            "1130/1130 [==============================] - 0s 430us/step - loss: 0.0191 - acc: 0.9973 - val_loss: 1.6127 - val_acc: 0.6667\n",
            "Epoch 60/100\n",
            "1130/1130 [==============================] - 0s 427us/step - loss: 0.0535 - acc: 0.9761 - val_loss: 1.7399 - val_acc: 0.6667\n",
            "Epoch 61/100\n",
            "1130/1130 [==============================] - 0s 433us/step - loss: 0.0179 - acc: 0.9982 - val_loss: 1.7460 - val_acc: 0.6250\n",
            "Epoch 62/100\n",
            "1130/1130 [==============================] - 0s 425us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 1.7576 - val_acc: 0.6667\n",
            "Epoch 63/100\n",
            "1130/1130 [==============================] - 0s 434us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 1.7913 - val_acc: 0.6667\n",
            "Epoch 64/100\n",
            "1130/1130 [==============================] - 0s 427us/step - loss: 0.0971 - acc: 0.9611 - val_loss: 2.3666 - val_acc: 0.7083\n",
            "Epoch 65/100\n",
            "1130/1130 [==============================] - 1s 453us/step - loss: 0.2842 - acc: 0.9044 - val_loss: 1.4086 - val_acc: 0.5833\n",
            "Epoch 66/100\n",
            "1130/1130 [==============================] - 0s 427us/step - loss: 0.0359 - acc: 0.9920 - val_loss: 1.7160 - val_acc: 0.5833\n",
            "Epoch 67/100\n",
            "1130/1130 [==============================] - 0s 434us/step - loss: 0.0233 - acc: 0.9973 - val_loss: 1.6967 - val_acc: 0.6667\n",
            "Epoch 68/100\n",
            "1130/1130 [==============================] - 0s 406us/step - loss: 0.0369 - acc: 0.9850 - val_loss: 2.2029 - val_acc: 0.6667\n",
            "Epoch 69/100\n",
            "1130/1130 [==============================] - 0s 384us/step - loss: 0.0384 - acc: 0.9858 - val_loss: 1.7983 - val_acc: 0.5833\n",
            "Epoch 70/100\n",
            "1130/1130 [==============================] - 0s 419us/step - loss: 0.0142 - acc: 0.9991 - val_loss: 1.7758 - val_acc: 0.6667\n",
            "Epoch 71/100\n",
            "1130/1130 [==============================] - 0s 395us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 1.8210 - val_acc: 0.6667\n",
            "Epoch 72/100\n",
            "1130/1130 [==============================] - 0s 403us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 1.8456 - val_acc: 0.6667\n",
            "Epoch 73/100\n",
            "1130/1130 [==============================] - 0s 395us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 1.8705 - val_acc: 0.6667\n",
            "Epoch 74/100\n",
            "1130/1130 [==============================] - 0s 416us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 1.9102 - val_acc: 0.7083\n",
            "Epoch 75/100\n",
            "1130/1130 [==============================] - 0s 396us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 1.9210 - val_acc: 0.6667\n",
            "Epoch 76/100\n",
            "1130/1130 [==============================] - 0s 409us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 1.9510 - val_acc: 0.6667\n",
            "Epoch 77/100\n",
            "1130/1130 [==============================] - 0s 394us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 1.9742 - val_acc: 0.6667\n",
            "Epoch 78/100\n",
            "1130/1130 [==============================] - 0s 418us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 1.9968 - val_acc: 0.6667\n",
            "Epoch 79/100\n",
            "1130/1130 [==============================] - 0s 425us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 2.0225 - val_acc: 0.6667\n",
            "Epoch 80/100\n",
            "1130/1130 [==============================] - 0s 392us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 2.0323 - val_acc: 0.6667\n",
            "Epoch 81/100\n",
            "1130/1130 [==============================] - 0s 413us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 2.0613 - val_acc: 0.6667\n",
            "Epoch 82/100\n",
            "1130/1130 [==============================] - 0s 393us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 2.0772 - val_acc: 0.6667\n",
            "Epoch 83/100\n",
            "1130/1130 [==============================] - 0s 411us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 2.1100 - val_acc: 0.6667\n",
            "Epoch 84/100\n",
            "1130/1130 [==============================] - 0s 393us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 2.1369 - val_acc: 0.7083\n",
            "Epoch 85/100\n",
            "1130/1130 [==============================] - 0s 403us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 2.1658 - val_acc: 0.6667\n",
            "Epoch 86/100\n",
            "1130/1130 [==============================] - 0s 413us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 2.1744 - val_acc: 0.6667\n",
            "Epoch 87/100\n",
            "1130/1130 [==============================] - 0s 429us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 2.1907 - val_acc: 0.6667\n",
            "Epoch 88/100\n",
            "1130/1130 [==============================] - 0s 422us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 2.1960 - val_acc: 0.6667\n",
            "Epoch 89/100\n",
            "1130/1130 [==============================] - 0s 421us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 2.2170 - val_acc: 0.7083\n",
            "Epoch 90/100\n",
            "1130/1130 [==============================] - 0s 428us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 2.2265 - val_acc: 0.6667\n",
            "Epoch 91/100\n",
            "1130/1130 [==============================] - 0s 414us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 2.2380 - val_acc: 0.7083\n",
            "Epoch 92/100\n",
            "1130/1130 [==============================] - 0s 431us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.2412 - val_acc: 0.6667\n",
            "Epoch 93/100\n",
            "1130/1130 [==============================] - 0s 425us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 2.2424 - val_acc: 0.7083\n",
            "Epoch 94/100\n",
            "1130/1130 [==============================] - 0s 434us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 2.2699 - val_acc: 0.6667\n",
            "Epoch 95/100\n",
            "1130/1130 [==============================] - 0s 441us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.2811 - val_acc: 0.6667\n",
            "Epoch 96/100\n",
            "1130/1130 [==============================] - 1s 444us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.2882 - val_acc: 0.6667\n",
            "Epoch 97/100\n",
            "1130/1130 [==============================] - 0s 441us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.2975 - val_acc: 0.6667\n",
            "Epoch 98/100\n",
            "1130/1130 [==============================] - 0s 416us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 2.3147 - val_acc: 0.6667\n",
            "Epoch 99/100\n",
            "1130/1130 [==============================] - 0s 415us/step - loss: 9.9761e-04 - acc: 1.0000 - val_loss: 2.3143 - val_acc: 0.7083\n",
            "Epoch 100/100\n",
            "1130/1130 [==============================] - 0s 419us/step - loss: 9.2450e-04 - acc: 1.0000 - val_loss: 2.3273 - val_acc: 0.6667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufpw7P-5ymjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save model\n",
        "sagital_abnormal_model.save('sagital_abnormal.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGuOOBk_1f15",
        "colab_type": "code",
        "outputId": "ac24076f-f4e9-4aee-b0db-ef5590f43e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Evaluate the model on Validation Set -- Test Accuracy\n",
        "\n",
        "sagital_abnormal_scores = sagital_abnormal_model.evaluate(sagital_abnormal_validation_features, sagital_abnormal_validation_labels)\n",
        "print(\"\\n%s: %.2f%%\" % (sagital_abnormal_model.metrics_names[1], sagital_abnormal_scores[1]*100))\n",
        "\n",
        "# print(\"Predictions\")\n",
        "# sagital_abnormal_pred = sagital_abnormal_model.predict_proba(validation_features)\n",
        "# for e in pred:\n",
        "#   print(e*100)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 0s 232us/step\n",
            "\n",
            "acc: 85.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFAHur8xuISV",
        "colab_type": "text"
      },
      "source": [
        "## 2. Sagittal - ACL CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a458c619-67d9-4b92-d30b-167f08a63322",
        "id": "qEitNwFqPmM0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "# Instantiate convolutional base\n",
        "from keras.applications import VGG16\n",
        "\n",
        "\n",
        "# include_top as false means that we want to train our own classifier on top of the convolutional base\n",
        "\n",
        "sagital_acl_conv_base = VGG16(weights='imagenet', \n",
        "                  include_top=False,\n",
        "                  input_shape=(224, 224, 3))  # 3 = number of channels in RGB pictures\n",
        "\n",
        "# Check architecture\n",
        "sagital_acl_conv_base.summary()\n",
        "\n",
        "\n",
        "#Extract Features\n",
        "\n",
        "#  X-> final_X_train_sagital\n",
        "#  Y - > Y_train_acl\n",
        "    \n",
        "\n",
        "sagital_acl_train_features, sagital_acl_train_labels = extract_features(sagital_acl_conv_base,final_X_train_sagital, Y_train_acl, final_X_train_sagital.shape[0])  \n",
        "\n",
        "sagital_acl_validation_features, sagital_acl_validation_labels = extract_features(sagital_acl_conv_base,final_X_valid_sagital, Y_valid_acl, final_X_valid_sagital.shape[0])\n",
        "\n",
        "sagital_acl_test_features, sagital_acl_test_labels = extract_features(sagital_acl_conv_base,final_X_test_sagital, Y_test_acl, final_X_test_sagital.shape[0])\n",
        "\n",
        "print(\"DONE !\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "DONE !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "622c8044-34dd-4ff9-c5d8-cf06c0cf87cb",
        "id": "je4odtlwPmM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Define model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "sagital_acl_model = models.Sequential()\n",
        "sagital_acl_model.add(layers.Flatten(input_shape=(7,7,512)))\n",
        "sagital_acl_model.add(layers.Dense(256, activation='relu', input_dim=(7*7*512)))\n",
        "sagital_acl_model.add(layers.Dense(128, activation='relu'))\n",
        "sagital_acl_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "sagital_acl_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 6,455,809\n",
            "Trainable params: 6,455,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8a05d85a-7aca-49e5-b67c-38720a489363",
        "id": "gA3IiEPlPmNC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3394
        }
      },
      "source": [
        "# Compile model\n",
        "sagital_acl_model.compile(optimizer=optimizers.Adam(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "# Train model\n",
        "sagital_acl_history = sagital_acl_model.fit(sagital_acl_train_features, sagital_acl_train_labels,\n",
        "                                      epochs=epochs,\n",
        "                                      batch_size=batch_size,\n",
        "                                      validation_data=(sagital_acl_validation_features, sagital_acl_validation_labels))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1130 samples, validate on 24 samples\n",
            "Epoch 1/100\n",
            "1130/1130 [==============================] - 1s 866us/step - loss: 0.6032 - acc: 0.7885 - val_loss: 0.7455 - val_acc: 0.5833\n",
            "Epoch 2/100\n",
            "1130/1130 [==============================] - 0s 351us/step - loss: 0.4857 - acc: 0.8062 - val_loss: 0.9251 - val_acc: 0.5833\n",
            "Epoch 3/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.4486 - acc: 0.8212 - val_loss: 0.9898 - val_acc: 0.5833\n",
            "Epoch 4/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.4133 - acc: 0.8354 - val_loss: 0.8859 - val_acc: 0.5417\n",
            "Epoch 5/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.4050 - acc: 0.8416 - val_loss: 1.1953 - val_acc: 0.5833\n",
            "Epoch 6/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.4057 - acc: 0.8283 - val_loss: 0.8335 - val_acc: 0.5000\n",
            "Epoch 7/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.3835 - acc: 0.8363 - val_loss: 1.1088 - val_acc: 0.5833\n",
            "Epoch 8/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.3876 - acc: 0.8345 - val_loss: 0.9540 - val_acc: 0.5000\n",
            "Epoch 9/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.3779 - acc: 0.8442 - val_loss: 0.8013 - val_acc: 0.5833\n",
            "Epoch 10/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.3234 - acc: 0.8646 - val_loss: 0.8615 - val_acc: 0.6250\n",
            "Epoch 11/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.3748 - acc: 0.8496 - val_loss: 1.0038 - val_acc: 0.5833\n",
            "Epoch 12/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.3089 - acc: 0.8717 - val_loss: 1.0196 - val_acc: 0.6250\n",
            "Epoch 13/100\n",
            "1130/1130 [==============================] - 0s 346us/step - loss: 0.3131 - acc: 0.8735 - val_loss: 0.9060 - val_acc: 0.5833\n",
            "Epoch 14/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.2672 - acc: 0.8805 - val_loss: 1.2723 - val_acc: 0.5417\n",
            "Epoch 15/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.2827 - acc: 0.8805 - val_loss: 1.0645 - val_acc: 0.4167\n",
            "Epoch 16/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.3673 - acc: 0.8389 - val_loss: 1.1388 - val_acc: 0.5833\n",
            "Epoch 17/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.2455 - acc: 0.8982 - val_loss: 1.7362 - val_acc: 0.5833\n",
            "Epoch 18/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.3208 - acc: 0.8549 - val_loss: 0.9735 - val_acc: 0.5000\n",
            "Epoch 19/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.2311 - acc: 0.9035 - val_loss: 1.1218 - val_acc: 0.4167\n",
            "Epoch 20/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.3046 - acc: 0.8770 - val_loss: 1.6484 - val_acc: 0.5833\n",
            "Epoch 21/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.2038 - acc: 0.9195 - val_loss: 1.0690 - val_acc: 0.5000\n",
            "Epoch 22/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.2583 - acc: 0.8947 - val_loss: 1.4505 - val_acc: 0.5417\n",
            "Epoch 23/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.1845 - acc: 0.9319 - val_loss: 1.1777 - val_acc: 0.5000\n",
            "Epoch 24/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.2440 - acc: 0.9000 - val_loss: 1.3074 - val_acc: 0.5833\n",
            "Epoch 25/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.2037 - acc: 0.9195 - val_loss: 1.5997 - val_acc: 0.5833\n",
            "Epoch 26/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.1722 - acc: 0.9327 - val_loss: 1.2112 - val_acc: 0.5000\n",
            "Epoch 27/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.3430 - acc: 0.8690 - val_loss: 1.5437 - val_acc: 0.5833\n",
            "Epoch 28/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.1847 - acc: 0.9239 - val_loss: 1.1572 - val_acc: 0.5000\n",
            "Epoch 29/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.2029 - acc: 0.9195 - val_loss: 1.3898 - val_acc: 0.5417\n",
            "Epoch 30/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.1144 - acc: 0.9655 - val_loss: 1.4759 - val_acc: 0.5417\n",
            "Epoch 31/100\n",
            "1130/1130 [==============================] - 0s 326us/step - loss: 0.2003 - acc: 0.9212 - val_loss: 1.6818 - val_acc: 0.5417\n",
            "Epoch 32/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.1053 - acc: 0.9655 - val_loss: 1.9501 - val_acc: 0.5417\n",
            "Epoch 33/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0839 - acc: 0.9788 - val_loss: 2.1472 - val_acc: 0.5417\n",
            "Epoch 34/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.2191 - acc: 0.9124 - val_loss: 3.2395 - val_acc: 0.5833\n",
            "Epoch 35/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.1062 - acc: 0.9664 - val_loss: 1.4870 - val_acc: 0.5000\n",
            "Epoch 36/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.1449 - acc: 0.9434 - val_loss: 1.7185 - val_acc: 0.5000\n",
            "Epoch 37/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.0915 - acc: 0.9673 - val_loss: 1.9504 - val_acc: 0.5417\n",
            "Epoch 38/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0838 - acc: 0.9735 - val_loss: 1.5091 - val_acc: 0.5000\n",
            "Epoch 39/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0723 - acc: 0.9770 - val_loss: 1.9703 - val_acc: 0.5000\n",
            "Epoch 40/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.1198 - acc: 0.9504 - val_loss: 2.9639 - val_acc: 0.5833\n",
            "Epoch 41/100\n",
            "1130/1130 [==============================] - 0s 325us/step - loss: 0.0745 - acc: 0.9752 - val_loss: 3.0516 - val_acc: 0.5833\n",
            "Epoch 42/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0728 - acc: 0.9761 - val_loss: 1.8079 - val_acc: 0.4583\n",
            "Epoch 43/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0409 - acc: 0.9876 - val_loss: 2.0637 - val_acc: 0.5000\n",
            "Epoch 44/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.0695 - acc: 0.9717 - val_loss: 2.8631 - val_acc: 0.5417\n",
            "Epoch 45/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.0267 - acc: 0.9965 - val_loss: 3.0829 - val_acc: 0.5833\n",
            "Epoch 46/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0328 - acc: 0.9920 - val_loss: 1.9356 - val_acc: 0.5000\n",
            "Epoch 47/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.3227 - acc: 0.9000 - val_loss: 1.5333 - val_acc: 0.5417\n",
            "Epoch 48/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0830 - acc: 0.9823 - val_loss: 2.7866 - val_acc: 0.5833\n",
            "Epoch 49/100\n",
            "1130/1130 [==============================] - 0s 325us/step - loss: 0.0549 - acc: 0.9841 - val_loss: 2.4818 - val_acc: 0.5833\n",
            "Epoch 50/100\n",
            "1130/1130 [==============================] - 0s 354us/step - loss: 0.0450 - acc: 0.9885 - val_loss: 3.0477 - val_acc: 0.5833\n",
            "Epoch 51/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.0307 - acc: 0.9947 - val_loss: 2.1056 - val_acc: 0.5000\n",
            "Epoch 52/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.0420 - acc: 0.9876 - val_loss: 2.7518 - val_acc: 0.5833\n",
            "Epoch 53/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0366 - acc: 0.9876 - val_loss: 2.4293 - val_acc: 0.5417\n",
            "Epoch 54/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0539 - acc: 0.9805 - val_loss: 2.8053 - val_acc: 0.5417\n",
            "Epoch 55/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0371 - acc: 0.9876 - val_loss: 3.1534 - val_acc: 0.5833\n",
            "Epoch 56/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0154 - acc: 0.9973 - val_loss: 3.0702 - val_acc: 0.5833\n",
            "Epoch 57/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0226 - acc: 0.9938 - val_loss: 2.8931 - val_acc: 0.5417\n",
            "Epoch 58/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 3.3962 - val_acc: 0.5833\n",
            "Epoch 59/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.0188 - acc: 0.9991 - val_loss: 3.1796 - val_acc: 0.5833\n",
            "Epoch 60/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.3742 - val_acc: 0.5833\n",
            "Epoch 61/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.4251 - val_acc: 0.5417\n",
            "Epoch 62/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.2972 - acc: 0.9336 - val_loss: 1.7473 - val_acc: 0.5833\n",
            "Epoch 63/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0840 - acc: 0.9841 - val_loss: 1.7951 - val_acc: 0.5417\n",
            "Epoch 64/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0337 - acc: 0.9991 - val_loss: 2.0095 - val_acc: 0.5417\n",
            "Epoch 65/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0321 - acc: 0.9965 - val_loss: 2.4502 - val_acc: 0.5417\n",
            "Epoch 66/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.3246 - acc: 0.9044 - val_loss: 1.5346 - val_acc: 0.5000\n",
            "Epoch 67/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0663 - acc: 0.9814 - val_loss: 1.7877 - val_acc: 0.5000\n",
            "Epoch 68/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0359 - acc: 0.9938 - val_loss: 2.4107 - val_acc: 0.5833\n",
            "Epoch 69/100\n",
            "1130/1130 [==============================] - 0s 326us/step - loss: 0.0182 - acc: 0.9991 - val_loss: 2.6737 - val_acc: 0.5833\n",
            "Epoch 70/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 2.7949 - val_acc: 0.5833\n",
            "Epoch 71/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 2.5910 - val_acc: 0.5417\n",
            "Epoch 72/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.2782 - acc: 0.9310 - val_loss: 4.7651 - val_acc: 0.5833\n",
            "Epoch 73/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.2197 - acc: 0.9230 - val_loss: 2.2175 - val_acc: 0.5833\n",
            "Epoch 74/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.0793 - acc: 0.9726 - val_loss: 2.5409 - val_acc: 0.5833\n",
            "Epoch 75/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0343 - acc: 0.9965 - val_loss: 2.2656 - val_acc: 0.5417\n",
            "Epoch 76/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0216 - acc: 1.0000 - val_loss: 2.7588 - val_acc: 0.5833\n",
            "Epoch 77/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0204 - acc: 1.0000 - val_loss: 2.4786 - val_acc: 0.5417\n",
            "Epoch 78/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0131 - acc: 1.0000 - val_loss: 3.1653 - val_acc: 0.5833\n",
            "Epoch 79/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 2.6832 - val_acc: 0.5417\n",
            "Epoch 80/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 2.3535 - val_acc: 0.5000\n",
            "Epoch 81/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 2.7186 - val_acc: 0.5417\n",
            "Epoch 82/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 3.1910 - val_acc: 0.5417\n",
            "Epoch 83/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.0365 - val_acc: 0.5417\n",
            "Epoch 84/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.1504 - val_acc: 0.5417\n",
            "Epoch 85/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 2.9070 - val_acc: 0.5417\n",
            "Epoch 86/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 4.0716 - val_acc: 0.5833\n",
            "Epoch 87/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.2067 - val_acc: 0.5417\n",
            "Epoch 88/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 3.2249 - val_acc: 0.5417\n",
            "Epoch 89/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 3.6277 - val_acc: 0.5417\n",
            "Epoch 90/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 3.4974 - val_acc: 0.5417\n",
            "Epoch 91/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 3.5481 - val_acc: 0.5417\n",
            "Epoch 92/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 3.6716 - val_acc: 0.5417\n",
            "Epoch 93/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 3.8607 - val_acc: 0.5833\n",
            "Epoch 94/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 3.3151 - val_acc: 0.5417\n",
            "Epoch 95/100\n",
            "1130/1130 [==============================] - 0s 326us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 3.7545 - val_acc: 0.5417\n",
            "Epoch 96/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 4.0555 - val_acc: 0.5833\n",
            "Epoch 97/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 3.7539 - val_acc: 0.5417\n",
            "Epoch 98/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 3.8700 - val_acc: 0.5417\n",
            "Epoch 99/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 3.9953 - val_acc: 0.5417\n",
            "Epoch 100/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 3.8914 - val_acc: 0.5417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MmXLidoQPmNF",
        "colab": {}
      },
      "source": [
        "# Save model\n",
        "model.save('sagital_acl.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "37cd80cf-0f9f-4233-9151-de003ea68ea4",
        "id": "Z8-5HyIRPmNH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Evaluate the model on Test Set\n",
        "sagital_acl_scores = sagital_acl_model.evaluate(sagital_acl_validation_features, sagital_acl_validation_labels)\n",
        "print(\"\\n%s: %.2f%%\" % (sagital_acl_model.metrics_names[1], sagital_acl_scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 0s 3ms/step\n",
            "\n",
            "acc: 59.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmdHuLKfuPn8",
        "colab_type": "text"
      },
      "source": [
        "## 3. Sagittal - Meniscus CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "640cda63-2185-479b-9bb7-b1a9ce2208c3",
        "id": "VBlJG2-1Ppkd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "# Instantiate convolutional base\n",
        "from keras.applications import VGG16\n",
        "\n",
        "\n",
        "# include_top as false means that we want to train our own classifier on top of the convolutional base\n",
        "\n",
        "sagital_meniscus_conv_base = VGG16(weights='imagenet', \n",
        "                                  include_top=False,\n",
        "                                  input_shape=(224, 224, 3))  # 3 = number of channels in RGB pictures\n",
        "\n",
        "# Check architecture\n",
        "sagital_meniscus_conv_base.summary()\n",
        "\n",
        "\n",
        "##Extract features\n",
        "\n",
        "#  X-> final_X_train_sagital\n",
        "#  Y - > Y_train_abnormal\n",
        "    \n",
        "sagital_meniscus_train_features, sagital_meniscus_train_labels = extract_features(sagital_meniscus_conv_base,final_X_train_sagital, Y_train_meniscus, final_X_train_sagital.shape[0])  # Agree with our small dataset size\n",
        "\n",
        "sagital_meniscus_validation_features, sagital_meniscus_validation_labels = extract_features(sagital_meniscus_conv_base,final_X_valid_sagital, Y_valid_meniscus, final_X_valid_sagital.shape[0])\n",
        "\n",
        "sagital_meniscus_test_features, sagital_test_validation_labels = extract_features(sagital_meniscus_conv_base,final_X_test_sagital, Y_test_meniscus, final_X_test_sagital.shape[0])\n",
        "\n",
        "print(\"DONE !\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "DONE !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6b9d0c81-cfee-459b-8a26-df784af056f4",
        "id": "vF7zD2LJPpkw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Define model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "sagital_meniscus_model = models.Sequential()\n",
        "sagital_meniscus_model.add(layers.Flatten(input_shape=(7,7,512)))\n",
        "sagital_meniscus_model.add(layers.Dense(256, activation='relu', input_dim=(7*7*512)))\n",
        "sagital_meniscus_model.add(layers.Dense(128, activation='relu'))\n",
        "sagital_meniscus_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "sagital_meniscus_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_3 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 6,455,809\n",
            "Trainable params: 6,455,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "15e45cb4-7e83-441f-c1e8-70ad77331735",
        "id": "vSAvsGtBPpk3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3394
        }
      },
      "source": [
        "# Compile model\n",
        "sagital_meniscus_model.compile(optimizer=optimizers.Adam(),\n",
        "                                loss='binary_crossentropy',\n",
        "                                metrics=['acc'])\n",
        "\n",
        "\n",
        "# Train model\n",
        "sagital_meniscus_history = sagital_meniscus_model.fit(sagital_meniscus_train_features, sagital_meniscus_train_labels,\n",
        "                                                      epochs=epochs,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      validation_data=(sagital_meniscus_validation_features, sagital_meniscus_validation_labels))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1130 samples, validate on 24 samples\n",
            "Epoch 1/100\n",
            "1130/1130 [==============================] - 1s 973us/step - loss: 0.7803 - acc: 0.5761 - val_loss: 0.6338 - val_acc: 0.7500\n",
            "Epoch 2/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.6455 - acc: 0.6442 - val_loss: 0.6216 - val_acc: 0.6667\n",
            "Epoch 3/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.6385 - acc: 0.6646 - val_loss: 0.6925 - val_acc: 0.5833\n",
            "Epoch 4/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.6089 - acc: 0.6690 - val_loss: 0.6617 - val_acc: 0.6250\n",
            "Epoch 5/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.6067 - acc: 0.6690 - val_loss: 0.7253 - val_acc: 0.5417\n",
            "Epoch 6/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.5874 - acc: 0.6832 - val_loss: 0.7173 - val_acc: 0.5833\n",
            "Epoch 7/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.5740 - acc: 0.7062 - val_loss: 0.6603 - val_acc: 0.6250\n",
            "Epoch 8/100\n",
            "1130/1130 [==============================] - 0s 354us/step - loss: 0.5412 - acc: 0.7274 - val_loss: 0.9099 - val_acc: 0.5417\n",
            "Epoch 9/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.5463 - acc: 0.7159 - val_loss: 0.7898 - val_acc: 0.5833\n",
            "Epoch 10/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.5837 - acc: 0.6894 - val_loss: 0.6130 - val_acc: 0.6250\n",
            "Epoch 11/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.5207 - acc: 0.7345 - val_loss: 0.7354 - val_acc: 0.6250\n",
            "Epoch 12/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.4964 - acc: 0.7513 - val_loss: 0.6023 - val_acc: 0.6667\n",
            "Epoch 13/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.5129 - acc: 0.7354 - val_loss: 0.6459 - val_acc: 0.6250\n",
            "Epoch 14/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.5704 - acc: 0.7027 - val_loss: 0.6125 - val_acc: 0.6667\n",
            "Epoch 15/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.4690 - acc: 0.7752 - val_loss: 0.6561 - val_acc: 0.6250\n",
            "Epoch 16/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.5270 - acc: 0.7381 - val_loss: 0.8381 - val_acc: 0.5833\n",
            "Epoch 17/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.4769 - acc: 0.7761 - val_loss: 0.8890 - val_acc: 0.5833\n",
            "Epoch 18/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.4753 - acc: 0.7717 - val_loss: 0.6841 - val_acc: 0.5417\n",
            "Epoch 19/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.4724 - acc: 0.7752 - val_loss: 0.6811 - val_acc: 0.5833\n",
            "Epoch 20/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.3932 - acc: 0.8186 - val_loss: 0.7724 - val_acc: 0.5833\n",
            "Epoch 21/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.3823 - acc: 0.8345 - val_loss: 0.7108 - val_acc: 0.6250\n",
            "Epoch 22/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.4214 - acc: 0.7894 - val_loss: 0.6869 - val_acc: 0.7083\n",
            "Epoch 23/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.3373 - acc: 0.8451 - val_loss: 0.7580 - val_acc: 0.6250\n",
            "Epoch 24/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.3228 - acc: 0.8549 - val_loss: 0.8487 - val_acc: 0.6667\n",
            "Epoch 25/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.3308 - acc: 0.8575 - val_loss: 0.9434 - val_acc: 0.5833\n",
            "Epoch 26/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.3372 - acc: 0.8540 - val_loss: 1.5414 - val_acc: 0.5417\n",
            "Epoch 27/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.3287 - acc: 0.8584 - val_loss: 0.7370 - val_acc: 0.6250\n",
            "Epoch 28/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.3044 - acc: 0.8761 - val_loss: 0.7518 - val_acc: 0.7083\n",
            "Epoch 29/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.2637 - acc: 0.8982 - val_loss: 0.9574 - val_acc: 0.5833\n",
            "Epoch 30/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.2696 - acc: 0.8841 - val_loss: 0.8253 - val_acc: 0.7083\n",
            "Epoch 31/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.2769 - acc: 0.8805 - val_loss: 0.7907 - val_acc: 0.6250\n",
            "Epoch 32/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.2440 - acc: 0.8973 - val_loss: 0.8012 - val_acc: 0.7500\n",
            "Epoch 33/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.2247 - acc: 0.9053 - val_loss: 0.9443 - val_acc: 0.6667\n",
            "Epoch 34/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.2246 - acc: 0.9133 - val_loss: 0.8911 - val_acc: 0.6667\n",
            "Epoch 35/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.1565 - acc: 0.9531 - val_loss: 1.0109 - val_acc: 0.6667\n",
            "Epoch 36/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.2056 - acc: 0.9133 - val_loss: 1.5925 - val_acc: 0.5833\n",
            "Epoch 37/100\n",
            "1130/1130 [==============================] - 0s 322us/step - loss: 0.4508 - acc: 0.7991 - val_loss: 0.7697 - val_acc: 0.5417\n",
            "Epoch 38/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.2334 - acc: 0.9159 - val_loss: 0.9065 - val_acc: 0.6667\n",
            "Epoch 39/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.1779 - acc: 0.9257 - val_loss: 1.1146 - val_acc: 0.6667\n",
            "Epoch 40/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.1715 - acc: 0.9310 - val_loss: 0.9904 - val_acc: 0.5833\n",
            "Epoch 41/100\n",
            "1130/1130 [==============================] - 0s 323us/step - loss: 0.1253 - acc: 0.9602 - val_loss: 1.2299 - val_acc: 0.5417\n",
            "Epoch 42/100\n",
            "1130/1130 [==============================] - 0s 326us/step - loss: 0.2188 - acc: 0.9062 - val_loss: 1.7942 - val_acc: 0.5833\n",
            "Epoch 43/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.3368 - acc: 0.8460 - val_loss: 1.0303 - val_acc: 0.5000\n",
            "Epoch 44/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.1738 - acc: 0.9336 - val_loss: 1.1701 - val_acc: 0.5417\n",
            "Epoch 45/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.1379 - acc: 0.9522 - val_loss: 1.5579 - val_acc: 0.5833\n",
            "Epoch 46/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.1199 - acc: 0.9584 - val_loss: 1.1847 - val_acc: 0.5417\n",
            "Epoch 47/100\n",
            "1130/1130 [==============================] - 0s 326us/step - loss: 0.0971 - acc: 0.9699 - val_loss: 1.2723 - val_acc: 0.4167\n",
            "Epoch 48/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0845 - acc: 0.9770 - val_loss: 1.2616 - val_acc: 0.4583\n",
            "Epoch 49/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0881 - acc: 0.9770 - val_loss: 1.3586 - val_acc: 0.4583\n",
            "Epoch 50/100\n",
            "1130/1130 [==============================] - 0s 325us/step - loss: 0.1189 - acc: 0.9558 - val_loss: 1.5994 - val_acc: 0.5000\n",
            "Epoch 51/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0770 - acc: 0.9832 - val_loss: 1.3429 - val_acc: 0.5000\n",
            "Epoch 52/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.1156 - acc: 0.9478 - val_loss: 1.6115 - val_acc: 0.5833\n",
            "Epoch 53/100\n",
            "1130/1130 [==============================] - 0s 325us/step - loss: 0.1458 - acc: 0.9372 - val_loss: 1.4123 - val_acc: 0.5000\n",
            "Epoch 54/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.1147 - acc: 0.9540 - val_loss: 1.4371 - val_acc: 0.5417\n",
            "Epoch 55/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0730 - acc: 0.9770 - val_loss: 1.6782 - val_acc: 0.4167\n",
            "Epoch 56/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0911 - acc: 0.9655 - val_loss: 1.8724 - val_acc: 0.5417\n",
            "Epoch 57/100\n",
            "1130/1130 [==============================] - 0s 346us/step - loss: 0.1291 - acc: 0.9425 - val_loss: 1.5764 - val_acc: 0.4583\n",
            "Epoch 58/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0836 - acc: 0.9646 - val_loss: 1.6710 - val_acc: 0.4167\n",
            "Epoch 59/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0486 - acc: 0.9867 - val_loss: 1.8460 - val_acc: 0.4583\n",
            "Epoch 60/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0442 - acc: 0.9912 - val_loss: 1.6756 - val_acc: 0.4583\n",
            "Epoch 61/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0265 - acc: 0.9991 - val_loss: 1.6893 - val_acc: 0.4583\n",
            "Epoch 62/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0512 - acc: 0.9850 - val_loss: 2.1558 - val_acc: 0.5833\n",
            "Epoch 63/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0323 - acc: 0.9929 - val_loss: 1.8265 - val_acc: 0.4583\n",
            "Epoch 64/100\n",
            "1130/1130 [==============================] - 0s 326us/step - loss: 0.0628 - acc: 0.9770 - val_loss: 2.1744 - val_acc: 0.5000\n",
            "Epoch 65/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.1274 - acc: 0.9504 - val_loss: 2.5890 - val_acc: 0.5417\n",
            "Epoch 66/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0968 - acc: 0.9619 - val_loss: 2.9871 - val_acc: 0.5417\n",
            "Epoch 67/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.3163 - acc: 0.8558 - val_loss: 1.3313 - val_acc: 0.5417\n",
            "Epoch 68/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0700 - acc: 0.9832 - val_loss: 1.5343 - val_acc: 0.5000\n",
            "Epoch 69/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0345 - acc: 0.9956 - val_loss: 1.6681 - val_acc: 0.5000\n",
            "Epoch 70/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0233 - acc: 0.9982 - val_loss: 1.7247 - val_acc: 0.5417\n",
            "Epoch 71/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 1.7931 - val_acc: 0.5000\n",
            "Epoch 72/100\n",
            "1130/1130 [==============================] - 0s 326us/step - loss: 0.0154 - acc: 1.0000 - val_loss: 1.9853 - val_acc: 0.4167\n",
            "Epoch 73/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0164 - acc: 1.0000 - val_loss: 2.1505 - val_acc: 0.5000\n",
            "Epoch 74/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.4435 - acc: 0.8487 - val_loss: 1.2242 - val_acc: 0.5833\n",
            "Epoch 75/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.1102 - acc: 0.9681 - val_loss: 1.2757 - val_acc: 0.4583\n",
            "Epoch 76/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0521 - acc: 0.9965 - val_loss: 1.4660 - val_acc: 0.5417\n",
            "Epoch 77/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0422 - acc: 0.9991 - val_loss: 1.4988 - val_acc: 0.5000\n",
            "Epoch 78/100\n",
            "1130/1130 [==============================] - 0s 325us/step - loss: 0.0412 - acc: 0.9973 - val_loss: 1.6420 - val_acc: 0.4167\n",
            "Epoch 79/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0324 - acc: 1.0000 - val_loss: 1.5903 - val_acc: 0.4583\n",
            "Epoch 80/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0320 - acc: 0.9956 - val_loss: 1.6717 - val_acc: 0.4167\n",
            "Epoch 81/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0238 - acc: 0.9991 - val_loss: 1.6997 - val_acc: 0.5000\n",
            "Epoch 82/100\n",
            "1130/1130 [==============================] - 0s 325us/step - loss: 0.0212 - acc: 1.0000 - val_loss: 1.7440 - val_acc: 0.5000\n",
            "Epoch 83/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0342 - acc: 0.9982 - val_loss: 1.8180 - val_acc: 0.5417\n",
            "Epoch 84/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.0221 - acc: 0.9991 - val_loss: 1.9375 - val_acc: 0.3750\n",
            "Epoch 85/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 1.9201 - val_acc: 0.4167\n",
            "Epoch 86/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.3692 - acc: 0.8743 - val_loss: 2.1274 - val_acc: 0.5000\n",
            "Epoch 87/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.3562 - acc: 0.8646 - val_loss: 1.2144 - val_acc: 0.5417\n",
            "Epoch 88/100\n",
            "1130/1130 [==============================] - 0s 326us/step - loss: 0.0741 - acc: 0.9867 - val_loss: 1.3467 - val_acc: 0.5000\n",
            "Epoch 89/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0345 - acc: 1.0000 - val_loss: 1.5187 - val_acc: 0.5000\n",
            "Epoch 90/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0269 - acc: 1.0000 - val_loss: 1.6010 - val_acc: 0.4583\n",
            "Epoch 91/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0338 - acc: 0.9938 - val_loss: 1.7268 - val_acc: 0.4167\n",
            "Epoch 92/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0256 - acc: 0.9973 - val_loss: 1.6908 - val_acc: 0.4583\n",
            "Epoch 93/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 1.7620 - val_acc: 0.4167\n",
            "Epoch 94/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 1.8233 - val_acc: 0.4167\n",
            "Epoch 95/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 1.8815 - val_acc: 0.4583\n",
            "Epoch 96/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 1.9253 - val_acc: 0.5000\n",
            "Epoch 97/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 1.9737 - val_acc: 0.4583\n",
            "Epoch 98/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 2.0043 - val_acc: 0.4167\n",
            "Epoch 99/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 2.0444 - val_acc: 0.5417\n",
            "Epoch 100/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 2.2040 - val_acc: 0.4167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vebfsSKvPpk5",
        "colab": {}
      },
      "source": [
        "# Save model\n",
        "model.save('sagital_meniscus.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4963efbe-6608-4e95-f250-68fb64abc643",
        "id": "agVJVFOnPpk8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Evaluate the model on Test Set\n",
        "sagital_meniscus_scores = sagital_meniscus_model.evaluate(sagital_meniscus_validation_features, sagital_meniscus_validation_labels)\n",
        "print(\"\\n%s: %.2f%%\" % (sagital_meniscus_model.metrics_names[1], sagital_meniscus_scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 0s 4ms/step\n",
            "\n",
            "acc: 51.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JRIeC2zaulbf"
      },
      "source": [
        "## 4. Coronal - Abnormal CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "33ae9b75-6d6b-4326-b9a7-0a3de0594aa1",
        "id": "3ETMmHzpPq6S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "# Instantiate convolutional base\n",
        "from keras.applications import VGG16\n",
        "\n",
        "\n",
        "# include_top as false means that we want to train our own classifier on top of the convolutional base\n",
        "\n",
        "coronal_abnormal_conv_base = VGG16(weights='imagenet', \n",
        "                                    include_top=False,\n",
        "                                    input_shape=(224, 224, 3))  # 3 = number of channels in RGB pictures\n",
        "\n",
        "# Check architecture\n",
        "coronal_abnormal_conv_base.summary()\n",
        "\n",
        "\n",
        "\n",
        "#Extract Features\n",
        "\n",
        "#  X-> final_X_train_coronal\n",
        "#  Y - > Y_train_abnormal\n",
        "    \n",
        "coronal_abnormal_train_features, coronal_abnormal_train_labels = extract_features(coronal_abnormal_conv_base, final_X_train_coronal, Y_train_abnormal, final_X_train_coronal.shape[0])  # Agree with our small dataset size\n",
        "\n",
        "coronal_abnormal_validation_features, coronal_abnormal_validation_labels = extract_features(coronal_abnormal_conv_base, final_X_valid_coronal, Y_valid_abnormal, final_X_valid_coronal.shape[0])\n",
        "\n",
        "coronal_abnormal_test_features, coronal_abnormal_test_labels = extract_features(coronal_abnormal_conv_base, final_X_test_coronal, Y_test_abnormal, final_X_test_coronal.shape[0])\n",
        "\n",
        "print(\"DONE !\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "DONE !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "62099564-eab0-47ec-fb43-eab278c5869c",
        "id": "ppk5mcq5Pq6h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Define model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "coronal_abnormal_model = models.Sequential()\n",
        "coronal_abnormal_model.add(layers.Flatten(input_shape=(7,7,512)))\n",
        "coronal_abnormal_model.add(layers.Dense(256, activation='relu', input_dim=(7*7*512)))\n",
        "coronal_abnormal_model.add(layers.Dense(128, activation='relu'))\n",
        "coronal_abnormal_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "coronal_abnormal_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_4 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 6,455,809\n",
            "Trainable params: 6,455,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1e6cad24-9857-4499-b831-bfc280afbaa7",
        "id": "RzsH_RJPPq6r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3394
        }
      },
      "source": [
        "# Compile model\n",
        "coronal_abnormal_model.compile(optimizer=optimizers.Adam(),\n",
        "                                loss='binary_crossentropy',\n",
        "                                metrics=['acc'])\n",
        "\n",
        "# Train model\n",
        "coronal_abnormal_history = coronal_abnormal_model.fit(coronal_abnormal_train_features, coronal_abnormal_train_labels,\n",
        "                                                      epochs=epochs,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      validation_data=(coronal_abnormal_validation_features, coronal_abnormal_validation_labels))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1130 samples, validate on 24 samples\n",
            "Epoch 1/100\n",
            "1130/1130 [==============================] - 1s 1ms/step - loss: 0.6137 - acc: 0.7602 - val_loss: 0.7368 - val_acc: 0.7917\n",
            "Epoch 2/100\n",
            "1130/1130 [==============================] - 0s 360us/step - loss: 0.4923 - acc: 0.7938 - val_loss: 0.6125 - val_acc: 0.7917\n",
            "Epoch 3/100\n",
            "1130/1130 [==============================] - 0s 345us/step - loss: 0.4683 - acc: 0.8150 - val_loss: 0.6397 - val_acc: 0.6250\n",
            "Epoch 4/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.4133 - acc: 0.8186 - val_loss: 0.7335 - val_acc: 0.7917\n",
            "Epoch 5/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.4007 - acc: 0.8177 - val_loss: 0.6353 - val_acc: 0.7083\n",
            "Epoch 6/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.4036 - acc: 0.8248 - val_loss: 0.7208 - val_acc: 0.5417\n",
            "Epoch 7/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.3896 - acc: 0.8239 - val_loss: 0.6537 - val_acc: 0.6667\n",
            "Epoch 8/100\n",
            "1130/1130 [==============================] - 0s 347us/step - loss: 0.3627 - acc: 0.8345 - val_loss: 0.6888 - val_acc: 0.7083\n",
            "Epoch 9/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.3320 - acc: 0.8558 - val_loss: 0.7366 - val_acc: 0.6250\n",
            "Epoch 10/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.3162 - acc: 0.8611 - val_loss: 0.9728 - val_acc: 0.7917\n",
            "Epoch 11/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.4428 - acc: 0.8035 - val_loss: 1.2342 - val_acc: 0.7917\n",
            "Epoch 12/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.4727 - acc: 0.8124 - val_loss: 0.7420 - val_acc: 0.5000\n",
            "Epoch 13/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.3649 - acc: 0.8389 - val_loss: 0.7680 - val_acc: 0.7917\n",
            "Epoch 14/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.3143 - acc: 0.8611 - val_loss: 0.7482 - val_acc: 0.7500\n",
            "Epoch 15/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.3094 - acc: 0.8549 - val_loss: 0.8191 - val_acc: 0.6667\n",
            "Epoch 16/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.3128 - acc: 0.8584 - val_loss: 1.0606 - val_acc: 0.5000\n",
            "Epoch 17/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.2530 - acc: 0.8956 - val_loss: 0.9256 - val_acc: 0.6667\n",
            "Epoch 18/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.2814 - acc: 0.8743 - val_loss: 1.3420 - val_acc: 0.4583\n",
            "Epoch 19/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.2311 - acc: 0.9115 - val_loss: 0.9668 - val_acc: 0.6667\n",
            "Epoch 20/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.2945 - acc: 0.8690 - val_loss: 1.0928 - val_acc: 0.7917\n",
            "Epoch 21/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.2365 - acc: 0.9009 - val_loss: 1.1128 - val_acc: 0.7917\n",
            "Epoch 22/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.3032 - acc: 0.8664 - val_loss: 1.0070 - val_acc: 0.7917\n",
            "Epoch 23/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.2410 - acc: 0.8903 - val_loss: 1.0419 - val_acc: 0.6667\n",
            "Epoch 24/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.1827 - acc: 0.9221 - val_loss: 1.2253 - val_acc: 0.7917\n",
            "Epoch 25/100\n",
            "1130/1130 [==============================] - 0s 361us/step - loss: 0.2257 - acc: 0.8973 - val_loss: 1.3364 - val_acc: 0.7917\n",
            "Epoch 26/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.1889 - acc: 0.9274 - val_loss: 1.2760 - val_acc: 0.7917\n",
            "Epoch 27/100\n",
            "1130/1130 [==============================] - 0s 359us/step - loss: 0.1847 - acc: 0.9248 - val_loss: 1.1832 - val_acc: 0.7083\n",
            "Epoch 28/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.2718 - acc: 0.8885 - val_loss: 1.2105 - val_acc: 0.7917\n",
            "Epoch 29/100\n",
            "1130/1130 [==============================] - 0s 348us/step - loss: 0.1746 - acc: 0.9310 - val_loss: 1.2865 - val_acc: 0.7500\n",
            "Epoch 30/100\n",
            "1130/1130 [==============================] - 0s 357us/step - loss: 0.1150 - acc: 0.9611 - val_loss: 1.4017 - val_acc: 0.7500\n",
            "Epoch 31/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.1075 - acc: 0.9637 - val_loss: 1.4988 - val_acc: 0.6667\n",
            "Epoch 32/100\n",
            "1130/1130 [==============================] - 0s 352us/step - loss: 0.1250 - acc: 0.9522 - val_loss: 1.3613 - val_acc: 0.6667\n",
            "Epoch 33/100\n",
            "1130/1130 [==============================] - 0s 348us/step - loss: 0.0834 - acc: 0.9743 - val_loss: 1.6593 - val_acc: 0.7083\n",
            "Epoch 34/100\n",
            "1130/1130 [==============================] - 0s 352us/step - loss: 0.2423 - acc: 0.9035 - val_loss: 1.7690 - val_acc: 0.7917\n",
            "Epoch 35/100\n",
            "1130/1130 [==============================] - 0s 351us/step - loss: 0.1607 - acc: 0.9292 - val_loss: 1.6219 - val_acc: 0.7083\n",
            "Epoch 36/100\n",
            "1130/1130 [==============================] - 0s 345us/step - loss: 0.1234 - acc: 0.9549 - val_loss: 1.5054 - val_acc: 0.6667\n",
            "Epoch 37/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.1143 - acc: 0.9504 - val_loss: 1.6363 - val_acc: 0.7083\n",
            "Epoch 38/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.0779 - acc: 0.9735 - val_loss: 1.6401 - val_acc: 0.6667\n",
            "Epoch 39/100\n",
            "1130/1130 [==============================] - 0s 348us/step - loss: 0.0620 - acc: 0.9779 - val_loss: 1.7681 - val_acc: 0.7083\n",
            "Epoch 40/100\n",
            "1130/1130 [==============================] - 0s 358us/step - loss: 0.0694 - acc: 0.9717 - val_loss: 1.8635 - val_acc: 0.6667\n",
            "Epoch 41/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.0455 - acc: 0.9894 - val_loss: 1.8728 - val_acc: 0.6667\n",
            "Epoch 42/100\n",
            "1130/1130 [==============================] - 0s 351us/step - loss: 0.0411 - acc: 0.9876 - val_loss: 2.0659 - val_acc: 0.5417\n",
            "Epoch 43/100\n",
            "1130/1130 [==============================] - 0s 358us/step - loss: 0.0752 - acc: 0.9673 - val_loss: 2.0143 - val_acc: 0.7083\n",
            "Epoch 44/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.0336 - acc: 0.9938 - val_loss: 2.0259 - val_acc: 0.7083\n",
            "Epoch 45/100\n",
            "1130/1130 [==============================] - 0s 362us/step - loss: 0.2356 - acc: 0.9159 - val_loss: 1.6041 - val_acc: 0.7500\n",
            "Epoch 46/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.1464 - acc: 0.9372 - val_loss: 1.5408 - val_acc: 0.7083\n",
            "Epoch 47/100\n",
            "1130/1130 [==============================] - 0s 347us/step - loss: 0.0797 - acc: 0.9717 - val_loss: 1.8298 - val_acc: 0.6667\n",
            "Epoch 48/100\n",
            "1130/1130 [==============================] - 0s 360us/step - loss: 0.0510 - acc: 0.9867 - val_loss: 1.8933 - val_acc: 0.7083\n",
            "Epoch 49/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.0858 - acc: 0.9619 - val_loss: 2.0144 - val_acc: 0.7500\n",
            "Epoch 50/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.0586 - acc: 0.9752 - val_loss: 2.0606 - val_acc: 0.7500\n",
            "Epoch 51/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0732 - acc: 0.9708 - val_loss: 1.9895 - val_acc: 0.7083\n",
            "Epoch 52/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.1614 - acc: 0.9301 - val_loss: 1.7809 - val_acc: 0.6667\n",
            "Epoch 53/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0985 - acc: 0.9593 - val_loss: 1.7832 - val_acc: 0.7083\n",
            "Epoch 54/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0351 - acc: 0.9920 - val_loss: 2.0786 - val_acc: 0.6250\n",
            "Epoch 55/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0285 - acc: 0.9929 - val_loss: 2.3177 - val_acc: 0.5417\n",
            "Epoch 56/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0397 - acc: 0.9841 - val_loss: 2.1873 - val_acc: 0.6667\n",
            "Epoch 57/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0216 - acc: 0.9965 - val_loss: 2.2741 - val_acc: 0.5417\n",
            "Epoch 58/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0181 - acc: 0.9965 - val_loss: 2.2333 - val_acc: 0.7083\n",
            "Epoch 59/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0119 - acc: 0.9991 - val_loss: 2.2630 - val_acc: 0.7500\n",
            "Epoch 60/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0131 - acc: 0.9982 - val_loss: 2.3135 - val_acc: 0.7083\n",
            "Epoch 61/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 2.3594 - val_acc: 0.6667\n",
            "Epoch 62/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0361 - acc: 0.9858 - val_loss: 2.4620 - val_acc: 0.7500\n",
            "Epoch 63/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0672 - acc: 0.9735 - val_loss: 2.3361 - val_acc: 0.7083\n",
            "Epoch 64/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.0188 - acc: 0.9965 - val_loss: 2.4762 - val_acc: 0.5417\n",
            "Epoch 65/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0082 - acc: 0.9991 - val_loss: 2.4224 - val_acc: 0.6250\n",
            "Epoch 66/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 2.4642 - val_acc: 0.5417\n",
            "Epoch 67/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 2.4394 - val_acc: 0.6667\n",
            "Epoch 68/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 2.4465 - val_acc: 0.7083\n",
            "Epoch 69/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 2.5071 - val_acc: 0.6250\n",
            "Epoch 70/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 2.5991 - val_acc: 0.5417\n",
            "Epoch 71/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 2.5480 - val_acc: 0.6667\n",
            "Epoch 72/100\n",
            "1130/1130 [==============================] - 0s 356us/step - loss: 0.0042 - acc: 0.9991 - val_loss: 2.5272 - val_acc: 0.7083\n",
            "Epoch 73/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 2.5417 - val_acc: 0.7083\n",
            "Epoch 74/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 2.6089 - val_acc: 0.5833\n",
            "Epoch 75/100\n",
            "1130/1130 [==============================] - 0s 345us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 2.6118 - val_acc: 0.6667\n",
            "Epoch 76/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 2.5877 - val_acc: 0.7083\n",
            "Epoch 77/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 2.6412 - val_acc: 0.6250\n",
            "Epoch 78/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 2.6075 - val_acc: 0.7083\n",
            "Epoch 79/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 2.6272 - val_acc: 0.7083\n",
            "Epoch 80/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 2.6301 - val_acc: 0.7083\n",
            "Epoch 81/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 2.6503 - val_acc: 0.7083\n",
            "Epoch 82/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 2.6694 - val_acc: 0.7083\n",
            "Epoch 83/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 2.6822 - val_acc: 0.7083\n",
            "Epoch 84/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 2.7433 - val_acc: 0.5833\n",
            "Epoch 85/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 2.6838 - val_acc: 0.7083\n",
            "Epoch 86/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.7178 - val_acc: 0.6250\n",
            "Epoch 87/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.7217 - val_acc: 0.7083\n",
            "Epoch 88/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.6977 - val_acc: 0.7083\n",
            "Epoch 89/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.7675 - val_acc: 0.5833\n",
            "Epoch 90/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 2.7163 - val_acc: 0.7083\n",
            "Epoch 91/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 2.7346 - val_acc: 0.7083\n",
            "Epoch 92/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.7484 - val_acc: 0.7083\n",
            "Epoch 93/100\n",
            "1130/1130 [==============================] - 0s 325us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 2.7619 - val_acc: 0.7083\n",
            "Epoch 94/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 9.8194e-04 - acc: 1.0000 - val_loss: 2.7840 - val_acc: 0.6667\n",
            "Epoch 95/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 9.4175e-04 - acc: 1.0000 - val_loss: 2.7697 - val_acc: 0.7083\n",
            "Epoch 96/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 8.8997e-04 - acc: 1.0000 - val_loss: 2.7835 - val_acc: 0.7083\n",
            "Epoch 97/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 7.9198e-04 - acc: 1.0000 - val_loss: 2.7974 - val_acc: 0.7083\n",
            "Epoch 98/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 7.8687e-04 - acc: 1.0000 - val_loss: 2.8042 - val_acc: 0.7083\n",
            "Epoch 99/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 7.8828e-04 - acc: 1.0000 - val_loss: 2.8794 - val_acc: 0.5833\n",
            "Epoch 100/100\n",
            "1130/1130 [==============================] - 0s 326us/step - loss: 7.6537e-04 - acc: 1.0000 - val_loss: 2.8360 - val_acc: 0.6250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "otxlVT6EPq6x",
        "colab": {}
      },
      "source": [
        "# Save model\n",
        "coronal_abnormal_model.save('coronal_abnormal.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f390c950-2872-44c2-84a5-1f821f4dbecf",
        "id": "e1DoN3lsPq6z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Evaluate the model on Test Set\n",
        "coronal_abnormal_scores = coronal_abnormal_model.evaluate(coronal_abnormal_validation_features, coronal_abnormal_validation_labels)\n",
        "print(\"\\n%s: %.2f%%\" % (coronal_abnormal_model.metrics_names[1], coronal_abnormal_scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 0s 4ms/step\n",
            "\n",
            "acc: 74.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-QfSJM-bup4e"
      },
      "source": [
        "## 5. Coronal- ACL CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "372aafbc-e012-447b-bf61-eb3e4820a733",
        "id": "M97wR_AaTBUM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "# Instantiate convolutional base\n",
        "from keras.applications import VGG16\n",
        "\n",
        "\n",
        "# include_top as false means that we want to train our own classifier on top of the convolutional base\n",
        "\n",
        "coronal_acl_conv_base = VGG16(weights='imagenet', \n",
        "                              include_top=False,\n",
        "                              input_shape=(224, 224, 3))  # 3 = number of channels in RGB pictures\n",
        "\n",
        "# Check architecture\n",
        "coronal_acl_conv_base.summary()\n",
        "\n",
        "\n",
        "#Extract features\n",
        "\n",
        "#  X-> final_X_train_sagital\n",
        "#  Y - > Y_train_abnormal\n",
        "    \n",
        "coronal_acl_train_features, coronal_acl_train_labels = extract_features(coronal_acl_conv_base, final_X_train_coronal, Y_train_acl, final_X_train_coronal.shape[0])  # Agree with our small dataset size\n",
        "\n",
        "coronal_acl_validation_features, coronal_acl_validation_labels = extract_features(coronal_acl_conv_base, final_X_valid_coronal, Y_valid_acl, final_X_valid_coronal.shape[0])\n",
        "\n",
        "coronal_acl_test_features, coronal_acl_test_labels = extract_features(coronal_acl_conv_base, final_X_test_coronal, Y_test_acl, final_X_test_coronal.shape[0])\n",
        "\n",
        "print(\"DONE !\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "DONE !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "50333e54-ae9b-40ba-852f-45b7440f0dc7",
        "id": "sOyxaiRwTBUW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Define model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "coronal_acl_model = models.Sequential()\n",
        "coronal_acl_model.add(layers.Flatten(input_shape=(7,7,512)))\n",
        "coronal_acl_model.add(layers.Dense(256, activation='relu', input_dim=(7*7*512)))\n",
        "coronal_acl_model.add(layers.Dense(128, activation='relu'))\n",
        "coronal_acl_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "coronal_acl_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_5 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 6,455,809\n",
            "Trainable params: 6,455,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "25e53a8a-e07e-49ff-fe0a-96fbcb67e3d4",
        "id": "_U7OFDs-TBUb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3394
        }
      },
      "source": [
        "# Compile model\n",
        "coronal_acl_model.compile(optimizer=optimizers.Adam(),\n",
        "                          loss='binary_crossentropy',\n",
        "                          metrics=['acc'])\n",
        "\n",
        "# Train model\n",
        "coronal_acl_history = coronal_acl_model.fit(coronal_acl_train_features, coronal_acl_train_labels,\n",
        "                                            epochs=epochs,\n",
        "                                            batch_size=batch_size,\n",
        "                                            validation_data=(coronal_acl_validation_features, coronal_acl_validation_labels))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1130 samples, validate on 24 samples\n",
            "Epoch 1/100\n",
            "1130/1130 [==============================] - 1s 1ms/step - loss: 0.6924 - acc: 0.7292 - val_loss: 1.5751 - val_acc: 0.5833\n",
            "Epoch 2/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.5329 - acc: 0.7850 - val_loss: 0.9267 - val_acc: 0.5833\n",
            "Epoch 3/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.4664 - acc: 0.8142 - val_loss: 0.8186 - val_acc: 0.5833\n",
            "Epoch 4/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.5370 - acc: 0.7894 - val_loss: 0.8679 - val_acc: 0.5833\n",
            "Epoch 5/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.4380 - acc: 0.8248 - val_loss: 0.8466 - val_acc: 0.5833\n",
            "Epoch 6/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.4520 - acc: 0.8265 - val_loss: 0.7653 - val_acc: 0.5833\n",
            "Epoch 7/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.4367 - acc: 0.8310 - val_loss: 0.8016 - val_acc: 0.5833\n",
            "Epoch 8/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.4129 - acc: 0.8354 - val_loss: 0.7251 - val_acc: 0.5833\n",
            "Epoch 9/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.3918 - acc: 0.8442 - val_loss: 0.8543 - val_acc: 0.5833\n",
            "Epoch 10/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.3772 - acc: 0.8496 - val_loss: 1.0636 - val_acc: 0.5833\n",
            "Epoch 11/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.3665 - acc: 0.8434 - val_loss: 0.8795 - val_acc: 0.5833\n",
            "Epoch 12/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.3842 - acc: 0.8336 - val_loss: 0.9059 - val_acc: 0.5833\n",
            "Epoch 13/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.3464 - acc: 0.8584 - val_loss: 0.7953 - val_acc: 0.4583\n",
            "Epoch 14/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.3330 - acc: 0.8593 - val_loss: 0.9090 - val_acc: 0.6250\n",
            "Epoch 15/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.3496 - acc: 0.8628 - val_loss: 1.0576 - val_acc: 0.5833\n",
            "Epoch 16/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.2973 - acc: 0.8770 - val_loss: 0.9144 - val_acc: 0.6250\n",
            "Epoch 17/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.2965 - acc: 0.8770 - val_loss: 1.7067 - val_acc: 0.5833\n",
            "Epoch 18/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.2832 - acc: 0.8912 - val_loss: 1.1044 - val_acc: 0.5833\n",
            "Epoch 19/100\n",
            "1130/1130 [==============================] - 0s 360us/step - loss: 0.2981 - acc: 0.8690 - val_loss: 1.0055 - val_acc: 0.6667\n",
            "Epoch 20/100\n",
            "1130/1130 [==============================] - 0s 345us/step - loss: 0.3325 - acc: 0.8637 - val_loss: 0.9986 - val_acc: 0.5833\n",
            "Epoch 21/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.2187 - acc: 0.9204 - val_loss: 1.0129 - val_acc: 0.6667\n",
            "Epoch 22/100\n",
            "1130/1130 [==============================] - 0s 360us/step - loss: 0.2296 - acc: 0.9150 - val_loss: 1.3796 - val_acc: 0.6250\n",
            "Epoch 23/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.2245 - acc: 0.9115 - val_loss: 2.5834 - val_acc: 0.5833\n",
            "Epoch 24/100\n",
            "1130/1130 [==============================] - 0s 347us/step - loss: 0.3454 - acc: 0.8593 - val_loss: 1.4797 - val_acc: 0.5833\n",
            "Epoch 25/100\n",
            "1130/1130 [==============================] - 0s 348us/step - loss: 0.2150 - acc: 0.9142 - val_loss: 1.7782 - val_acc: 0.5833\n",
            "Epoch 26/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.1894 - acc: 0.9283 - val_loss: 0.7920 - val_acc: 0.4583\n",
            "Epoch 27/100\n",
            "1130/1130 [==============================] - 0s 366us/step - loss: 0.1617 - acc: 0.9451 - val_loss: 0.9959 - val_acc: 0.5417\n",
            "Epoch 28/100\n",
            "1130/1130 [==============================] - 0s 348us/step - loss: 0.1791 - acc: 0.9292 - val_loss: 1.6835 - val_acc: 0.5833\n",
            "Epoch 29/100\n",
            "1130/1130 [==============================] - 0s 345us/step - loss: 0.3261 - acc: 0.8912 - val_loss: 0.8413 - val_acc: 0.5833\n",
            "Epoch 30/100\n",
            "1130/1130 [==============================] - 0s 358us/step - loss: 0.1794 - acc: 0.9283 - val_loss: 1.3253 - val_acc: 0.5833\n",
            "Epoch 31/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.2228 - acc: 0.9062 - val_loss: 1.3144 - val_acc: 0.5833\n",
            "Epoch 32/100\n",
            "1130/1130 [==============================] - 0s 359us/step - loss: 0.1770 - acc: 0.9354 - val_loss: 2.5729 - val_acc: 0.5833\n",
            "Epoch 33/100\n",
            "1130/1130 [==============================] - 0s 354us/step - loss: 0.3031 - acc: 0.8903 - val_loss: 1.5708 - val_acc: 0.5833\n",
            "Epoch 34/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.2111 - acc: 0.9071 - val_loss: 1.2485 - val_acc: 0.5833\n",
            "Epoch 35/100\n",
            "1130/1130 [==============================] - 0s 362us/step - loss: 0.1105 - acc: 0.9575 - val_loss: 2.4579 - val_acc: 0.5833\n",
            "Epoch 36/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.1581 - acc: 0.9292 - val_loss: 1.0621 - val_acc: 0.5417\n",
            "Epoch 37/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.1145 - acc: 0.9602 - val_loss: 2.4139 - val_acc: 0.5833\n",
            "Epoch 38/100\n",
            "1130/1130 [==============================] - 0s 354us/step - loss: 0.1469 - acc: 0.9407 - val_loss: 1.4410 - val_acc: 0.5000\n",
            "Epoch 39/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.0711 - acc: 0.9752 - val_loss: 2.0046 - val_acc: 0.6250\n",
            "Epoch 40/100\n",
            "1130/1130 [==============================] - 0s 355us/step - loss: 0.0510 - acc: 0.9912 - val_loss: 1.6641 - val_acc: 0.5417\n",
            "Epoch 41/100\n",
            "1130/1130 [==============================] - 0s 348us/step - loss: 0.0392 - acc: 0.9885 - val_loss: 1.8798 - val_acc: 0.5417\n",
            "Epoch 42/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.0581 - acc: 0.9770 - val_loss: 2.2621 - val_acc: 0.6250\n",
            "Epoch 43/100\n",
            "1130/1130 [==============================] - 0s 356us/step - loss: 0.0287 - acc: 0.9947 - val_loss: 1.9500 - val_acc: 0.5417\n",
            "Epoch 44/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0268 - acc: 0.9938 - val_loss: 2.1562 - val_acc: 0.5417\n",
            "Epoch 45/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0231 - acc: 0.9956 - val_loss: 2.2078 - val_acc: 0.5417\n",
            "Epoch 46/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 3.0369 - val_acc: 0.6250\n",
            "Epoch 47/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0214 - acc: 0.9973 - val_loss: 2.8283 - val_acc: 0.5833\n",
            "Epoch 48/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0182 - acc: 0.9982 - val_loss: 2.9458 - val_acc: 0.5833\n",
            "Epoch 49/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0721 - acc: 0.9673 - val_loss: 1.5856 - val_acc: 0.5417\n",
            "Epoch 50/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.2259 - acc: 0.9212 - val_loss: 1.4373 - val_acc: 0.5417\n",
            "Epoch 51/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0353 - acc: 0.9938 - val_loss: 2.2363 - val_acc: 0.5417\n",
            "Epoch 52/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0281 - acc: 0.9929 - val_loss: 1.8861 - val_acc: 0.5417\n",
            "Epoch 53/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 1.8686 - val_acc: 0.5417\n",
            "Epoch 54/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0144 - acc: 0.9991 - val_loss: 2.8084 - val_acc: 0.6250\n",
            "Epoch 55/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 2.3880 - val_acc: 0.5000\n",
            "Epoch 56/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 2.5062 - val_acc: 0.5417\n",
            "Epoch 57/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 2.9601 - val_acc: 0.5833\n",
            "Epoch 58/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 3.2040 - val_acc: 0.6250\n",
            "Epoch 59/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 2.6763 - val_acc: 0.5000\n",
            "Epoch 60/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.0004 - val_acc: 0.5417\n",
            "Epoch 61/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 2.7728 - val_acc: 0.5000\n",
            "Epoch 62/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 2.9746 - val_acc: 0.5417\n",
            "Epoch 63/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 2.9467 - val_acc: 0.5417\n",
            "Epoch 64/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 2.9152 - val_acc: 0.5000\n",
            "Epoch 65/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 2.8111 - val_acc: 0.5000\n",
            "Epoch 66/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 2.9745 - val_acc: 0.5000\n",
            "Epoch 67/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 2.9644 - val_acc: 0.5000\n",
            "Epoch 68/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 2.9854 - val_acc: 0.5000\n",
            "Epoch 69/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 2.9623 - val_acc: 0.5000\n",
            "Epoch 70/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 2.9216 - val_acc: 0.5417\n",
            "Epoch 71/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.3761 - val_acc: 0.5417\n",
            "Epoch 72/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 3.2486 - val_acc: 0.5417\n",
            "Epoch 73/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 3.2735 - val_acc: 0.5417\n",
            "Epoch 74/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 3.3816 - val_acc: 0.5417\n",
            "Epoch 75/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 9.8229e-04 - acc: 1.0000 - val_loss: 3.1205 - val_acc: 0.5417\n",
            "Epoch 76/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 3.3706 - val_acc: 0.5417\n",
            "Epoch 77/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 8.9726e-04 - acc: 1.0000 - val_loss: 3.5170 - val_acc: 0.5417\n",
            "Epoch 78/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 7.9502e-04 - acc: 1.0000 - val_loss: 3.4773 - val_acc: 0.5417\n",
            "Epoch 79/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 7.4230e-04 - acc: 1.0000 - val_loss: 3.5047 - val_acc: 0.5417\n",
            "Epoch 80/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 7.5173e-04 - acc: 1.0000 - val_loss: 3.5825 - val_acc: 0.5417\n",
            "Epoch 81/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 6.8011e-04 - acc: 1.0000 - val_loss: 3.5785 - val_acc: 0.5417\n",
            "Epoch 82/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 6.6997e-04 - acc: 1.0000 - val_loss: 3.6242 - val_acc: 0.5417\n",
            "Epoch 83/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 6.4811e-04 - acc: 1.0000 - val_loss: 3.4882 - val_acc: 0.5000\n",
            "Epoch 84/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 6.0962e-04 - acc: 1.0000 - val_loss: 3.5389 - val_acc: 0.5000\n",
            "Epoch 85/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 5.9278e-04 - acc: 1.0000 - val_loss: 3.5378 - val_acc: 0.5000\n",
            "Epoch 86/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 5.7234e-04 - acc: 1.0000 - val_loss: 3.5411 - val_acc: 0.5000\n",
            "Epoch 87/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 5.5110e-04 - acc: 1.0000 - val_loss: 3.4681 - val_acc: 0.5000\n",
            "Epoch 88/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 6.8294e-04 - acc: 1.0000 - val_loss: 3.5922 - val_acc: 0.5000\n",
            "Epoch 89/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 5.1116e-04 - acc: 1.0000 - val_loss: 3.5133 - val_acc: 0.5000\n",
            "Epoch 90/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 4.7046e-04 - acc: 1.0000 - val_loss: 3.7628 - val_acc: 0.5417\n",
            "Epoch 91/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 4.8651e-04 - acc: 1.0000 - val_loss: 3.8575 - val_acc: 0.5417\n",
            "Epoch 92/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 4.4081e-04 - acc: 1.0000 - val_loss: 3.7195 - val_acc: 0.5000\n",
            "Epoch 93/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 4.1471e-04 - acc: 1.0000 - val_loss: 3.7249 - val_acc: 0.5000\n",
            "Epoch 94/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 3.9162e-04 - acc: 1.0000 - val_loss: 3.6780 - val_acc: 0.5000\n",
            "Epoch 95/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 3.8103e-04 - acc: 1.0000 - val_loss: 3.9228 - val_acc: 0.5417\n",
            "Epoch 96/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 3.8089e-04 - acc: 1.0000 - val_loss: 3.6863 - val_acc: 0.5000\n",
            "Epoch 97/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 3.7497e-04 - acc: 1.0000 - val_loss: 3.7322 - val_acc: 0.5000\n",
            "Epoch 98/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 3.9402e-04 - acc: 1.0000 - val_loss: 3.6604 - val_acc: 0.5417\n",
            "Epoch 99/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 3.9745e-04 - acc: 1.0000 - val_loss: 3.8742 - val_acc: 0.5000\n",
            "Epoch 100/100\n",
            "1130/1130 [==============================] - 0s 346us/step - loss: 3.2766e-04 - acc: 1.0000 - val_loss: 4.0274 - val_acc: 0.5417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z3wLKvFdTBUf",
        "colab": {}
      },
      "source": [
        "# Save model\n",
        "coronal_acl_model.save('coronal_acl.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "131e4fa6-1ebd-4792-bd19-d7c863e5906c",
        "id": "iTBayJBITBUh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Evaluate the model on Test Set\n",
        "coronal_acl_scores = coronal_acl_model.evaluate(coronal_acl_validation_features, coronal_acl_validation_labels)\n",
        "print(\"\\n%s: %.2f%%\" % (coronal_acl_model.metrics_names[1], coronal_acl_scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 1s 4ms/step\n",
            "\n",
            "acc: 60.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zc6xNnpKu1vK"
      },
      "source": [
        "## 6. Coronal - Meniscus CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5c9e3460-ca98-437d-f77e-8f93af5ab1c2",
        "id": "CV-B0PZOTkNM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "# Instantiate convolutional base\n",
        "from keras.applications import VGG16\n",
        "\n",
        "\n",
        "# include_top as false means that we want to train our own classifier on top of the convolutional base\n",
        "\n",
        "coronal_meniscus_conv_base = VGG16(weights='imagenet', \n",
        "                                    include_top=False,\n",
        "                                    input_shape=(224, 224, 3))  # 3 = number of channels in RGB pictures\n",
        "\n",
        "# Check architecture\n",
        "coronal_meniscus_conv_base.summary()\n",
        "\n",
        "\n",
        "#Extract features\n",
        "\n",
        "#  X-> final_X_train_sagital\n",
        "#  Y - > Y_train_abnormal\n",
        "    \n",
        "coronal_meniscus_train_features, coronal_meniscus_train_labels = extract_features(coronal_meniscus_conv_base, final_X_train_coronal, Y_train_meniscus, final_X_train_coronal.shape[0])  # Agree with our small dataset size\n",
        "\n",
        "coronal_meniscus_validation_features, coronal_meniscus_validation_labels = extract_features(coronal_meniscus_conv_base, final_X_valid_coronal, Y_valid_meniscus, final_X_valid_coronal.shape[0])\n",
        "\n",
        "coronal_meniscus_test_features, coronal_meniscus_test_labels = extract_features(coronal_meniscus_conv_base, final_X_test_coronal, Y_test_meniscus, final_X_test_coronal.shape[0])\n",
        "\n",
        "print(\"DONE !\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "DONE !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "55f86bcf-dd47-48d1-9d83-6bb153af0cd1",
        "id": "fbanOkd4TkNb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Define model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "coronal_meniscus_model = models.Sequential()\n",
        "coronal_meniscus_model.add(layers.Flatten(input_shape=(7,7,512)))\n",
        "coronal_meniscus_model.add(layers.Dense(256, activation='relu', input_dim=(7*7*512)))\n",
        "coronal_meniscus_model.add(layers.Dense(128, activation='relu'))\n",
        "coronal_meniscus_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "coronal_meniscus_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_6 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 6,455,809\n",
            "Trainable params: 6,455,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "db6f1bf0-5cf9-43d0-a3a6-4a6d6f3c7c9f",
        "id": "QPv6Jrq9TkNh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3394
        }
      },
      "source": [
        "# Compile model\n",
        "coronal_meniscus_model.compile(optimizer=optimizers.Adam(),\n",
        "                                loss='binary_crossentropy',\n",
        "                                metrics=['acc'])\n",
        "\n",
        "# Train model\n",
        "coronal_meniscus_history = coronal_meniscus_model.fit(coronal_meniscus_train_features, coronal_meniscus_train_labels,\n",
        "                                                      epochs=epochs,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      validation_data=(coronal_meniscus_validation_features, coronal_meniscus_validation_labels))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1130 samples, validate on 24 samples\n",
            "Epoch 1/100\n",
            "1130/1130 [==============================] - 2s 2ms/step - loss: 0.8182 - acc: 0.6080 - val_loss: 0.7171 - val_acc: 0.5417\n",
            "Epoch 2/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.6529 - acc: 0.6381 - val_loss: 0.7000 - val_acc: 0.4583\n",
            "Epoch 3/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.6764 - acc: 0.6097 - val_loss: 0.6768 - val_acc: 0.6667\n",
            "Epoch 4/100\n",
            "1130/1130 [==============================] - 0s 346us/step - loss: 0.6156 - acc: 0.6708 - val_loss: 0.7310 - val_acc: 0.6250\n",
            "Epoch 5/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.6177 - acc: 0.6451 - val_loss: 0.6637 - val_acc: 0.6667\n",
            "Epoch 6/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.5869 - acc: 0.6867 - val_loss: 0.7111 - val_acc: 0.6667\n",
            "Epoch 7/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.5730 - acc: 0.7027 - val_loss: 0.6474 - val_acc: 0.7083\n",
            "Epoch 8/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.5551 - acc: 0.7062 - val_loss: 0.7025 - val_acc: 0.6667\n",
            "Epoch 9/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.5478 - acc: 0.7062 - val_loss: 0.8052 - val_acc: 0.5833\n",
            "Epoch 10/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.5456 - acc: 0.7142 - val_loss: 0.6572 - val_acc: 0.7083\n",
            "Epoch 11/100\n",
            "1130/1130 [==============================] - 0s 324us/step - loss: 0.5595 - acc: 0.7000 - val_loss: 0.6719 - val_acc: 0.6250\n",
            "Epoch 12/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.5294 - acc: 0.7097 - val_loss: 0.6466 - val_acc: 0.6667\n",
            "Epoch 13/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.4788 - acc: 0.7770 - val_loss: 0.7381 - val_acc: 0.7083\n",
            "Epoch 14/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.5373 - acc: 0.7389 - val_loss: 0.7615 - val_acc: 0.6667\n",
            "Epoch 15/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.5931 - acc: 0.6973 - val_loss: 0.6606 - val_acc: 0.7083\n",
            "Epoch 16/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.5096 - acc: 0.7575 - val_loss: 0.7397 - val_acc: 0.5417\n",
            "Epoch 17/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.5728 - acc: 0.7053 - val_loss: 0.8327 - val_acc: 0.6250\n",
            "Epoch 18/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.4657 - acc: 0.7761 - val_loss: 0.6941 - val_acc: 0.6667\n",
            "Epoch 19/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.4130 - acc: 0.8124 - val_loss: 0.7812 - val_acc: 0.7083\n",
            "Epoch 20/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.3788 - acc: 0.8310 - val_loss: 0.7589 - val_acc: 0.7083\n",
            "Epoch 21/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.4170 - acc: 0.8150 - val_loss: 0.7929 - val_acc: 0.6250\n",
            "Epoch 22/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.3703 - acc: 0.8310 - val_loss: 0.7346 - val_acc: 0.6250\n",
            "Epoch 23/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.3716 - acc: 0.8292 - val_loss: 1.0152 - val_acc: 0.5000\n",
            "Epoch 24/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.3008 - acc: 0.8735 - val_loss: 0.8156 - val_acc: 0.7083\n",
            "Epoch 25/100\n",
            "1130/1130 [==============================] - 0s 348us/step - loss: 0.3146 - acc: 0.8673 - val_loss: 0.8207 - val_acc: 0.5833\n",
            "Epoch 26/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.3318 - acc: 0.8513 - val_loss: 0.8366 - val_acc: 0.7083\n",
            "Epoch 27/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.3254 - acc: 0.8398 - val_loss: 0.9853 - val_acc: 0.6250\n",
            "Epoch 28/100\n",
            "1130/1130 [==============================] - 0s 326us/step - loss: 0.2507 - acc: 0.9000 - val_loss: 0.8385 - val_acc: 0.6667\n",
            "Epoch 29/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.2120 - acc: 0.9257 - val_loss: 1.0437 - val_acc: 0.5833\n",
            "Epoch 30/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.2122 - acc: 0.9221 - val_loss: 1.0270 - val_acc: 0.5833\n",
            "Epoch 31/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.1975 - acc: 0.9195 - val_loss: 1.1505 - val_acc: 0.6250\n",
            "Epoch 32/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.1908 - acc: 0.9248 - val_loss: 1.0163 - val_acc: 0.7083\n",
            "Epoch 33/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.1451 - acc: 0.9522 - val_loss: 1.0239 - val_acc: 0.7500\n",
            "Epoch 34/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.1378 - acc: 0.9602 - val_loss: 1.0507 - val_acc: 0.6250\n",
            "Epoch 35/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.1347 - acc: 0.9566 - val_loss: 1.6880 - val_acc: 0.5417\n",
            "Epoch 36/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.1871 - acc: 0.9159 - val_loss: 1.0153 - val_acc: 0.7083\n",
            "Epoch 37/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.1404 - acc: 0.9442 - val_loss: 2.4312 - val_acc: 0.5833\n",
            "Epoch 38/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.2407 - acc: 0.8965 - val_loss: 1.6182 - val_acc: 0.5417\n",
            "Epoch 39/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.1696 - acc: 0.9230 - val_loss: 1.3095 - val_acc: 0.5417\n",
            "Epoch 40/100\n",
            "1130/1130 [==============================] - 0s 347us/step - loss: 0.1128 - acc: 0.9575 - val_loss: 1.4473 - val_acc: 0.5417\n",
            "Epoch 41/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.1375 - acc: 0.9416 - val_loss: 1.1601 - val_acc: 0.6667\n",
            "Epoch 42/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.0943 - acc: 0.9699 - val_loss: 1.7330 - val_acc: 0.5417\n",
            "Epoch 43/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.1242 - acc: 0.9540 - val_loss: 1.4204 - val_acc: 0.5833\n",
            "Epoch 44/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0674 - acc: 0.9814 - val_loss: 1.5083 - val_acc: 0.5833\n",
            "Epoch 45/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.0520 - acc: 0.9903 - val_loss: 1.2537 - val_acc: 0.6667\n",
            "Epoch 46/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0434 - acc: 0.9956 - val_loss: 1.4010 - val_acc: 0.7083\n",
            "Epoch 47/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0329 - acc: 0.9982 - val_loss: 1.4719 - val_acc: 0.6250\n",
            "Epoch 48/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0303 - acc: 0.9982 - val_loss: 1.4505 - val_acc: 0.6250\n",
            "Epoch 49/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0331 - acc: 0.9965 - val_loss: 1.4055 - val_acc: 0.6667\n",
            "Epoch 50/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.0292 - acc: 0.9973 - val_loss: 1.4094 - val_acc: 0.7083\n",
            "Epoch 51/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 1.4078 - val_acc: 0.6667\n",
            "Epoch 52/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0231 - acc: 0.9991 - val_loss: 1.4421 - val_acc: 0.6667\n",
            "Epoch 53/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.0213 - acc: 0.9982 - val_loss: 1.6724 - val_acc: 0.6250\n",
            "Epoch 54/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 1.5840 - val_acc: 0.6250\n",
            "Epoch 55/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 1.6333 - val_acc: 0.6250\n",
            "Epoch 56/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 1.5660 - val_acc: 0.6667\n",
            "Epoch 57/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 1.7985 - val_acc: 0.5833\n",
            "Epoch 58/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 1.8949 - val_acc: 0.6250\n",
            "Epoch 59/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0118 - acc: 1.0000 - val_loss: 1.5559 - val_acc: 0.6250\n",
            "Epoch 60/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0153 - acc: 0.9991 - val_loss: 1.9337 - val_acc: 0.5833\n",
            "Epoch 61/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 1.7854 - val_acc: 0.6667\n",
            "Epoch 62/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 1.8596 - val_acc: 0.6250\n",
            "Epoch 63/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 2.0504 - val_acc: 0.5833\n",
            "Epoch 64/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 1.9925 - val_acc: 0.6250\n",
            "Epoch 65/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 1.8994 - val_acc: 0.6667\n",
            "Epoch 66/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 2.0695 - val_acc: 0.6250\n",
            "Epoch 67/100\n",
            "1130/1130 [==============================] - 0s 356us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 1.9992 - val_acc: 0.6250\n",
            "Epoch 68/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 1.9872 - val_acc: 0.6667\n",
            "Epoch 69/100\n",
            "1130/1130 [==============================] - 0s 358us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 2.2551 - val_acc: 0.5833\n",
            "Epoch 70/100\n",
            "1130/1130 [==============================] - 0s 352us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 2.0202 - val_acc: 0.6250\n",
            "Epoch 71/100\n",
            "1130/1130 [==============================] - 0s 346us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 2.0168 - val_acc: 0.6250\n",
            "Epoch 72/100\n",
            "1130/1130 [==============================] - 0s 356us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 2.0057 - val_acc: 0.6667\n",
            "Epoch 73/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 2.2303 - val_acc: 0.5833\n",
            "Epoch 74/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 2.1461 - val_acc: 0.6250\n",
            "Epoch 75/100\n",
            "1130/1130 [==============================] - 0s 351us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 1.9679 - val_acc: 0.6667\n",
            "Epoch 76/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 1.9804 - val_acc: 0.6667\n",
            "Epoch 77/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 2.1237 - val_acc: 0.6250\n",
            "Epoch 78/100\n",
            "1130/1130 [==============================] - 0s 346us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 2.1994 - val_acc: 0.6250\n",
            "Epoch 79/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 2.2175 - val_acc: 0.6250\n",
            "Epoch 80/100\n",
            "1130/1130 [==============================] - 0s 363us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 2.2588 - val_acc: 0.6250\n",
            "Epoch 81/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 2.1295 - val_acc: 0.6250\n",
            "Epoch 82/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 2.1892 - val_acc: 0.6250\n",
            "Epoch 83/100\n",
            "1130/1130 [==============================] - 0s 345us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 2.2518 - val_acc: 0.6250\n",
            "Epoch 84/100\n",
            "1130/1130 [==============================] - 0s 357us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 2.2636 - val_acc: 0.6250\n",
            "Epoch 85/100\n",
            "1130/1130 [==============================] - 0s 362us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 2.3418 - val_acc: 0.6250\n",
            "Epoch 86/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 2.1851 - val_acc: 0.6250\n",
            "Epoch 87/100\n",
            "1130/1130 [==============================] - 0s 356us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.2817 - val_acc: 0.6250\n",
            "Epoch 88/100\n",
            "1130/1130 [==============================] - 0s 351us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.3772 - val_acc: 0.6250\n",
            "Epoch 89/100\n",
            "1130/1130 [==============================] - 0s 351us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 2.3436 - val_acc: 0.6250\n",
            "Epoch 90/100\n",
            "1130/1130 [==============================] - 0s 356us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 2.3184 - val_acc: 0.6250\n",
            "Epoch 91/100\n",
            "1130/1130 [==============================] - 0s 347us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.3779 - val_acc: 0.6250\n",
            "Epoch 92/100\n",
            "1130/1130 [==============================] - 0s 352us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 2.3998 - val_acc: 0.6250\n",
            "Epoch 93/100\n",
            "1130/1130 [==============================] - 0s 348us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 2.4070 - val_acc: 0.6250\n",
            "Epoch 94/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 9.6349e-04 - acc: 1.0000 - val_loss: 2.3662 - val_acc: 0.6250\n",
            "Epoch 95/100\n",
            "1130/1130 [==============================] - 0s 345us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.4042 - val_acc: 0.6250\n",
            "Epoch 96/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 9.6936e-04 - acc: 1.0000 - val_loss: 2.4826 - val_acc: 0.6250\n",
            "Epoch 97/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 9.1599e-04 - acc: 1.0000 - val_loss: 2.4571 - val_acc: 0.6250\n",
            "Epoch 98/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 8.4726e-04 - acc: 1.0000 - val_loss: 2.3966 - val_acc: 0.6250\n",
            "Epoch 99/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 8.5771e-04 - acc: 1.0000 - val_loss: 2.5317 - val_acc: 0.6250\n",
            "Epoch 100/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 8.5992e-04 - acc: 1.0000 - val_loss: 2.4976 - val_acc: 0.6250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tz5vc_ykTkNj",
        "colab": {}
      },
      "source": [
        "# Save model\n",
        "coronal_meniscus_model.save('coronal_meniscus.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0f3c706f-f484-4314-bc3e-eb329d8241f0",
        "id": "NukRnbivTkNl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Evaluate the model on Test Set\n",
        "coronal_meniscus_scores = coronal_meniscus_model.evaluate(coronal_meniscus_validation_features, coronal_meniscus_validation_labels)\n",
        "print(\"\\n%s: %.2f%%\" % (coronal_meniscus_model.metrics_names[1], coronal_meniscus_scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 0s 265us/step\n",
            "\n",
            "acc: 54.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yjlSwgnlvGLL"
      },
      "source": [
        "## 7. Axial - Abnormal CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "21505c0b-70be-4824-bb03-4d7b68147c31",
        "id": "3yDjMfnhTz-t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "# Instantiate convolutional base\n",
        "from keras.applications import VGG16\n",
        "\n",
        "\n",
        "# include_top as false means that we want to train our own classifier on top of the convolutional base\n",
        "\n",
        "axial_abnormal_conv_base = VGG16(weights='imagenet', \n",
        "                                include_top=False,\n",
        "                                input_shape=(224, 224, 3))  # 3 = number of channels in RGB pictures\n",
        "\n",
        "# Check architecture\n",
        "axial_abnormal_conv_base.summary()\n",
        "\n",
        "\n",
        "\n",
        "# Extract features\n",
        "\n",
        "#  X-> final_X_train_sagital\n",
        "#  Y - > Y_train_abnormal\n",
        "    \n",
        "axial_abnormal_train_features, axial_abnormal_train_labels = extract_features(axial_abnormal_conv_base, final_X_train_axial, Y_train_abnormal, final_X_train_axial.shape[0])  # Agree with our small dataset size\n",
        "\n",
        "axial_abnormal_validation_features, axial_abnormal_validation_labels = extract_features(axial_abnormal_conv_base, final_X_valid_axial, Y_valid_abnormal, final_X_valid_axial.shape[0])\n",
        "\n",
        "axial_abnormal_test_features, axial_abnormal_test_labels = extract_features(axial_abnormal_conv_base, final_X_test_axial, Y_test_abnormal, final_X_test_axial.shape[0])\n",
        "\n",
        "print(\"DONE !\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "DONE !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b990d6b4-4176-4799-ea65-af3936681188",
        "id": "hdyiJyAUTz-_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Define model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "axial_abnormal_model = models.Sequential()\n",
        "axial_abnormal_model.add(layers.Flatten(input_shape=(7,7,512)))\n",
        "axial_abnormal_model.add(layers.Dense(256, activation='relu', input_dim=(7*7*512)))\n",
        "axial_abnormal_model.add(layers.Dense(128, activation='relu'))\n",
        "axial_abnormal_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "axial_abnormal_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_7 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 6,455,809\n",
            "Trainable params: 6,455,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a8e250a6-33ba-4d8c-87c2-f0206d3dfe23",
        "id": "KiHBpx3XTz_J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3394
        }
      },
      "source": [
        "# Compile model\n",
        "axial_abnormal_model.compile(optimizer=optimizers.Adam(),\n",
        "                            loss='binary_crossentropy',\n",
        "                            metrics=['acc'])\n",
        "\n",
        "# Train model\n",
        "axial_abnormal_history = axial_abnormal_model.fit(axial_abnormal_train_features, axial_abnormal_train_labels,\n",
        "                                                  epochs=epochs,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  validation_data=(axial_abnormal_validation_features, axial_abnormal_validation_labels))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1130 samples, validate on 24 samples\n",
            "Epoch 1/100\n",
            "1130/1130 [==============================] - 1s 1ms/step - loss: 0.5750 - acc: 0.7903 - val_loss: 0.8174 - val_acc: 0.7917\n",
            "Epoch 2/100\n",
            "1130/1130 [==============================] - 0s 346us/step - loss: 0.4400 - acc: 0.8044 - val_loss: 0.4549 - val_acc: 0.8333\n",
            "Epoch 3/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.3948 - acc: 0.8186 - val_loss: 0.5777 - val_acc: 0.7917\n",
            "Epoch 4/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.4063 - acc: 0.8230 - val_loss: 0.5422 - val_acc: 0.7917\n",
            "Epoch 5/100\n",
            "1130/1130 [==============================] - 0s 348us/step - loss: 0.4410 - acc: 0.8212 - val_loss: 0.5852 - val_acc: 0.7917\n",
            "Epoch 6/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.3850 - acc: 0.8434 - val_loss: 0.3825 - val_acc: 0.8750\n",
            "Epoch 7/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.4384 - acc: 0.8159 - val_loss: 0.6345 - val_acc: 0.7917\n",
            "Epoch 8/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.3747 - acc: 0.8372 - val_loss: 0.4111 - val_acc: 0.8333\n",
            "Epoch 9/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.3440 - acc: 0.8611 - val_loss: 0.3583 - val_acc: 0.8750\n",
            "Epoch 10/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.3259 - acc: 0.8628 - val_loss: 0.3368 - val_acc: 0.9167\n",
            "Epoch 11/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.3387 - acc: 0.8522 - val_loss: 0.3518 - val_acc: 0.8333\n",
            "Epoch 12/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.3247 - acc: 0.8584 - val_loss: 0.3805 - val_acc: 0.8333\n",
            "Epoch 13/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.2972 - acc: 0.8708 - val_loss: 0.3976 - val_acc: 0.8333\n",
            "Epoch 14/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.3092 - acc: 0.8637 - val_loss: 0.4006 - val_acc: 0.8333\n",
            "Epoch 15/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.3169 - acc: 0.8619 - val_loss: 0.3529 - val_acc: 0.8750\n",
            "Epoch 16/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.2763 - acc: 0.9000 - val_loss: 0.3263 - val_acc: 0.9167\n",
            "Epoch 17/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.2699 - acc: 0.8841 - val_loss: 0.4825 - val_acc: 0.7917\n",
            "Epoch 18/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.2352 - acc: 0.9124 - val_loss: 0.3112 - val_acc: 0.9167\n",
            "Epoch 19/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.2614 - acc: 0.8912 - val_loss: 0.3185 - val_acc: 0.9167\n",
            "Epoch 20/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.2591 - acc: 0.8929 - val_loss: 0.4399 - val_acc: 0.8333\n",
            "Epoch 21/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.2177 - acc: 0.9106 - val_loss: 0.3289 - val_acc: 0.9167\n",
            "Epoch 22/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.2159 - acc: 0.9115 - val_loss: 0.3240 - val_acc: 0.9167\n",
            "Epoch 23/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.3558 - acc: 0.8469 - val_loss: 0.3828 - val_acc: 0.8750\n",
            "Epoch 24/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.2455 - acc: 0.9027 - val_loss: 0.4257 - val_acc: 0.7083\n",
            "Epoch 25/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.2448 - acc: 0.8991 - val_loss: 0.4463 - val_acc: 0.7500\n",
            "Epoch 26/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.2684 - acc: 0.8752 - val_loss: 0.3949 - val_acc: 0.8750\n",
            "Epoch 27/100\n",
            "1130/1130 [==============================] - 0s 326us/step - loss: 0.2206 - acc: 0.9124 - val_loss: 0.6896 - val_acc: 0.7917\n",
            "Epoch 28/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.1906 - acc: 0.9195 - val_loss: 0.6312 - val_acc: 0.8333\n",
            "Epoch 29/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.1954 - acc: 0.9142 - val_loss: 0.4545 - val_acc: 0.8750\n",
            "Epoch 30/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.1974 - acc: 0.9186 - val_loss: 0.5512 - val_acc: 0.8750\n",
            "Epoch 31/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.1382 - acc: 0.9487 - val_loss: 0.9709 - val_acc: 0.7917\n",
            "Epoch 32/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.1711 - acc: 0.9301 - val_loss: 0.8909 - val_acc: 0.7917\n",
            "Epoch 33/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.1394 - acc: 0.9460 - val_loss: 0.7947 - val_acc: 0.8333\n",
            "Epoch 34/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.1136 - acc: 0.9584 - val_loss: 0.7970 - val_acc: 0.8333\n",
            "Epoch 35/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.1123 - acc: 0.9611 - val_loss: 0.5011 - val_acc: 0.8333\n",
            "Epoch 36/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.1550 - acc: 0.9319 - val_loss: 0.8902 - val_acc: 0.8333\n",
            "Epoch 37/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.1559 - acc: 0.9381 - val_loss: 0.6641 - val_acc: 0.8750\n",
            "Epoch 38/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.1529 - acc: 0.9407 - val_loss: 0.6556 - val_acc: 0.8750\n",
            "Epoch 39/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.1188 - acc: 0.9531 - val_loss: 0.4807 - val_acc: 0.8333\n",
            "Epoch 40/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.1132 - acc: 0.9558 - val_loss: 0.9531 - val_acc: 0.8333\n",
            "Epoch 41/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.2527 - acc: 0.8912 - val_loss: 0.4816 - val_acc: 0.8750\n",
            "Epoch 42/100\n",
            "1130/1130 [==============================] - 0s 352us/step - loss: 0.1283 - acc: 0.9478 - val_loss: 0.8867 - val_acc: 0.8750\n",
            "Epoch 43/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.1473 - acc: 0.9398 - val_loss: 0.8977 - val_acc: 0.8750\n",
            "Epoch 44/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.0765 - acc: 0.9805 - val_loss: 0.6360 - val_acc: 0.8750\n",
            "Epoch 45/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.1635 - acc: 0.9372 - val_loss: 0.6565 - val_acc: 0.8750\n",
            "Epoch 46/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.1267 - acc: 0.9487 - val_loss: 0.6351 - val_acc: 0.8750\n",
            "Epoch 47/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0746 - acc: 0.9735 - val_loss: 0.6480 - val_acc: 0.8750\n",
            "Epoch 48/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.0668 - acc: 0.9805 - val_loss: 0.8538 - val_acc: 0.8750\n",
            "Epoch 49/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0685 - acc: 0.9761 - val_loss: 0.9837 - val_acc: 0.8750\n",
            "Epoch 50/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.0927 - acc: 0.9611 - val_loss: 0.7796 - val_acc: 0.8750\n",
            "Epoch 51/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0761 - acc: 0.9699 - val_loss: 0.8883 - val_acc: 0.8750\n",
            "Epoch 52/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0429 - acc: 0.9858 - val_loss: 0.7879 - val_acc: 0.8750\n",
            "Epoch 53/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0498 - acc: 0.9796 - val_loss: 0.9873 - val_acc: 0.8750\n",
            "Epoch 54/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0323 - acc: 0.9929 - val_loss: 0.9564 - val_acc: 0.8750\n",
            "Epoch 55/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0238 - acc: 0.9938 - val_loss: 1.2606 - val_acc: 0.8750\n",
            "Epoch 56/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.0625 - acc: 0.9752 - val_loss: 1.2143 - val_acc: 0.8750\n",
            "Epoch 57/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0550 - acc: 0.9761 - val_loss: 2.2874 - val_acc: 0.7917\n",
            "Epoch 58/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.1686 - acc: 0.9460 - val_loss: 0.9451 - val_acc: 0.8750\n",
            "Epoch 59/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.1201 - acc: 0.9522 - val_loss: 0.7414 - val_acc: 0.8750\n",
            "Epoch 60/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0490 - acc: 0.9841 - val_loss: 1.0185 - val_acc: 0.8750\n",
            "Epoch 61/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.0341 - acc: 0.9876 - val_loss: 0.9577 - val_acc: 0.8750\n",
            "Epoch 62/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0251 - acc: 0.9920 - val_loss: 1.2628 - val_acc: 0.8750\n",
            "Epoch 63/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0321 - acc: 0.9885 - val_loss: 1.1019 - val_acc: 0.8750\n",
            "Epoch 64/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.2462 - acc: 0.9195 - val_loss: 1.2350 - val_acc: 0.7917\n",
            "Epoch 65/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.1876 - acc: 0.9319 - val_loss: 0.6017 - val_acc: 0.8750\n",
            "Epoch 66/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0574 - acc: 0.9858 - val_loss: 0.9749 - val_acc: 0.8750\n",
            "Epoch 67/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.0430 - acc: 0.9885 - val_loss: 1.3625 - val_acc: 0.7917\n",
            "Epoch 68/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.1786 - acc: 0.9354 - val_loss: 0.6226 - val_acc: 0.8750\n",
            "Epoch 69/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.0409 - acc: 0.9973 - val_loss: 0.8909 - val_acc: 0.8750\n",
            "Epoch 70/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0282 - acc: 0.9982 - val_loss: 0.9928 - val_acc: 0.8750\n",
            "Epoch 71/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0284 - acc: 0.9965 - val_loss: 1.0132 - val_acc: 0.8750\n",
            "Epoch 72/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.0139 - acc: 0.9991 - val_loss: 1.1447 - val_acc: 0.8750\n",
            "Epoch 73/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0214 - acc: 0.9947 - val_loss: 1.0550 - val_acc: 0.8750\n",
            "Epoch 74/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0143 - acc: 0.9991 - val_loss: 1.3007 - val_acc: 0.8750\n",
            "Epoch 75/100\n",
            "1130/1130 [==============================] - 0s 346us/step - loss: 0.0088 - acc: 0.9991 - val_loss: 1.2658 - val_acc: 0.8750\n",
            "Epoch 76/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 1.3861 - val_acc: 0.8750\n",
            "Epoch 77/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 1.1124 - val_acc: 0.8750\n",
            "Epoch 78/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0292 - acc: 0.9867 - val_loss: 1.2382 - val_acc: 0.8750\n",
            "Epoch 79/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 1.4625 - val_acc: 0.8750\n",
            "Epoch 80/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 1.3359 - val_acc: 0.8750\n",
            "Epoch 81/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 1.4537 - val_acc: 0.8750\n",
            "Epoch 82/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 1.3580 - val_acc: 0.8750\n",
            "Epoch 83/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 1.4531 - val_acc: 0.8750\n",
            "Epoch 84/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 1.4714 - val_acc: 0.8750\n",
            "Epoch 85/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 1.4649 - val_acc: 0.8750\n",
            "Epoch 86/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 1.4819 - val_acc: 0.8750\n",
            "Epoch 87/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 1.5740 - val_acc: 0.8750\n",
            "Epoch 88/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 1.5634 - val_acc: 0.8750\n",
            "Epoch 89/100\n",
            "1130/1130 [==============================] - 0s 327us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 1.5188 - val_acc: 0.8750\n",
            "Epoch 90/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 1.5174 - val_acc: 0.8750\n",
            "Epoch 91/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 1.7146 - val_acc: 0.8750\n",
            "Epoch 92/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 1.5576 - val_acc: 0.8750\n",
            "Epoch 93/100\n",
            "1130/1130 [==============================] - 0s 360us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.6765 - val_acc: 0.8750\n",
            "Epoch 94/100\n",
            "1130/1130 [==============================] - 0s 354us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.5865 - val_acc: 0.8750\n",
            "Epoch 95/100\n",
            "1130/1130 [==============================] - 0s 348us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 1.5901 - val_acc: 0.8750\n",
            "Epoch 96/100\n",
            "1130/1130 [==============================] - 0s 360us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 1.6411 - val_acc: 0.8750\n",
            "Epoch 97/100\n",
            "1130/1130 [==============================] - 0s 347us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 1.7292 - val_acc: 0.8750\n",
            "Epoch 98/100\n",
            "1130/1130 [==============================] - 0s 352us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 1.6361 - val_acc: 0.8750\n",
            "Epoch 99/100\n",
            "1130/1130 [==============================] - 0s 365us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 1.6647 - val_acc: 0.8750\n",
            "Epoch 100/100\n",
            "1130/1130 [==============================] - 0s 353us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.6789 - val_acc: 0.8750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u9ijE8UqTz_O",
        "colab": {}
      },
      "source": [
        "# Save model\n",
        "axial_abnormal_model.save('axial_abnormal.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ba69e487-73e0-4553-9f08-3185b461210e",
        "id": "gayaCEgQTz_P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Evaluate the model on Test Set\n",
        "axial_abnormal_scores = axial_abnormal_model.evaluate(axial_abnormal_validation_features, axial_abnormal_validation_labels)\n",
        "print(\"\\n%s: %.2f%%\" % (axial_abnormal_model.metrics_names[1], axial_abnormal_scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 1s 6ms/step\n",
            "\n",
            "acc: 83.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FBboPLoyvNdp"
      },
      "source": [
        "## 8. Axial - ACL CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "16556e87-04c7-4cca-c0a1-5feb334cdb3a",
        "id": "Cp2BveZjT0zV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "# Instantiate convolutional base\n",
        "from keras.applications import VGG16\n",
        "\n",
        "\n",
        "# include_top as false means that we want to train our own classifier on top of the convolutional base\n",
        "\n",
        "axial_acl_conv_base = VGG16(weights='imagenet', \n",
        "                  include_top=False,\n",
        "                  input_shape=(224, 224, 3))  # 3 = number of channels in RGB pictures\n",
        "\n",
        "# Check architecture\n",
        "axial_acl_conv_base.summary()\n",
        "\n",
        "\n",
        "#Extract Features\n",
        "\n",
        "#  X-> final_X_train_sagital\n",
        "#  Y - > Y_train_abnormal\n",
        "    \n",
        "axial_acl_train_features, axial_acl_train_labels = extract_features(axial_acl_conv_base, final_X_train_axial, Y_train_acl, final_X_train_axial.shape[0])  # Agree with our small dataset size\n",
        "\n",
        "axial_acl_validation_features, axial_acl_validation_labels = extract_features(axial_acl_conv_base, final_X_valid_axial, Y_valid_acl, final_X_valid_axial.shape[0])\n",
        "\n",
        "axial_acl_test_features, axial_acl_test_labels = extract_features(axial_acl_conv_base, final_X_test_axial, Y_test_acl, final_X_test_axial.shape[0])\n",
        "\n",
        "print(\"DONE !\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "DONE !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f24d5c2d-beb9-43af-fafe-16e4d954dd79",
        "id": "CB86KXIqT0zo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Define model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "axial_acl_model = models.Sequential()\n",
        "axial_acl_model.add(layers.Flatten(input_shape=(7,7,512)))\n",
        "axial_acl_model.add(layers.Dense(256, activation='relu', input_dim=(7*7*512)))\n",
        "axial_acl_model.add(layers.Dense(128, activation='relu'))\n",
        "axial_acl_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "axial_acl_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_8 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 6,455,809\n",
            "Trainable params: 6,455,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e7c5ad43-18de-4570-b5f8-9166379ac2e5",
        "id": "RL_DAtKqT0zx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3394
        }
      },
      "source": [
        "# Compile model\n",
        "axial_acl_model.compile(optimizer=optimizers.Adam(),\n",
        "                        loss='binary_crossentropy',\n",
        "                        metrics=['acc'])\n",
        "\n",
        "# Train model\n",
        "axial_acl_history = axial_acl_model.fit(axial_acl_train_features, axial_acl_train_labels,\n",
        "                                        epochs=epochs,\n",
        "                                        batch_size=batch_size,\n",
        "                                        validation_data=(axial_acl_validation_features, axial_acl_validation_labels))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1130 samples, validate on 24 samples\n",
            "Epoch 1/100\n",
            "1130/1130 [==============================] - 2s 1ms/step - loss: 0.5831 - acc: 0.7867 - val_loss: 1.1122 - val_acc: 0.5833\n",
            "Epoch 2/100\n",
            "1130/1130 [==============================] - 0s 347us/step - loss: 0.4568 - acc: 0.8159 - val_loss: 1.2677 - val_acc: 0.5833\n",
            "Epoch 3/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.4399 - acc: 0.8177 - val_loss: 0.8593 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.4077 - acc: 0.8212 - val_loss: 1.4227 - val_acc: 0.5833\n",
            "Epoch 5/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.4100 - acc: 0.8212 - val_loss: 1.3568 - val_acc: 0.5833\n",
            "Epoch 6/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.4190 - acc: 0.8292 - val_loss: 1.0506 - val_acc: 0.5417\n",
            "Epoch 7/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.3735 - acc: 0.8354 - val_loss: 0.9418 - val_acc: 0.3750\n",
            "Epoch 8/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.3964 - acc: 0.8283 - val_loss: 1.0033 - val_acc: 0.3750\n",
            "Epoch 9/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.3949 - acc: 0.8310 - val_loss: 1.2196 - val_acc: 0.5417\n",
            "Epoch 10/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.4105 - acc: 0.8345 - val_loss: 1.0226 - val_acc: 0.5417\n",
            "Epoch 11/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.3574 - acc: 0.8469 - val_loss: 1.2301 - val_acc: 0.5833\n",
            "Epoch 12/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.3289 - acc: 0.8637 - val_loss: 1.1756 - val_acc: 0.5000\n",
            "Epoch 13/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.3175 - acc: 0.8646 - val_loss: 1.1877 - val_acc: 0.5000\n",
            "Epoch 14/100\n",
            "1130/1130 [==============================] - 0s 348us/step - loss: 0.3516 - acc: 0.8549 - val_loss: 0.9918 - val_acc: 0.5000\n",
            "Epoch 15/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.3444 - acc: 0.8566 - val_loss: 1.1368 - val_acc: 0.5417\n",
            "Epoch 16/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.3499 - acc: 0.8575 - val_loss: 1.7095 - val_acc: 0.5833\n",
            "Epoch 17/100\n",
            "1130/1130 [==============================] - 0s 348us/step - loss: 0.3450 - acc: 0.8558 - val_loss: 1.2188 - val_acc: 0.5417\n",
            "Epoch 18/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.3029 - acc: 0.8761 - val_loss: 1.1015 - val_acc: 0.3750\n",
            "Epoch 19/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.2784 - acc: 0.8823 - val_loss: 1.3495 - val_acc: 0.5000\n",
            "Epoch 20/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.2690 - acc: 0.8867 - val_loss: 1.2155 - val_acc: 0.3750\n",
            "Epoch 21/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.2795 - acc: 0.8850 - val_loss: 1.6891 - val_acc: 0.5417\n",
            "Epoch 22/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.2259 - acc: 0.9044 - val_loss: 1.6284 - val_acc: 0.4583\n",
            "Epoch 23/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.2516 - acc: 0.8929 - val_loss: 1.3652 - val_acc: 0.4583\n",
            "Epoch 24/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.2541 - acc: 0.8912 - val_loss: 2.1067 - val_acc: 0.5417\n",
            "Epoch 25/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.2440 - acc: 0.8982 - val_loss: 1.7171 - val_acc: 0.5000\n",
            "Epoch 26/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.1957 - acc: 0.9212 - val_loss: 1.4699 - val_acc: 0.4583\n",
            "Epoch 27/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.1938 - acc: 0.9195 - val_loss: 1.4932 - val_acc: 0.4583\n",
            "Epoch 28/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.2087 - acc: 0.9159 - val_loss: 1.9223 - val_acc: 0.4583\n",
            "Epoch 29/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.1953 - acc: 0.9257 - val_loss: 1.8464 - val_acc: 0.5000\n",
            "Epoch 30/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.2509 - acc: 0.9018 - val_loss: 1.4768 - val_acc: 0.3750\n",
            "Epoch 31/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.1662 - acc: 0.9319 - val_loss: 2.1598 - val_acc: 0.5417\n",
            "Epoch 32/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.1480 - acc: 0.9460 - val_loss: 1.4575 - val_acc: 0.3750\n",
            "Epoch 33/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.1806 - acc: 0.9292 - val_loss: 1.9432 - val_acc: 0.4583\n",
            "Epoch 34/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.1917 - acc: 0.9186 - val_loss: 1.5630 - val_acc: 0.3750\n",
            "Epoch 35/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.1342 - acc: 0.9487 - val_loss: 1.9052 - val_acc: 0.4583\n",
            "Epoch 36/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.1123 - acc: 0.9584 - val_loss: 1.8634 - val_acc: 0.5000\n",
            "Epoch 37/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.2125 - acc: 0.9106 - val_loss: 2.5363 - val_acc: 0.5417\n",
            "Epoch 38/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.2476 - acc: 0.9115 - val_loss: 1.4986 - val_acc: 0.4583\n",
            "Epoch 39/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0979 - acc: 0.9699 - val_loss: 2.1024 - val_acc: 0.5000\n",
            "Epoch 40/100\n",
            "1130/1130 [==============================] - 0s 326us/step - loss: 0.1223 - acc: 0.9558 - val_loss: 1.6552 - val_acc: 0.4167\n",
            "Epoch 41/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.1609 - acc: 0.9239 - val_loss: 1.8894 - val_acc: 0.5000\n",
            "Epoch 42/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.1866 - acc: 0.9168 - val_loss: 1.6426 - val_acc: 0.4167\n",
            "Epoch 43/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.1959 - acc: 0.9097 - val_loss: 2.3011 - val_acc: 0.5417\n",
            "Epoch 44/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.1116 - acc: 0.9566 - val_loss: 2.0758 - val_acc: 0.4583\n",
            "Epoch 45/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0792 - acc: 0.9814 - val_loss: 2.4653 - val_acc: 0.4583\n",
            "Epoch 46/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.1036 - acc: 0.9619 - val_loss: 2.3081 - val_acc: 0.5000\n",
            "Epoch 47/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0655 - acc: 0.9823 - val_loss: 2.3211 - val_acc: 0.4583\n",
            "Epoch 48/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0966 - acc: 0.9646 - val_loss: 3.2184 - val_acc: 0.5417\n",
            "Epoch 49/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.0679 - acc: 0.9805 - val_loss: 2.5328 - val_acc: 0.4583\n",
            "Epoch 50/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0544 - acc: 0.9876 - val_loss: 2.8752 - val_acc: 0.4583\n",
            "Epoch 51/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0561 - acc: 0.9841 - val_loss: 3.0097 - val_acc: 0.4583\n",
            "Epoch 52/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.0607 - acc: 0.9779 - val_loss: 2.7462 - val_acc: 0.4583\n",
            "Epoch 53/100\n",
            "1130/1130 [==============================] - 0s 324us/step - loss: 0.0281 - acc: 0.9956 - val_loss: 2.9128 - val_acc: 0.4583\n",
            "Epoch 54/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.0527 - acc: 0.9850 - val_loss: 2.6640 - val_acc: 0.5000\n",
            "Epoch 55/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0951 - acc: 0.9558 - val_loss: 2.6789 - val_acc: 0.5000\n",
            "Epoch 56/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0594 - acc: 0.9788 - val_loss: 2.7651 - val_acc: 0.5000\n",
            "Epoch 57/100\n",
            "1130/1130 [==============================] - 0s 370us/step - loss: 0.0481 - acc: 0.9832 - val_loss: 3.3381 - val_acc: 0.4583\n",
            "Epoch 58/100\n",
            "1130/1130 [==============================] - 0s 361us/step - loss: 0.2044 - acc: 0.9239 - val_loss: 1.9753 - val_acc: 0.4167\n",
            "Epoch 59/100\n",
            "1130/1130 [==============================] - 0s 360us/step - loss: 0.1544 - acc: 0.9363 - val_loss: 2.4233 - val_acc: 0.4583\n",
            "Epoch 60/100\n",
            "1130/1130 [==============================] - 0s 360us/step - loss: 0.0362 - acc: 0.9929 - val_loss: 2.3543 - val_acc: 0.5000\n",
            "Epoch 61/100\n",
            "1130/1130 [==============================] - 0s 360us/step - loss: 0.0247 - acc: 0.9973 - val_loss: 2.7881 - val_acc: 0.4583\n",
            "Epoch 62/100\n",
            "1130/1130 [==============================] - 0s 379us/step - loss: 0.0212 - acc: 0.9991 - val_loss: 2.9289 - val_acc: 0.4583\n",
            "Epoch 63/100\n",
            "1130/1130 [==============================] - 0s 371us/step - loss: 0.0187 - acc: 0.9982 - val_loss: 3.1146 - val_acc: 0.4583\n",
            "Epoch 64/100\n",
            "1130/1130 [==============================] - 0s 380us/step - loss: 0.0273 - acc: 0.9920 - val_loss: 2.8828 - val_acc: 0.5000\n",
            "Epoch 65/100\n",
            "1130/1130 [==============================] - 0s 361us/step - loss: 0.0311 - acc: 0.9894 - val_loss: 3.3494 - val_acc: 0.4583\n",
            "Epoch 66/100\n",
            "1130/1130 [==============================] - 0s 357us/step - loss: 0.0120 - acc: 0.9991 - val_loss: 3.1057 - val_acc: 0.5000\n",
            "Epoch 67/100\n",
            "1130/1130 [==============================] - 0s 375us/step - loss: 0.0267 - acc: 0.9929 - val_loss: 3.6537 - val_acc: 0.4583\n",
            "Epoch 68/100\n",
            "1130/1130 [==============================] - 0s 364us/step - loss: 0.1305 - acc: 0.9584 - val_loss: 5.1089 - val_acc: 0.5417\n",
            "Epoch 69/100\n",
            "1130/1130 [==============================] - 0s 376us/step - loss: 0.2390 - acc: 0.9283 - val_loss: 2.8381 - val_acc: 0.4583\n",
            "Epoch 70/100\n",
            "1130/1130 [==============================] - 0s 361us/step - loss: 0.0305 - acc: 0.9956 - val_loss: 2.9856 - val_acc: 0.4583\n",
            "Epoch 71/100\n",
            "1130/1130 [==============================] - 0s 360us/step - loss: 0.0153 - acc: 0.9991 - val_loss: 3.1496 - val_acc: 0.4583\n",
            "Epoch 72/100\n",
            "1130/1130 [==============================] - 0s 369us/step - loss: 0.0108 - acc: 1.0000 - val_loss: 3.3982 - val_acc: 0.4583\n",
            "Epoch 73/100\n",
            "1130/1130 [==============================] - 0s 365us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 3.5124 - val_acc: 0.4583\n",
            "Epoch 74/100\n",
            "1130/1130 [==============================] - 0s 366us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 3.4545 - val_acc: 0.5000\n",
            "Epoch 75/100\n",
            "1130/1130 [==============================] - 0s 358us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 3.5715 - val_acc: 0.5000\n",
            "Epoch 76/100\n",
            "1130/1130 [==============================] - 0s 363us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.8128 - val_acc: 0.4583\n",
            "Epoch 77/100\n",
            "1130/1130 [==============================] - 0s 367us/step - loss: 0.0059 - acc: 0.9991 - val_loss: 3.8789 - val_acc: 0.4583\n",
            "Epoch 78/100\n",
            "1130/1130 [==============================] - 0s 354us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 3.8523 - val_acc: 0.5000\n",
            "Epoch 79/100\n",
            "1130/1130 [==============================] - 0s 372us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.8489 - val_acc: 0.5000\n",
            "Epoch 80/100\n",
            "1130/1130 [==============================] - 0s 362us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 4.3998 - val_acc: 0.4583\n",
            "Epoch 81/100\n",
            "1130/1130 [==============================] - 0s 357us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 4.1788 - val_acc: 0.4583\n",
            "Epoch 82/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 4.6009 - val_acc: 0.4583\n",
            "Epoch 83/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0285 - acc: 0.9903 - val_loss: 3.4794 - val_acc: 0.4583\n",
            "Epoch 84/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0232 - acc: 0.9929 - val_loss: 4.4173 - val_acc: 0.5000\n",
            "Epoch 85/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 4.2541 - val_acc: 0.5000\n",
            "Epoch 86/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 4.2407 - val_acc: 0.5000\n",
            "Epoch 87/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 4.6907 - val_acc: 0.4583\n",
            "Epoch 88/100\n",
            "1130/1130 [==============================] - 0s 328us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 4.5482 - val_acc: 0.4583\n",
            "Epoch 89/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 4.5725 - val_acc: 0.4583\n",
            "Epoch 90/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 4.5626 - val_acc: 0.4583\n",
            "Epoch 91/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 9.3127e-04 - acc: 1.0000 - val_loss: 4.5504 - val_acc: 0.5000\n",
            "Epoch 92/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 8.8667e-04 - acc: 1.0000 - val_loss: 4.6971 - val_acc: 0.4583\n",
            "Epoch 93/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 9.5551e-04 - acc: 1.0000 - val_loss: 4.7100 - val_acc: 0.4583\n",
            "Epoch 94/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 9.5162e-04 - acc: 1.0000 - val_loss: 4.6332 - val_acc: 0.5000\n",
            "Epoch 95/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 8.8631e-04 - acc: 1.0000 - val_loss: 4.7092 - val_acc: 0.4583\n",
            "Epoch 96/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 7.1263e-04 - acc: 1.0000 - val_loss: 4.6842 - val_acc: 0.5000\n",
            "Epoch 97/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 7.1743e-04 - acc: 1.0000 - val_loss: 4.7344 - val_acc: 0.4583\n",
            "Epoch 98/100\n",
            "1130/1130 [==============================] - 0s 351us/step - loss: 7.1191e-04 - acc: 1.0000 - val_loss: 4.7884 - val_acc: 0.4583\n",
            "Epoch 99/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 6.5790e-04 - acc: 1.0000 - val_loss: 4.7698 - val_acc: 0.4583\n",
            "Epoch 100/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 6.3729e-04 - acc: 1.0000 - val_loss: 4.8184 - val_acc: 0.4583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "haXswEhHT0z1",
        "colab": {}
      },
      "source": [
        "# Save model\n",
        "axial_acl_model.save('axial_acl.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fd287af0-254b-46f3-972a-324a82d5f663",
        "id": "u3TtahjyT0z3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Evaluate the model on Test Set\n",
        "axial_acl_scores = axial_acl_model.evaluate(axial_acl_validation_features, axial_acl_validation_labels)\n",
        "print(\"\\n%s: %.2f%%\" % (axial_acl_model.metrics_names[1], axial_acl_scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 1s 6ms/step\n",
            "\n",
            "acc: 64.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JdQbOpzQvS5R"
      },
      "source": [
        "## 9. Axial - Meniscus CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1455c917-f1c2-4c8a-c17d-f652e03109a7",
        "id": "EUbTOaIyT1xO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "# Instantiate convolutional base\n",
        "from keras.applications import VGG16\n",
        "\n",
        "\n",
        "# include_top as false means that we want to train our own classifier on top of the convolutional base\n",
        "\n",
        "axial_meniscus_conv_base = VGG16(weights='imagenet', \n",
        "                  include_top=False,\n",
        "                  input_shape=(224, 224, 3))  # 3 = number of channels in RGB pictures\n",
        "\n",
        "# Check architecture\n",
        "axial_meniscus_conv_base.summary()\n",
        "\n",
        "\n",
        "#Extract features\n",
        "\n",
        "#  X-> final_X_train_sagital\n",
        "#  Y - > Y_train_abnormal\n",
        "    \n",
        "axial_meniscus_train_features, axial_meniscus_train_labels = extract_features(axial_meniscus_conv_base, final_X_train_axial, Y_train_meniscus, final_X_train_axial.shape[0])  # Agree with our small dataset size\n",
        "\n",
        "axial_meniscus_validation_features, axial_meniscus_validation_labels = extract_features(axial_meniscus_conv_base, final_X_valid_axial, Y_valid_meniscus, final_X_valid_axial.shape[0])\n",
        "\n",
        "axial_meniscus_test_features, axial_meniscus_test_labels = extract_features(axial_meniscus_conv_base, final_X_test_axial, Y_test_meniscus, final_X_test_axial.shape[0])\n",
        "\n",
        "print(\"DONE !\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "DONE !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9eaab240-fc99-4461-feb7-b45b374595f6",
        "id": "qYynbzbGT1xh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Define model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "axial_meniscus_model = models.Sequential()\n",
        "axial_meniscus_model.add(layers.Flatten(input_shape=(7,7,512)))\n",
        "axial_meniscus_model.add(layers.Dense(256, activation='relu', input_dim=(7*7*512)))\n",
        "axial_meniscus_model.add(layers.Dense(128, activation='relu'))\n",
        "axial_meniscus_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "axial_meniscus_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_9 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 6,455,809\n",
            "Trainable params: 6,455,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d0df765b-66e3-4deb-dc0e-431b40e10e03",
        "id": "9XvGAhoYT1xp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Compile model\n",
        "axial_meniscus_model.compile(optimizer=optimizers.Adam(),\n",
        "                              loss='binary_crossentropy',\n",
        "                              metrics=['acc'])\n",
        "\n",
        "# Train model\n",
        "axial_meniscus_history = axial_meniscus_model.fit(axial_meniscus_train_features, axial_meniscus_train_labels,\n",
        "                                                  epochs=epochs,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  validation_data=(axial_meniscus_validation_features, axial_meniscus_validation_labels))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1130 samples, validate on 24 samples\n",
            "Epoch 1/100\n",
            "1130/1130 [==============================] - 2s 2ms/step - loss: 0.8029 - acc: 0.6195 - val_loss: 0.7642 - val_acc: 0.5417\n",
            "Epoch 2/100\n",
            "1130/1130 [==============================] - 0s 359us/step - loss: 0.6060 - acc: 0.6664 - val_loss: 0.7686 - val_acc: 0.5417\n",
            "Epoch 3/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.6055 - acc: 0.6628 - val_loss: 0.7467 - val_acc: 0.5417\n",
            "Epoch 4/100\n",
            "1130/1130 [==============================] - 0s 359us/step - loss: 0.5748 - acc: 0.6814 - val_loss: 0.7353 - val_acc: 0.5833\n",
            "Epoch 5/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.5634 - acc: 0.6858 - val_loss: 0.7362 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            "1130/1130 [==============================] - 0s 363us/step - loss: 0.5598 - acc: 0.7009 - val_loss: 0.7059 - val_acc: 0.5417\n",
            "Epoch 7/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.5931 - acc: 0.6770 - val_loss: 0.7255 - val_acc: 0.5000\n",
            "Epoch 8/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.5811 - acc: 0.6805 - val_loss: 0.7179 - val_acc: 0.5000\n",
            "Epoch 9/100\n",
            "1130/1130 [==============================] - 0s 356us/step - loss: 0.5494 - acc: 0.7044 - val_loss: 0.8175 - val_acc: 0.5417\n",
            "Epoch 10/100\n",
            "1130/1130 [==============================] - 0s 349us/step - loss: 0.5538 - acc: 0.7159 - val_loss: 0.6929 - val_acc: 0.7500\n",
            "Epoch 11/100\n",
            "1130/1130 [==============================] - 0s 364us/step - loss: 0.5114 - acc: 0.7478 - val_loss: 0.8168 - val_acc: 0.4583\n",
            "Epoch 12/100\n",
            "1130/1130 [==============================] - 0s 347us/step - loss: 0.5154 - acc: 0.7327 - val_loss: 0.6981 - val_acc: 0.6250\n",
            "Epoch 13/100\n",
            "1130/1130 [==============================] - 0s 348us/step - loss: 0.4904 - acc: 0.7442 - val_loss: 0.8521 - val_acc: 0.5417\n",
            "Epoch 14/100\n",
            "1130/1130 [==============================] - 0s 360us/step - loss: 0.5146 - acc: 0.7425 - val_loss: 0.7100 - val_acc: 0.6250\n",
            "Epoch 15/100\n",
            "1130/1130 [==============================] - 0s 351us/step - loss: 0.4719 - acc: 0.7690 - val_loss: 0.8336 - val_acc: 0.5000\n",
            "Epoch 16/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.5287 - acc: 0.7248 - val_loss: 0.7134 - val_acc: 0.5417\n",
            "Epoch 17/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.5245 - acc: 0.7416 - val_loss: 0.7198 - val_acc: 0.7083\n",
            "Epoch 18/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.4552 - acc: 0.7708 - val_loss: 0.7370 - val_acc: 0.6250\n",
            "Epoch 19/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.4577 - acc: 0.7823 - val_loss: 0.7033 - val_acc: 0.6250\n",
            "Epoch 20/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.5234 - acc: 0.7398 - val_loss: 0.7034 - val_acc: 0.5833\n",
            "Epoch 21/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.4360 - acc: 0.7867 - val_loss: 0.7934 - val_acc: 0.5833\n",
            "Epoch 22/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.4327 - acc: 0.7858 - val_loss: 0.8032 - val_acc: 0.5833\n",
            "Epoch 23/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.4294 - acc: 0.8009 - val_loss: 0.8585 - val_acc: 0.5833\n",
            "Epoch 24/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.4090 - acc: 0.8027 - val_loss: 0.7776 - val_acc: 0.5417\n",
            "Epoch 25/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.3668 - acc: 0.8345 - val_loss: 0.8960 - val_acc: 0.5833\n",
            "Epoch 26/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.3423 - acc: 0.8496 - val_loss: 1.0309 - val_acc: 0.5833\n",
            "Epoch 27/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.4182 - acc: 0.7894 - val_loss: 1.0330 - val_acc: 0.5000\n",
            "Epoch 28/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.4599 - acc: 0.7726 - val_loss: 0.7964 - val_acc: 0.5417\n",
            "Epoch 29/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.3714 - acc: 0.8487 - val_loss: 0.8362 - val_acc: 0.5833\n",
            "Epoch 30/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.3771 - acc: 0.8319 - val_loss: 0.8170 - val_acc: 0.5000\n",
            "Epoch 31/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.3385 - acc: 0.8540 - val_loss: 0.9186 - val_acc: 0.6250\n",
            "Epoch 32/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.4012 - acc: 0.8044 - val_loss: 0.9734 - val_acc: 0.5417\n",
            "Epoch 33/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.3828 - acc: 0.8221 - val_loss: 1.0157 - val_acc: 0.5833\n",
            "Epoch 34/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.3233 - acc: 0.8575 - val_loss: 0.8864 - val_acc: 0.6250\n",
            "Epoch 35/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.3194 - acc: 0.8602 - val_loss: 1.1251 - val_acc: 0.4583\n",
            "Epoch 36/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.3820 - acc: 0.8142 - val_loss: 1.0489 - val_acc: 0.5417\n",
            "Epoch 37/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.3717 - acc: 0.8195 - val_loss: 1.2835 - val_acc: 0.5417\n",
            "Epoch 38/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.3483 - acc: 0.8478 - val_loss: 0.8913 - val_acc: 0.5000\n",
            "Epoch 39/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.2525 - acc: 0.9124 - val_loss: 0.9164 - val_acc: 0.4583\n",
            "Epoch 40/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.2634 - acc: 0.8894 - val_loss: 0.9716 - val_acc: 0.5000\n",
            "Epoch 41/100\n",
            "1130/1130 [==============================] - 0s 342us/step - loss: 0.2491 - acc: 0.9097 - val_loss: 0.9802 - val_acc: 0.5417\n",
            "Epoch 42/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.2452 - acc: 0.9027 - val_loss: 1.3214 - val_acc: 0.5833\n",
            "Epoch 43/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.2367 - acc: 0.9088 - val_loss: 1.1094 - val_acc: 0.5417\n",
            "Epoch 44/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.2647 - acc: 0.8912 - val_loss: 1.1242 - val_acc: 0.5417\n",
            "Epoch 45/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.2324 - acc: 0.9088 - val_loss: 1.0791 - val_acc: 0.5000\n",
            "Epoch 46/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.2567 - acc: 0.8876 - val_loss: 1.3457 - val_acc: 0.5833\n",
            "Epoch 47/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.2514 - acc: 0.8858 - val_loss: 1.3135 - val_acc: 0.5833\n",
            "Epoch 48/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.2554 - acc: 0.8832 - val_loss: 1.1354 - val_acc: 0.4583\n",
            "Epoch 49/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.2075 - acc: 0.9230 - val_loss: 1.1210 - val_acc: 0.5417\n",
            "Epoch 50/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.1843 - acc: 0.9363 - val_loss: 1.1753 - val_acc: 0.4583\n",
            "Epoch 51/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.1662 - acc: 0.9389 - val_loss: 1.3573 - val_acc: 0.5000\n",
            "Epoch 52/100\n",
            "1130/1130 [==============================] - 0s 351us/step - loss: 0.1540 - acc: 0.9478 - val_loss: 1.2487 - val_acc: 0.4583\n",
            "Epoch 53/100\n",
            "1130/1130 [==============================] - 0s 354us/step - loss: 0.1558 - acc: 0.9478 - val_loss: 1.2644 - val_acc: 0.5417\n",
            "Epoch 54/100\n",
            "1130/1130 [==============================] - 0s 360us/step - loss: 0.1254 - acc: 0.9655 - val_loss: 1.3250 - val_acc: 0.5000\n",
            "Epoch 55/100\n",
            "1130/1130 [==============================] - 0s 358us/step - loss: 0.1580 - acc: 0.9372 - val_loss: 1.3252 - val_acc: 0.5000\n",
            "Epoch 56/100\n",
            "1130/1130 [==============================] - 0s 360us/step - loss: 0.1383 - acc: 0.9416 - val_loss: 1.3587 - val_acc: 0.4583\n",
            "Epoch 57/100\n",
            "1130/1130 [==============================] - 0s 368us/step - loss: 0.1330 - acc: 0.9513 - val_loss: 1.4759 - val_acc: 0.4583\n",
            "Epoch 58/100\n",
            "1130/1130 [==============================] - 0s 357us/step - loss: 0.1763 - acc: 0.9310 - val_loss: 1.4353 - val_acc: 0.5833\n",
            "Epoch 59/100\n",
            "1130/1130 [==============================] - 0s 364us/step - loss: 0.1692 - acc: 0.9248 - val_loss: 1.4486 - val_acc: 0.5000\n",
            "Epoch 60/100\n",
            "1130/1130 [==============================] - 0s 356us/step - loss: 0.1192 - acc: 0.9584 - val_loss: 1.5259 - val_acc: 0.4583\n",
            "Epoch 61/100\n",
            "1130/1130 [==============================] - 0s 366us/step - loss: 0.0883 - acc: 0.9726 - val_loss: 1.5614 - val_acc: 0.4583\n",
            "Epoch 62/100\n",
            "1130/1130 [==============================] - 0s 375us/step - loss: 0.0915 - acc: 0.9699 - val_loss: 1.5058 - val_acc: 0.5000\n",
            "Epoch 63/100\n",
            "1130/1130 [==============================] - 0s 367us/step - loss: 0.1250 - acc: 0.9531 - val_loss: 1.8128 - val_acc: 0.5417\n",
            "Epoch 64/100\n",
            "1130/1130 [==============================] - 0s 356us/step - loss: 0.2149 - acc: 0.8973 - val_loss: 2.0982 - val_acc: 0.4583\n",
            "Epoch 65/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.1683 - acc: 0.9283 - val_loss: 1.9058 - val_acc: 0.4583\n",
            "Epoch 66/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0996 - acc: 0.9637 - val_loss: 1.5249 - val_acc: 0.4583\n",
            "Epoch 67/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.0675 - acc: 0.9841 - val_loss: 1.7540 - val_acc: 0.4583\n",
            "Epoch 68/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0661 - acc: 0.9832 - val_loss: 1.6052 - val_acc: 0.5000\n",
            "Epoch 69/100\n",
            "1130/1130 [==============================] - 0s 343us/step - loss: 0.0625 - acc: 0.9850 - val_loss: 1.8460 - val_acc: 0.5000\n",
            "Epoch 70/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0585 - acc: 0.9876 - val_loss: 1.9400 - val_acc: 0.5000\n",
            "Epoch 71/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0758 - acc: 0.9735 - val_loss: 1.8077 - val_acc: 0.5000\n",
            "Epoch 72/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.1274 - acc: 0.9522 - val_loss: 1.9412 - val_acc: 0.5000\n",
            "Epoch 73/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0689 - acc: 0.9770 - val_loss: 1.7561 - val_acc: 0.5000\n",
            "Epoch 74/100\n",
            "1130/1130 [==============================] - 0s 329us/step - loss: 0.0391 - acc: 0.9912 - val_loss: 1.9708 - val_acc: 0.4583\n",
            "Epoch 75/100\n",
            "1130/1130 [==============================] - 0s 346us/step - loss: 0.1006 - acc: 0.9646 - val_loss: 2.2979 - val_acc: 0.5833\n",
            "Epoch 76/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.3825 - acc: 0.8655 - val_loss: 1.7566 - val_acc: 0.5833\n",
            "Epoch 77/100\n",
            "1130/1130 [==============================] - 0s 345us/step - loss: 0.0531 - acc: 0.9867 - val_loss: 1.6508 - val_acc: 0.5417\n",
            "Epoch 78/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0437 - acc: 0.9912 - val_loss: 1.6535 - val_acc: 0.5000\n",
            "Epoch 79/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0385 - acc: 0.9938 - val_loss: 1.8257 - val_acc: 0.4583\n",
            "Epoch 80/100\n",
            "1130/1130 [==============================] - 0s 337us/step - loss: 0.0469 - acc: 0.9903 - val_loss: 1.7457 - val_acc: 0.5417\n",
            "Epoch 81/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.0267 - acc: 0.9982 - val_loss: 1.7474 - val_acc: 0.5417\n",
            "Epoch 82/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0339 - acc: 0.9947 - val_loss: 1.8298 - val_acc: 0.5000\n",
            "Epoch 83/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.0260 - acc: 0.9973 - val_loss: 1.9449 - val_acc: 0.5000\n",
            "Epoch 84/100\n",
            "1130/1130 [==============================] - 0s 338us/step - loss: 0.0298 - acc: 0.9929 - val_loss: 2.0536 - val_acc: 0.5417\n",
            "Epoch 85/100\n",
            "1130/1130 [==============================] - 0s 331us/step - loss: 0.6565 - acc: 0.8177 - val_loss: 1.1536 - val_acc: 0.4583\n",
            "Epoch 86/100\n",
            "1130/1130 [==============================] - 0s 350us/step - loss: 0.1037 - acc: 0.9690 - val_loss: 1.3317 - val_acc: 0.5417\n",
            "Epoch 87/100\n",
            "1130/1130 [==============================] - 0s 340us/step - loss: 0.0667 - acc: 0.9867 - val_loss: 1.4958 - val_acc: 0.5417\n",
            "Epoch 88/100\n",
            "1130/1130 [==============================] - 0s 346us/step - loss: 0.0887 - acc: 0.9708 - val_loss: 1.7116 - val_acc: 0.5000\n",
            "Epoch 89/100\n",
            "1130/1130 [==============================] - 0s 333us/step - loss: 0.0715 - acc: 0.9841 - val_loss: 1.4325 - val_acc: 0.5000\n",
            "Epoch 90/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0414 - acc: 0.9947 - val_loss: 1.5359 - val_acc: 0.5000\n",
            "Epoch 91/100\n",
            "1130/1130 [==============================] - 0s 341us/step - loss: 0.0386 - acc: 0.9965 - val_loss: 1.6368 - val_acc: 0.5000\n",
            "Epoch 92/100\n",
            "1130/1130 [==============================] - 0s 334us/step - loss: 0.0477 - acc: 0.9929 - val_loss: 2.1428 - val_acc: 0.5417\n",
            "Epoch 93/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.0385 - acc: 0.9965 - val_loss: 1.7104 - val_acc: 0.5000\n",
            "Epoch 94/100\n",
            "1130/1130 [==============================] - 0s 344us/step - loss: 0.0240 - acc: 1.0000 - val_loss: 1.7916 - val_acc: 0.5000\n",
            "Epoch 95/100\n",
            "1130/1130 [==============================] - 0s 332us/step - loss: 0.0282 - acc: 0.9982 - val_loss: 1.8295 - val_acc: 0.4167\n",
            "Epoch 96/100\n",
            "1130/1130 [==============================] - 0s 335us/step - loss: 0.0390 - acc: 0.9947 - val_loss: 1.8970 - val_acc: 0.4167\n",
            "Epoch 97/100\n",
            "1130/1130 [==============================] - 0s 336us/step - loss: 0.2772 - acc: 0.8867 - val_loss: 2.5469 - val_acc: 0.4583\n",
            "Epoch 98/100\n",
            "1130/1130 [==============================] - 0s 330us/step - loss: 0.0838 - acc: 0.9699 - val_loss: 1.6273 - val_acc: 0.5833\n",
            "Epoch 99/100\n",
            "1130/1130 [==============================] - 0s 357us/step - loss: 0.0258 - acc: 0.9982 - val_loss: 1.6725 - val_acc: 0.5417\n",
            "Epoch 100/100\n",
            "1130/1130 [==============================] - 0s 339us/step - loss: 0.0414 - acc: 0.9903 - val_loss: 1.7548 - val_acc: 0.5417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v2ILBJBdT1xs",
        "colab": {}
      },
      "source": [
        "# Save model\n",
        "axial_meniscus_model.save('axial_meniscus.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c344003a-6996-40f1-ae89-24194def2af8",
        "id": "_7BxwgRlT1xt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Evaluate the model on Test Set\n",
        "axial_meniscus_scores = axial_meniscus_model.evaluate(axial_meniscus_validation_features, axial_meniscus_validation_labels)\n",
        "print(\"\\n%s: %.2f%%\" % (axial_meniscus_model.metrics_names[1], axial_meniscus_scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 1s 7ms/step\n",
            "\n",
            "acc: 55.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlvfVFR0yXdR",
        "colab_type": "text"
      },
      "source": [
        "# Ensemble Models + Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFNflJJ-vQhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### F1 Score Function\n",
        "import keras.backend as K\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "\n",
        "    # Count positive samples.\n",
        "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "\n",
        "    # If there are no true samples, fix the F1 score at 0.\n",
        "    if c3 == 0:\n",
        "        return 0\n",
        "\n",
        "    # How many selected items are relevant?\n",
        "    precision = c1 / c2\n",
        "\n",
        "    # How many relevant items are selected?\n",
        "    recall = c1 / c3\n",
        "\n",
        "    # Calculate f1_score\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0237Xobvj7K",
        "colab_type": "text"
      },
      "source": [
        "## Abnormal Stacked Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrvKnQmwyzxK",
        "colab_type": "code",
        "outputId": "45b995dd-e0a9-4ed2-a017-20109a91a1a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1734
        }
      },
      "source": [
        "### Abnormal stacked Ensemble\n",
        "\n",
        "abnormal_stacked_model = models.Sequential()\n",
        "\n",
        "abnormal_stacked_model.add(layers.Dense(1, activation='sigmoid', input_dim=3))\n",
        "\n",
        "#compile model\n",
        "abnormal_stacked_model.compile(loss='binary_crossentropy',\n",
        "                                optimizer='adam',\n",
        "                                metrics=['acc', f1_score])\n",
        "\n",
        "\n",
        "#Ensemble models\n",
        "train_dat = np.hstack(( sagital_abnormal_model.predict(sagital_abnormal_train_features),\n",
        "                       coronal_abnormal_model.predict(coronal_abnormal_train_features),\n",
        "                       axial_abnormal_model.predict(axial_abnormal_train_features) ) )\n",
        "\n",
        "valid_dat = np.hstack(( sagital_abnormal_model.predict(sagital_abnormal_validation_features),\n",
        "                       coronal_abnormal_model.predict(coronal_abnormal_validation_features),\n",
        "                       axial_abnormal_model.predict(axial_abnormal_validation_features) ) )\n",
        "\n",
        "#Train Model\n",
        "history = abnormal_stacked_model.fit(train_dat, Y_train_abnormal,\n",
        "                           batch_size=32, epochs=50,\n",
        "                           validation_data=(valid_dat, Y_valid_abnormal))\n",
        "\n",
        "# #Test Scores\n",
        "# abnormal_stacked_scores = abnormal_stacked_model.evaluate(axial_meniscus_validation_features, axial_meniscus_validation_labels)\n",
        "# print(\"\\n%s: %.2f%%\" % (axial_meniscus_model.metrics_names[1], axial_meniscus_scores[1]*100))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1130 samples, validate on 24 samples\n",
            "Epoch 1/50\n",
            "1130/1130 [==============================] - 1s 831us/step - loss: 1.0472 - acc: 0.1938 - f1_score: nan - val_loss: 1.0250 - val_acc: 0.2083 - val_f1_score: nan\n",
            "Epoch 2/50\n",
            "1130/1130 [==============================] - 0s 102us/step - loss: 0.9869 - acc: 0.1938 - f1_score: nan - val_loss: 0.9656 - val_acc: 0.2083 - val_f1_score: nan\n",
            "Epoch 3/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.9309 - acc: 0.1929 - f1_score: nan - val_loss: 0.9115 - val_acc: 0.2083 - val_f1_score: nan\n",
            "Epoch 4/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.8799 - acc: 0.2133 - f1_score: nan - val_loss: 0.8612 - val_acc: 0.2083 - val_f1_score: nan\n",
            "Epoch 5/50\n",
            "1130/1130 [==============================] - 0s 106us/step - loss: 0.8325 - acc: 0.2133 - f1_score: nan - val_loss: 0.8166 - val_acc: 0.2083 - val_f1_score: nan\n",
            "Epoch 6/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.7905 - acc: 0.2540 - f1_score: nan - val_loss: 0.7753 - val_acc: 0.2083 - val_f1_score: 0.0952\n",
            "Epoch 7/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.7521 - acc: 0.3097 - f1_score: 0.3041 - val_loss: 0.7389 - val_acc: 0.2083 - val_f1_score: 0.0952\n",
            "Epoch 8/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.7182 - acc: 0.3195 - f1_score: 0.3159 - val_loss: 0.7072 - val_acc: 0.4167 - val_f1_score: 0.4615\n",
            "Epoch 9/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.6884 - acc: 0.4398 - f1_score: 0.5090 - val_loss: 0.6785 - val_acc: 0.7500 - val_f1_score: 0.8500\n",
            "Epoch 10/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.6619 - acc: 0.7469 - f1_score: 0.8493 - val_loss: 0.6538 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 11/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.6383 - acc: 0.8080 - f1_score: 0.8920 - val_loss: 0.6321 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 12/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.6178 - acc: 0.8080 - f1_score: 0.8926 - val_loss: 0.6139 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 13/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.6001 - acc: 0.8080 - f1_score: 0.8923 - val_loss: 0.5976 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 14/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.5849 - acc: 0.8080 - f1_score: 0.8911 - val_loss: 0.5833 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 15/50\n",
            "1130/1130 [==============================] - 0s 105us/step - loss: 0.5710 - acc: 0.8080 - f1_score: 0.8921 - val_loss: 0.5714 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 16/50\n",
            "1130/1130 [==============================] - 0s 91us/step - loss: 0.5593 - acc: 0.8080 - f1_score: 0.8922 - val_loss: 0.5612 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 17/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.5492 - acc: 0.8080 - f1_score: 0.8919 - val_loss: 0.5521 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 18/50\n",
            "1130/1130 [==============================] - 0s 97us/step - loss: 0.5402 - acc: 0.8080 - f1_score: 0.8919 - val_loss: 0.5449 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 19/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.5326 - acc: 0.8080 - f1_score: 0.8922 - val_loss: 0.5381 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 20/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.5258 - acc: 0.8080 - f1_score: 0.8923 - val_loss: 0.5330 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 21/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.5202 - acc: 0.8080 - f1_score: 0.8923 - val_loss: 0.5282 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 22/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.5154 - acc: 0.8080 - f1_score: 0.8925 - val_loss: 0.5242 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 23/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.5112 - acc: 0.8080 - f1_score: 0.8918 - val_loss: 0.5211 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 24/50\n",
            "1130/1130 [==============================] - 0s 107us/step - loss: 0.5077 - acc: 0.8080 - f1_score: 0.8925 - val_loss: 0.5184 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 25/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.5047 - acc: 0.8080 - f1_score: 0.8921 - val_loss: 0.5161 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 26/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.5021 - acc: 0.8080 - f1_score: 0.8926 - val_loss: 0.5144 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 27/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.5001 - acc: 0.8080 - f1_score: 0.8924 - val_loss: 0.5130 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 28/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.4983 - acc: 0.8080 - f1_score: 0.8922 - val_loss: 0.5117 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 29/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.4967 - acc: 0.8080 - f1_score: 0.8921 - val_loss: 0.5107 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 30/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.4954 - acc: 0.8080 - f1_score: 0.8918 - val_loss: 0.5099 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 31/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.4943 - acc: 0.8080 - f1_score: 0.8922 - val_loss: 0.5094 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 32/50\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 0.4933 - acc: 0.8080 - f1_score: 0.8924 - val_loss: 0.5089 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 33/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.4925 - acc: 0.8080 - f1_score: 0.8920 - val_loss: 0.5085 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 34/50\n",
            "1130/1130 [==============================] - 0s 106us/step - loss: 0.4918 - acc: 0.8080 - f1_score: 0.8927 - val_loss: 0.5083 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 35/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.4913 - acc: 0.8080 - f1_score: 0.8924 - val_loss: 0.5081 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 36/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.4909 - acc: 0.8080 - f1_score: 0.8921 - val_loss: 0.5080 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 37/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.4906 - acc: 0.8080 - f1_score: 0.8918 - val_loss: 0.5079 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 38/50\n",
            "1130/1130 [==============================] - 0s 97us/step - loss: 0.4903 - acc: 0.8080 - f1_score: 0.8924 - val_loss: 0.5078 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 39/50\n",
            "1130/1130 [==============================] - 0s 98us/step - loss: 0.4900 - acc: 0.8080 - f1_score: 0.8914 - val_loss: 0.5078 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 40/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.4898 - acc: 0.8080 - f1_score: 0.8917 - val_loss: 0.5078 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 41/50\n",
            "1130/1130 [==============================] - 0s 108us/step - loss: 0.4896 - acc: 0.8080 - f1_score: 0.8925 - val_loss: 0.5078 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 42/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.4895 - acc: 0.8080 - f1_score: 0.8920 - val_loss: 0.5078 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 43/50\n",
            "1130/1130 [==============================] - 0s 109us/step - loss: 0.4894 - acc: 0.8080 - f1_score: 0.8922 - val_loss: 0.5079 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 44/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.4893 - acc: 0.8080 - f1_score: 0.8920 - val_loss: 0.5079 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 45/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.4892 - acc: 0.8080 - f1_score: 0.8922 - val_loss: 0.5079 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 46/50\n",
            "1130/1130 [==============================] - 0s 97us/step - loss: 0.4891 - acc: 0.8080 - f1_score: 0.8929 - val_loss: 0.5080 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 47/50\n",
            "1130/1130 [==============================] - 0s 98us/step - loss: 0.4891 - acc: 0.8080 - f1_score: 0.8922 - val_loss: 0.5081 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 48/50\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 0.4890 - acc: 0.8080 - f1_score: 0.8927 - val_loss: 0.5080 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 49/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.4890 - acc: 0.8080 - f1_score: 0.8922 - val_loss: 0.5081 - val_acc: 0.7917 - val_f1_score: 0.8837\n",
            "Epoch 50/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.4889 - acc: 0.8080 - f1_score: 0.8923 - val_loss: 0.5082 - val_acc: 0.7917 - val_f1_score: 0.8837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvDkH8rJwB6I",
        "colab_type": "code",
        "outputId": "5c03cacf-54fe-460d-aace-f21bd83bfb02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        }
      },
      "source": [
        "# Plot results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "f1_score = history.history['f1_score']\n",
        "\n",
        "val_acc = history.history['val_acc']\n",
        "val_loss = history.history['val_loss']\n",
        "val_f1_score = history.history['val_f1_score']\n",
        "\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, f1_score, 'bo', label='Training f1_score')\n",
        "plt.plot(epochs, val_f1_score, 'r', label='Validation f1_score')\n",
        "plt.title('Training and validation f1_score')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3EVEwpCSu4hIGFJyFxEJQ0ruIiJh6P8BAFZ/bDpnqmcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4SRL-OTwktJ",
        "colab_type": "code",
        "outputId": "59d8f327-5240-46c4-84cd-365a6809563d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#### Determine Ensemble Accuracy (On Test Data)\n",
        "\n",
        "\n",
        "test_dat = np.hstack(( sagital_abnormal_model.predict(sagital_abnormal_test_features),\n",
        "                       coronal_abnormal_model.predict(coronal_abnormal_test_features),\n",
        "                       axial_abnormal_model.predict(axial_abnormal_test_features) ) )\n",
        "\n",
        "abnormal_stacked_scores = abnormal_stacked_model.evaluate(test_dat, Y_test_abnormal)\n",
        "print(\"\\n%s: %.2f%%\" % (abnormal_stacked_model.metrics_names[1], abnormal_stacked_scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 0s 109us/step\n",
            "\n",
            "acc: 79.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0kQ5F2bvrvC",
        "colab_type": "text"
      },
      "source": [
        "## ACL Stacked Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w5ee8S56Xwl",
        "colab_type": "code",
        "outputId": "935608a9-442f-4135-82a2-4bd9a1c7fb6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1734
        }
      },
      "source": [
        "### Acl stacked Ensemble\n",
        "\n",
        "acl_stacked_model = models.Sequential()\n",
        "\n",
        "acl_stacked_model.add(layers.Dense(1, activation='sigmoid', input_dim=3))\n",
        "\n",
        "#compile model\n",
        "acl_stacked_model.compile(loss='binary_crossentropy',\n",
        "                                optimizer='adam',\n",
        "                                metrics=['acc', f1_score])\n",
        "\n",
        "\n",
        "#Ensemble models\n",
        "train_dat = np.hstack(( sagital_acl_model.predict(sagital_acl_train_features),\n",
        "                       coronal_acl_model.predict(coronal_acl_train_features),\n",
        "                       axial_acl_model.predict(axial_acl_train_features) ) )\n",
        "\n",
        "valid_dat = np.hstack(( sagital_acl_model.predict(sagital_acl_validation_features),\n",
        "                       coronal_acl_model.predict(coronal_acl_validation_features),\n",
        "                       axial_acl_model.predict(axial_acl_validation_features) ) )\n",
        "\n",
        "#Train Model\n",
        "history = acl_stacked_model.fit(train_dat, Y_train_acl,\n",
        "                      batch_size=32, epochs=50,\n",
        "                      validation_data=(valid_dat,Y_valid_acl))\n",
        "\n",
        "\n",
        "# #Test Scores\n",
        "# abnormal_stacked_scores = abnormal_stacked_model.evaluate(axial_meniscus_validation_features, axial_meniscus_validation_labels)\n",
        "# print(\"\\n%s: %.2f%%\" % (axial_meniscus_model.metrics_names[1], axial_meniscus_scores[1]*100))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1130 samples, validate on 24 samples\n",
            "Epoch 1/50\n",
            "1130/1130 [==============================] - 1s 934us/step - loss: 0.7315 - acc: 0.6832 - f1_score: nan - val_loss: 0.7348 - val_acc: 0.3750 - val_f1_score: nan\n",
            "Epoch 2/50\n",
            "1130/1130 [==============================] - 0s 98us/step - loss: 0.7135 - acc: 0.7009 - f1_score: nan - val_loss: 0.7269 - val_acc: 0.3750 - val_f1_score: nan\n",
            "Epoch 3/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.6966 - acc: 0.7009 - f1_score: nan - val_loss: 0.7200 - val_acc: 0.3750 - val_f1_score: nan\n",
            "Epoch 4/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.6809 - acc: 0.7009 - f1_score: nan - val_loss: 0.7136 - val_acc: 0.4167 - val_f1_score: nan\n",
            "Epoch 5/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.6661 - acc: 0.7009 - f1_score: nan - val_loss: 0.7081 - val_acc: 0.4167 - val_f1_score: nan\n",
            "Epoch 6/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.6525 - acc: 0.7018 - f1_score: nan - val_loss: 0.7031 - val_acc: 0.5000 - val_f1_score: nan\n",
            "Epoch 7/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.6398 - acc: 0.7035 - f1_score: nan - val_loss: 0.6989 - val_acc: 0.5000 - val_f1_score: nan\n",
            "Epoch 8/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.6281 - acc: 0.7354 - f1_score: nan - val_loss: 0.6953 - val_acc: 0.5000 - val_f1_score: nan\n",
            "Epoch 9/50\n",
            "1130/1130 [==============================] - 0s 106us/step - loss: 0.6171 - acc: 0.7389 - f1_score: nan - val_loss: 0.6922 - val_acc: 0.5000 - val_f1_score: nan\n",
            "Epoch 10/50\n",
            "1130/1130 [==============================] - 0s 91us/step - loss: 0.6070 - acc: 0.7389 - f1_score: nan - val_loss: 0.6897 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 11/50\n",
            "1130/1130 [==============================] - 0s 104us/step - loss: 0.5977 - acc: 0.7389 - f1_score: nan - val_loss: 0.6877 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 12/50\n",
            "1130/1130 [==============================] - 0s 102us/step - loss: 0.5892 - acc: 0.8159 - f1_score: nan - val_loss: 0.6861 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 13/50\n",
            "1130/1130 [==============================] - 0s 101us/step - loss: 0.5811 - acc: 0.8159 - f1_score: nan - val_loss: 0.6848 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 14/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.5737 - acc: 0.8159 - f1_score: nan - val_loss: 0.6839 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 15/50\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 0.5669 - acc: 0.8159 - f1_score: nan - val_loss: 0.6834 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 16/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.5607 - acc: 0.8159 - f1_score: nan - val_loss: 0.6832 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 17/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.5549 - acc: 0.8159 - f1_score: nan - val_loss: 0.6832 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 18/50\n",
            "1130/1130 [==============================] - 0s 104us/step - loss: 0.5496 - acc: 0.8159 - f1_score: nan - val_loss: 0.6835 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 19/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.5447 - acc: 0.8159 - f1_score: nan - val_loss: 0.6840 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 20/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.5402 - acc: 0.8159 - f1_score: nan - val_loss: 0.6848 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 21/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.5360 - acc: 0.8159 - f1_score: nan - val_loss: 0.6856 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 22/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.5321 - acc: 0.8159 - f1_score: nan - val_loss: 0.6866 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 23/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.5285 - acc: 0.8159 - f1_score: nan - val_loss: 0.6879 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 24/50\n",
            "1130/1130 [==============================] - 0s 97us/step - loss: 0.5250 - acc: 0.8159 - f1_score: nan - val_loss: 0.6892 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 25/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.5219 - acc: 0.8159 - f1_score: nan - val_loss: 0.6907 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 26/50\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 0.5191 - acc: 0.8159 - f1_score: nan - val_loss: 0.6922 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 27/50\n",
            "1130/1130 [==============================] - 0s 98us/step - loss: 0.5165 - acc: 0.8159 - f1_score: nan - val_loss: 0.6939 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 28/50\n",
            "1130/1130 [==============================] - 0s 102us/step - loss: 0.5139 - acc: 0.8159 - f1_score: nan - val_loss: 0.6956 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 29/50\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 0.5117 - acc: 0.8159 - f1_score: nan - val_loss: 0.6975 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 30/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.5096 - acc: 0.8159 - f1_score: nan - val_loss: 0.6992 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 31/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.5077 - acc: 0.8159 - f1_score: nan - val_loss: 0.7011 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 32/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.5059 - acc: 0.8159 - f1_score: nan - val_loss: 0.7030 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 33/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.5043 - acc: 0.8159 - f1_score: nan - val_loss: 0.7049 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 34/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.5028 - acc: 0.8159 - f1_score: nan - val_loss: 0.7068 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 35/50\n",
            "1130/1130 [==============================] - 0s 99us/step - loss: 0.5014 - acc: 0.8159 - f1_score: nan - val_loss: 0.7088 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 36/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.5000 - acc: 0.8159 - f1_score: nan - val_loss: 0.7110 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 37/50\n",
            "1130/1130 [==============================] - 0s 104us/step - loss: 0.4987 - acc: 0.8159 - f1_score: nan - val_loss: 0.7130 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 38/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.4975 - acc: 0.8159 - f1_score: nan - val_loss: 0.7152 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 39/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.4965 - acc: 0.8159 - f1_score: nan - val_loss: 0.7172 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 40/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.4954 - acc: 0.8159 - f1_score: nan - val_loss: 0.7190 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 41/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.4946 - acc: 0.8159 - f1_score: nan - val_loss: 0.7211 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 42/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.4937 - acc: 0.8159 - f1_score: nan - val_loss: 0.7229 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 43/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.4929 - acc: 0.8159 - f1_score: nan - val_loss: 0.7249 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 44/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.4922 - acc: 0.8159 - f1_score: nan - val_loss: 0.7269 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 45/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.4914 - acc: 0.8159 - f1_score: nan - val_loss: 0.7288 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 46/50\n",
            "1130/1130 [==============================] - 0s 107us/step - loss: 0.4908 - acc: 0.8159 - f1_score: nan - val_loss: 0.7306 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 47/50\n",
            "1130/1130 [==============================] - 0s 91us/step - loss: 0.4901 - acc: 0.8159 - f1_score: nan - val_loss: 0.7328 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 48/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.4895 - acc: 0.8159 - f1_score: nan - val_loss: 0.7345 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 49/50\n",
            "1130/1130 [==============================] - 0s 102us/step - loss: 0.4890 - acc: 0.8159 - f1_score: nan - val_loss: 0.7363 - val_acc: 0.5833 - val_f1_score: nan\n",
            "Epoch 50/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.4885 - acc: 0.8159 - f1_score: nan - val_loss: 0.7382 - val_acc: 0.5833 - val_f1_score: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "102c61b6-74f7-474f-d730-9f3b1c2bddba",
        "id": "NgEWzng0zyny",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        }
      },
      "source": [
        "# Plot results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "f1_score = history.history['f1_score']\n",
        "\n",
        "val_acc = history.history['val_acc']\n",
        "val_loss = history.history['val_loss']\n",
        "val_f1_score = history.history['val_f1_score']\n",
        "\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, f1_score, 'bo', label='Training f1_score')\n",
        "plt.plot(epochs, val_f1_score, 'r', label='Validation f1_score')\n",
        "plt.title('Training and validation f1_score')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEICAYAAABbOlNNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+FX5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ATKMDJ5mzyn2",
        "outputId": "3a6ca17d-45de-4ecb-a52f-8e0ded386115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#### Determine Ensemble Accuracy (On Test Data)\n",
        "\n",
        "test_dat = np.hstack(( sagital_acl_model.predict(sagital_acl_test_features),\n",
        "                       coronal_acl_model.predict(coronal_acl_test_features),\n",
        "                       axial_acl_model.predict(axial_acl_test_features) ) )\n",
        "\n",
        "acl_stacked_scores = acl_stacked_model.evaluate(test_dat, Y_test_acl)\n",
        "print(\"\\n%s: %.2f%%\" % (acl_stacked_model.metrics_names[1], acl_stacked_scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 0s 82us/step\n",
            "\n",
            "acc: 54.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep0QmwuIvx3D",
        "colab_type": "text"
      },
      "source": [
        "## Meniscus Stacked Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpUe8Zg96aHE",
        "colab_type": "code",
        "outputId": "bd954ba1-c023-40b6-dd5a-78bb27deb7fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1734
        }
      },
      "source": [
        "### Meniscus stacked Ensemble\n",
        "\n",
        "meniscus_stacked_model = models.Sequential()\n",
        "\n",
        "meniscus_stacked_model.add(layers.Dense(1, activation='sigmoid', input_dim=3))\n",
        "\n",
        "#compile model\n",
        "meniscus_stacked_model.compile(loss='binary_crossentropy',\n",
        "                                optimizer='adam',\n",
        "                                metrics=['acc', f1_score])\n",
        "\n",
        "\n",
        "#Ensemble models\n",
        "train_dat = np.hstack(( sagital_meniscus_model.predict(sagital_meniscus_train_features),\n",
        "                       coronal_meniscus_model.predict(coronal_meniscus_train_features),\n",
        "                       axial_meniscus_model.predict(axial_meniscus_train_features) ) )\n",
        "\n",
        "valid_dat = np.hstack(( sagital_meniscus_model.predict(sagital_meniscus_validation_features),\n",
        "                       coronal_meniscus_model.predict(coronal_meniscus_validation_features),\n",
        "                       axial_meniscus_model.predict(axial_meniscus_validation_features) ) )\n",
        "\n",
        "\n",
        "#Train Model\n",
        "history = meniscus_stacked_model.fit(train_dat, Y_train_meniscus,\n",
        "                                     batch_size=32, epochs=50,\n",
        "                                    validation_data=(valid_dat,Y_valid_meniscus))\n",
        "\n",
        "\n",
        "# #Test Scores\n",
        "# abnormal_stacked_scores = abnormal_stacked_model.evaluate(axial_meniscus_validation_features, axial_meniscus_validation_labels)\n",
        "# print(\"\\n%s: %.2f%%\" % (axial_meniscus_model.metrics_names[1], axial_meniscus_scores[1]*100))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1130 samples, validate on 24 samples\n",
            "Epoch 1/50\n",
            "1130/1130 [==============================] - 1s 934us/step - loss: 0.9419 - acc: 0.4027 - f1_score: 0.4901 - val_loss: 0.8295 - val_acc: 0.4167 - val_f1_score: 0.5882\n",
            "Epoch 2/50\n",
            "1130/1130 [==============================] - 0s 97us/step - loss: 0.9168 - acc: 0.4389 - f1_score: 0.4910 - val_loss: 0.8112 - val_acc: 0.4167 - val_f1_score: 0.5882\n",
            "Epoch 3/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.8930 - acc: 0.4442 - f1_score: 0.4885 - val_loss: 0.7940 - val_acc: 0.4167 - val_f1_score: 0.5882\n",
            "Epoch 4/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.8704 - acc: 0.4487 - f1_score: 0.4884 - val_loss: 0.7779 - val_acc: 0.5000 - val_f1_score: 0.6250\n",
            "Epoch 5/50\n",
            "1130/1130 [==============================] - 0s 108us/step - loss: 0.8492 - acc: 0.4540 - f1_score: 0.4877 - val_loss: 0.7633 - val_acc: 0.5000 - val_f1_score: 0.6250\n",
            "Epoch 6/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.8293 - acc: 0.4496 - f1_score: nan - val_loss: 0.7501 - val_acc: 0.5000 - val_f1_score: 0.6000\n",
            "Epoch 7/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.8109 - acc: 0.4584 - f1_score: 0.4697 - val_loss: 0.7380 - val_acc: 0.5000 - val_f1_score: 0.6000\n",
            "Epoch 8/50\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 0.7941 - acc: 0.4956 - f1_score: nan - val_loss: 0.7273 - val_acc: 0.4583 - val_f1_score: 0.5517\n",
            "Epoch 9/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.7788 - acc: 0.4982 - f1_score: 0.4611 - val_loss: 0.7174 - val_acc: 0.4583 - val_f1_score: 0.5517\n",
            "Epoch 10/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.7647 - acc: 0.4965 - f1_score: 0.4543 - val_loss: 0.7091 - val_acc: 0.4167 - val_f1_score: 0.5000\n",
            "Epoch 11/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.7519 - acc: 0.5000 - f1_score: 0.4577 - val_loss: 0.7018 - val_acc: 0.4583 - val_f1_score: 0.5185\n",
            "Epoch 12/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.7401 - acc: 0.5000 - f1_score: 0.4567 - val_loss: 0.6956 - val_acc: 0.5000 - val_f1_score: 0.5385\n",
            "Epoch 13/50\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 0.7295 - acc: 0.5000 - f1_score: 0.4588 - val_loss: 0.6902 - val_acc: 0.5417 - val_f1_score: 0.5600\n",
            "Epoch 14/50\n",
            "1130/1130 [==============================] - 0s 104us/step - loss: 0.7200 - acc: 0.4991 - f1_score: 0.4526 - val_loss: 0.6856 - val_acc: 0.5417 - val_f1_score: 0.5600\n",
            "Epoch 15/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.7113 - acc: 0.4991 - f1_score: 0.4492 - val_loss: 0.6820 - val_acc: 0.5417 - val_f1_score: 0.5600\n",
            "Epoch 16/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.7037 - acc: 0.4991 - f1_score: 0.4564 - val_loss: 0.6787 - val_acc: 0.5417 - val_f1_score: 0.5600\n",
            "Epoch 17/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.6967 - acc: 0.4991 - f1_score: 0.4554 - val_loss: 0.6764 - val_acc: 0.6250 - val_f1_score: 0.6087\n",
            "Epoch 18/50\n",
            "1130/1130 [==============================] - 0s 91us/step - loss: 0.6905 - acc: 0.5283 - f1_score: 0.4205 - val_loss: 0.6745 - val_acc: 0.5833 - val_f1_score: 0.5000\n",
            "Epoch 19/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.6851 - acc: 0.5327 - f1_score: 0.3265 - val_loss: 0.6731 - val_acc: 0.5833 - val_f1_score: 0.5000\n",
            "Epoch 20/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.6801 - acc: 0.5469 - f1_score: nan - val_loss: 0.6723 - val_acc: 0.6250 - val_f1_score: 0.5263\n",
            "Epoch 21/50\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 0.6759 - acc: 0.5673 - f1_score: nan - val_loss: 0.6718 - val_acc: 0.6250 - val_f1_score: 0.5263\n",
            "Epoch 22/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.6721 - acc: 0.6053 - f1_score: nan - val_loss: 0.6716 - val_acc: 0.6250 - val_f1_score: 0.4706\n",
            "Epoch 23/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.6689 - acc: 0.6142 - f1_score: nan - val_loss: 0.6717 - val_acc: 0.5833 - val_f1_score: 0.2857\n",
            "Epoch 24/50\n",
            "1130/1130 [==============================] - 0s 110us/step - loss: 0.6660 - acc: 0.6168 - f1_score: nan - val_loss: 0.6720 - val_acc: 0.5833 - val_f1_score: 0.2857\n",
            "Epoch 25/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.6635 - acc: 0.6168 - f1_score: nan - val_loss: 0.6726 - val_acc: 0.5833 - val_f1_score: 0.2857\n",
            "Epoch 26/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.6614 - acc: 0.6177 - f1_score: nan - val_loss: 0.6733 - val_acc: 0.5833 - val_f1_score: 0.2857\n",
            "Epoch 27/50\n",
            "1130/1130 [==============================] - 0s 106us/step - loss: 0.6594 - acc: 0.6177 - f1_score: nan - val_loss: 0.6740 - val_acc: 0.5833 - val_f1_score: 0.2857\n",
            "Epoch 28/50\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 0.6579 - acc: 0.6186 - f1_score: nan - val_loss: 0.6750 - val_acc: 0.5833 - val_f1_score: 0.2857\n",
            "Epoch 29/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.6564 - acc: 0.6195 - f1_score: nan - val_loss: 0.6759 - val_acc: 0.5833 - val_f1_score: 0.1667\n",
            "Epoch 30/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.6552 - acc: 0.6292 - f1_score: nan - val_loss: 0.6770 - val_acc: 0.5833 - val_f1_score: 0.1667\n",
            "Epoch 31/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.6542 - acc: 0.6292 - f1_score: nan - val_loss: 0.6777 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 32/50\n",
            "1130/1130 [==============================] - 0s 104us/step - loss: 0.6532 - acc: 0.6292 - f1_score: nan - val_loss: 0.6787 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 33/50\n",
            "1130/1130 [==============================] - 0s 108us/step - loss: 0.6524 - acc: 0.6265 - f1_score: nan - val_loss: 0.6799 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 34/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.6517 - acc: 0.6434 - f1_score: nan - val_loss: 0.6808 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 35/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.6511 - acc: 0.6487 - f1_score: nan - val_loss: 0.6815 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 36/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.6506 - acc: 0.6487 - f1_score: nan - val_loss: 0.6824 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 37/50\n",
            "1130/1130 [==============================] - 0s 99us/step - loss: 0.6500 - acc: 0.6487 - f1_score: nan - val_loss: 0.6835 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 38/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.6496 - acc: 0.6487 - f1_score: nan - val_loss: 0.6843 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 39/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.6492 - acc: 0.6487 - f1_score: nan - val_loss: 0.6849 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 40/50\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 0.6489 - acc: 0.6487 - f1_score: nan - val_loss: 0.6858 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 41/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.6486 - acc: 0.6487 - f1_score: nan - val_loss: 0.6864 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 42/50\n",
            "1130/1130 [==============================] - 0s 112us/step - loss: 0.6483 - acc: 0.6487 - f1_score: nan - val_loss: 0.6870 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 43/50\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 0.6482 - acc: 0.6487 - f1_score: nan - val_loss: 0.6879 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 44/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.6479 - acc: 0.6487 - f1_score: nan - val_loss: 0.6881 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 45/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.6477 - acc: 0.6487 - f1_score: nan - val_loss: 0.6887 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 46/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.6475 - acc: 0.6487 - f1_score: nan - val_loss: 0.6895 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 47/50\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 0.6473 - acc: 0.6487 - f1_score: nan - val_loss: 0.6897 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 48/50\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 0.6471 - acc: 0.6487 - f1_score: nan - val_loss: 0.6902 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 49/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.6470 - acc: 0.6487 - f1_score: nan - val_loss: 0.6911 - val_acc: 0.5417 - val_f1_score: nan\n",
            "Epoch 50/50\n",
            "1130/1130 [==============================] - 0s 94us/step - loss: 0.6468 - acc: 0.6487 - f1_score: nan - val_loss: 0.6912 - val_acc: 0.5417 - val_f1_score: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6b20a837-fd3c-4e4a-e2ab-c712262a14e3",
        "id": "tAVxd3dvz2b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        }
      },
      "source": [
        "# Plot results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "f1_score = history.history['f1_score']\n",
        "\n",
        "val_acc = history.history['val_acc']\n",
        "val_loss = history.history['val_loss']\n",
        "val_f1_score = history.history['val_f1_score']\n",
        "\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, f1_score, 'bo', label='Training f1_score')\n",
        "plt.plot(epochs, val_f1_score, 'r', label='Validation f1_score')\n",
        "plt.title('Training and validation f1_score')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkf8PIDf3oE38y5YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGJOELLkbY0wS+n8alrn6azzJQwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Ds2oOVJz2cG",
        "outputId": "84150f1f-34cc-42a9-b99e-1534768c1a2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#### Determine Ensemble Accuracy (On Test Data)\n",
        "\n",
        "\n",
        "test_dat = np.hstack(( sagital_meniscus_model.predict(sagital_meniscus_test_features),\n",
        "                       coronal_meniscus_model.predict(coronal_meniscus_test_features),\n",
        "                       axial_meniscus_model.predict(axial_meniscus_test_features) ) )\n",
        "\n",
        "meniscus_stacked_scores = meniscus_stacked_model.evaluate(test_dat, Y_test_meniscus)\n",
        "print(\"\\n%s: %.2f%%\" % (meniscus_stacked_model.metrics_names[1], meniscus_stacked_scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 0s 83us/step\n",
            "\n",
            "acc: 57.29%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}